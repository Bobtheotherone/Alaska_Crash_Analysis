#make a script to clean the dataset
import pandas as pd
import argparse
from pathlib import Path
import config
config.validate()  #validate config values
from unknown_discovery import discover_unknown_placeholders


#now have user enter in data rather than hardcoded filepath
def load_table(path: Path) -> pd.DataFrame:
    #load excel file into pandas dataframe, also accept .csv files of all formats

    full_ext = ''.join(path.suffixes).lower() #get all suffixes, lowercase for consistency
    if any(full_ext.endswith(ext) for ext in ['.xlsx', '.xlsm', '.xls']):
        return pd.read_excel(path)

    #if statement below generated by ChatGPT to handle various csv/tsv files                         
    if any(full_ext.endswith(ext) for ext in ('.csv', '.csv.gz', '.tsv', '.txt')):
        # 1) Try delimiter sniffing with UTF-8
        try:
            return pd.read_csv(path, sep=None, engine='python', low_memory=False)
        except UnicodeDecodeError:
            # 2) Fallback to latin-1 if encoding trips
            try:
                return pd.read_csv(path, sep=None, engine='python', low_memory=False, encoding='latin-1')
            except Exception:
                pass
        except Exception:
            pass
        # 3) Last-ditch: assume comma, then tab
        for sep_guess in (',', '\t'):
            for enc in (None, 'latin-1'):
                try:
                    return pd.read_csv(path, sep=sep_guess, low_memory=False, encoding=enc)
                except Exception:
                    continue
        raise SystemExit(f"Could not read delimited file: {path}")

    raise SystemExit(
        f"Unsupported file type: {path.suffix or full_ext}. "
        "Use .xlsx/.xlsm/.xls or .csv/.csv.gz/.tsv/.txt."
    )

def percent_unknowns_per_column(df, unknown_strings):
    #scans each column for percentage of unknown/missing values
    tokens = {s.strip().lower() for s in unknown_strings} #tokenize and lowercase column inputs
    results = []
    for col in df.columns:
        series = df[col]            #each row in the column
        total_count = len(series)   #total rows in the column
        if total_count == 0:
            percent_unknown = 0.0
        else:
            mask_null = series.isna() #to avoid pandas 'nan' string issue
            series_norm = series[~mask_null].astype(str).str.strip().str.lower() #while not pandas null, convert to string, strip string, lowercase string
            mask_token = series_norm.isin(tokens)   #check if in our unknown tokens
            unknown_count = int(mask_null.sum()) + int(mask_token.sum()) #sum pandas nans and our known unknowns
            percent_unknown = (unknown_count / total_count) * 100
        results.append((col, percent_unknown))

    return results

def yes_no(df, unknown_strings):
    #scans each column for percentage of yes/no values
    tokens = {s.strip().lower() for s in unknown_strings} #tokenize and lowercase column inputs
    YES = {"yes", "y", "true", "t"}
    NO = {"no", "n", "false", "f"}

    out = {}
    for col in df.columns:
        series = df[col]            #each row in the column

        mask_not_null = (~series.isna()) #filter out pandas nulls
        if not mask_not_null.any():
            continue

        series_norm = series[mask_not_null].astype(str).str.strip().str.lower() #while not pandas null, convert to string, strip string, lowercase string
        series_norm = series_norm[~series_norm.isin(tokens)] #filter out known unknowns

        is_yes = series_norm.isin(YES)
        is_no = series_norm.isin(NO)

        yes_count = int(is_yes.sum())
        no_count = int(is_no.sum())
        total_count = yes_count + no_count      #total count of yes/no for percentage calc

        if total_count > 0:
            yes_pct = round(100.0 * yes_count / total_count, 2)
            no_pct  = round(100.0 * no_count  / total_count, 2)
            out[col] = (yes_pct, no_pct, yes_count, no_count, total_count)

    return out

def unique_non_unknown_counts(df, unknown_strings):
    #get unique value counts per column excluding unknowns
    tokens = {s.strip().lower() for s in unknown_strings} #tokenize and lowercase column inputs
    out = {}

    for col in df.columns:
        series = df[col]
        mask_not_null = (~series.isna()) #filter out pandas nulls

        if not mask_not_null.any(): #catch all unknown columns for error prevention
            out[col] = 0
            continue

        series_norm = series[mask_not_null].astype(str).str.strip().str.lower() #while not pandas null, convert to string, strip string, lowercase string
        series_norm = series_norm[~series_norm.isin(tokens)] #filter out known unknowns

        out[col] = int(series_norm.nunique(dropna=True)) #count unique non-unknown values

    return out



UNKNOWN_STRINGS = {
    "no data",
    "missing value",
    "null value",
    "missing",
    "na", "n/a", "n.a.",
    "none",
    "null",
    "nan",
    "unknown",
    "unspecified",
    "not specified",
    "not applicable",
    "tbd", "tba", "to be determined",
    "-", "--",
    "(blank)", "blank",
    "(null)",
    "?", 
    "prefer not to say",
    "refused"
}

#get our unknown and yes/no thresholds from config file
unknown_thresh = float(config.UNKNOWN_THRESHOLD)
yes_no_thresh = float(config.YES_NO_THRESHOLD)



parser = argparse.ArgumentParser(description="Clean a dataset by identifying unknown/missing values.")
parser.add_argument("-f", "--file", required=False, help="Path to the input .xlsx or .csv file.")
args = parser.parse_args()

if args.file:
    input_file = Path(args.file)
else:
    #prompt user for file path if not provided
    raw = input("Enter the path to the input file: ").strip().strip('"') #use raw for input cleaning
    raw = raw.replace("'", "").replace('"', '') #remove single quotes or double quotes if user added them
    input_file = Path(raw)

if not input_file.exists():
    raise SystemExit(f"Error: The file {input_file} does not exist.") #failsafe if file not found
if input_file.is_dir(): #directory, not file entered
    raise SystemExit(f"Error: The path {input_file} is a directory, not a file.")


df1 = load_table(input_file)
print(f"Loaded {input_file} with {df1.shape[0]} rows and {df1.shape[1]} columns.")


aug_unknowns = discover_unknown_placeholders(df1, UNKNOWN_STRINGS) #get new unknown values
print(f"Discovered {len(aug_unknowns) - len(UNKNOWN_STRINGS)} new unknown tokens.")
#for val in aug_unknowns:
#    print(f"{val}\n")


#get our percentages
uk = percent_unknowns_per_column(df1, aug_unknowns)
uk_pcts = dict(uk)  #dictionary for quicker lookup
yn = yes_no(df1, aug_unknowns)
yn_coverage_min = 50.0 #to cover mistaken yes/no
uniq = unique_non_unknown_counts(df1, aug_unknowns)

to_drop = set()     #set of columns to drop
n_rows = len(df1)   #get total # of rows

#drop columns above the unknown threshold
for col, pct in uk_pcts.items():
    if pct > unknown_thresh:
        to_drop.add(col)

#drop columns with extreme yes/no imbalance
for col, (y_pct, n_pct, y_cnt, n_cnt, total_yesno) in yn.items():
    coverage_pct = (total_yesno / n_rows * 100.0) if n_rows else 0.0
    if coverage_pct >= yn_coverage_min: #to cover cases where a few values are falsely flagged as yes/no
        if y_pct < yes_no_thresh or n_pct < yes_no_thresh:
            to_drop.add(col)

#drop columns with only 1 unique non-unknown value or equal to # of rows
for col, unique in uniq.items():
    if unique <= 1 or unique >= n_rows:
        to_drop.add(col)

#make a new df to write to csv
clean_df = df1.drop(columns=sorted(to_drop), errors="ignore")

#write to new csv file with _cleaned appended
out_csv = input_file.with_name(f"{input_file.stem}_cleaned_test.csv")
clean_df.to_csv(out_csv, index=False)

#print out confirmation message
print(f"Dropped {len(to_drop)} columns. New shape: {clean_df.shape[0]} rows x {clean_df.shape[1]} cols")
print(f"Saved: {out_csv}")


#just print for now until we decide functionality
uk.sort(key=lambda x: x[1], reverse=True)
print("\n% Unknown by column (NaN + known placeholders):")
for col, pct in uk:
    extra = ""
    if col in yn:
        y_pct, n_pct, y_cnt, n_cnt, total = yn[col]
        extra = f"   (Yes/No: {y_pct:.2f}%/{n_pct:.2f}% of {total} known)"
    print(f"{pct:6.2f}%  -  {col}{extra}")