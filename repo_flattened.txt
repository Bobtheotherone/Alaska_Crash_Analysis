# Flattened repository file listing
# Root: C:\Users\dimen\Downloads\alaska_crash_analysis

================================================================================
===== BEGIN FILE: .gitignore =====
================================================================================

# Python
__pycache__/
*.py[cod]
*.sqlite3
.env
.venv/

# Django static
staticfiles/

# Node
node_modules/
alaska_ui/dist/
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# IDE
.vscode/
.idea/
*.iml

================================================================================
===== END FILE: .gitignore =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_project\__init__.py =====
================================================================================


================================================================================
===== END FILE: alaska_project\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_project\asgi.py =====
================================================================================

import os
from django.core.asgi import get_asgi_application

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "alaska_project.settings")

application = get_asgi_application()

================================================================================
===== END FILE: alaska_project\asgi.py =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_project\security.py =====
================================================================================

from django.utils.deprecation import MiddlewareMixin


class SecurityHeadersMiddleware(MiddlewareMixin):
    """Attach a small set of OWASP-aligned security headers.

    These headers complement Django's SecurityMiddleware and are safe to run
    in both development and production. More restrictive CSP policies can be
    configured at the reverse proxy layer if needed.
    """

    def process_response(self, request, response):  # noqa: D401
        # Protect against MIME confusion attacks.
        response.setdefault("X-Content-Type-Options", "nosniff")
        # Clickjacking protection.
        response.setdefault("X-Frame-Options", "DENY")
        # Limit referrer leakage.
        response.setdefault("Referrer-Policy", "same-origin")
        # Cross-origin isolation for modern browsers.
        response.setdefault("Cross-Origin-Opener-Policy", "same-origin")
        # Simple CSP tuned for a React single-page app served from the same origin.
        csp = (
            "default-src 'self'; "
            "script-src 'self'; "
            "style-src 'self' 'unsafe-inline'; "
            "img-src 'self' data:; "
            "connect-src 'self'; "
            "font-src 'self' data:; "
            "object-src 'none'; "
            "frame-ancestors 'none'"
        )
        response.setdefault("Content-Security-Policy", csp)
        return response

================================================================================
===== END FILE: alaska_project\security.py =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_project\settings.py =====
================================================================================

from pathlib import Path
import os

BASE_DIR = Path(__file__).resolve().parent.parent

# ---------------------------------------------------------------------------
# Core Django settings
# ---------------------------------------------------------------------------

DEBUG = os.environ.get("DJANGO_DEBUG", "true").lower() == "true"

# SECRET_KEY must always come from the environment when DEBUG is False.
if DEBUG:
    SECRET_KEY = os.environ.get(
        "DJANGO_SECRET_KEY",
        "dev-secret-key-not-for-production",
    )
else:
    SECRET_KEY = os.environ["DJANGO_SECRET_KEY"]

# More secure default: only localhost unless DJANGO_ALLOWED_HOSTS is set.
# Example: DJANGO_ALLOWED_HOSTS="example.com,api.example.com"
ALLOWED_HOSTS = [
    host.strip()
    for host in os.environ.get(
        "DJANGO_ALLOWED_HOSTS",
        "127.0.0.1,localhost",
    ).split(",")
    if host.strip()
]

# ---------------------------------------------------------------------------
# Applications
# ---------------------------------------------------------------------------

INSTALLED_APPS = [
    "django.contrib.admin",
    "django.contrib.auth",
    "django.contrib.contenttypes",
    "django.contrib.sessions",
    "django.contrib.messages",
    "django.contrib.staticfiles",
    "django.contrib.gis",
    "rest_framework",
    "django_filters",
    "drf_spectacular",
    "corsheaders",
    "ingestion",
    "crashdata",
    "analysis",
]

MIDDLEWARE = [
    "django.middleware.security.SecurityMiddleware",
    "django.contrib.sessions.middleware.SessionMiddleware",
    "corsheaders.middleware.CorsMiddleware",
    "django.middleware.common.CommonMiddleware",
    "django.middleware.csrf.CsrfViewMiddleware",
    "django.contrib.auth.middleware.AuthenticationMiddleware",
    "django.contrib.messages.middleware.MessageMiddleware",
    "django.middleware.clickjacking.XFrameOptionsMiddleware",
]

ROOT_URLCONF = "alaska_project.urls"

TEMPLATES = [
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "DIRS": [],
        "APP_DIRS": True,
        "OPTIONS": {
            "context_processors": [
                "django.template.context_processors.debug",
                "django.template.context_processors.request",
                "django.contrib.auth.context_processors.auth",
                "django.contrib.messages.context_processors.messages",
            ],
        },
    },
]

WSGI_APPLICATION = "alaska_project.wsgi.application"

# ---------------------------------------------------------------------------
# Database
# ---------------------------------------------------------------------------

DATABASES = {
    "default": {
        "ENGINE": "django.contrib.gis.db.backends.postgis",
        "NAME": os.environ.get("POSTGRES_DB", "alaska_crash_analysis"),
        "USER": os.environ.get("POSTGRES_USER", "postgres"),
        "PASSWORD": os.environ.get("POSTGRES_PASSWORD", ""),
        "HOST": os.environ.get("POSTGRES_HOST", "localhost"),
        "PORT": os.environ.get("POSTGRES_PORT", "5432"),
    }
}

# ---------------------------------------------------------------------------
# Password validation
# ---------------------------------------------------------------------------

AUTH_PASSWORD_VALIDATORS = [
    {
        "NAME": "django.contrib.auth.password_validation.UserAttributeSimilarityValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.MinimumLengthValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.CommonPasswordValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.NumericPasswordValidator",
    },
]

# ---------------------------------------------------------------------------
# Internationalization
# ---------------------------------------------------------------------------

LANGUAGE_CODE = "en-us"

TIME_ZONE = "UTC"

USE_I18N = True

USE_TZ = True

# ---------------------------------------------------------------------------
# Static files
# ---------------------------------------------------------------------------

STATIC_URL = "/static/"
STATIC_ROOT = os.path.join(BASE_DIR, "staticfiles")

MEDIA_ROOT = BASE_DIR / "media"
MEDIA_URL = "/media/"


DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"

# ---------------------------------------------------------------------------
# REST framework configuration
# ---------------------------------------------------------------------------

REST_FRAMEWORK = {
    "DEFAULT_AUTHENTICATION_CLASSES": [
        "rest_framework.authentication.SessionAuthentication",
        "rest_framework.authentication.BasicAuthentication",
    ],
    "DEFAULT_PERMISSION_CLASSES": [
        "rest_framework.permissions.IsAuthenticated",
    ],
    "DEFAULT_SCHEMA_CLASS": "drf_spectacular.openapi.AutoSchema",
    "DEFAULT_FILTER_BACKENDS": [
        "django_filters.rest_framework.DjangoFilterBackend",
        "rest_framework.filters.OrderingFilter",
        "rest_framework.filters.SearchFilter",
    ],
    "DEFAULT_THROTTLE_RATES": {
        # Upload gateway: fairly low by default to avoid abuse.
        "ingest_upload": "10/hour",
        # Export endpoints: lower rate to avoid accidental DoS.
        "exports": "5/hour",
    },
}

SPECTACULAR_SETTINGS = {
    "TITLE": "Alaska Crash Analysis API",
    "DESCRIPTION": "Django backend for ingesting and exploring Alaska crash data.",
    "VERSION": "1.0.0",
}

# ---------------------------------------------------------------------------
# CORS
# ---------------------------------------------------------------------------

CORS_ALLOW_ALL_ORIGINS = os.environ.get("CORS_ALLOW_ALL_ORIGINS", "true").lower() == "true"

# ---------------------------------------------------------------------------
# Upload gateway / ingestion configuration
# ---------------------------------------------------------------------------

INGESTION_MAX_FILE_SIZE_BYTES = int(
    os.environ.get("INGESTION_MAX_FILE_SIZE_BYTES", str(10 * 1024 * 1024))  # 10 MB
)

# Comma-separated list of allowed file extensions for uploads.
# Production deployments should set this via environment or .env.
# Example: INGESTION_ALLOWED_EXTENSIONS=".csv,.parquet"
INGESTION_ALLOWED_EXTENSIONS = [
    ext.strip().lower()
    for ext in os.environ.get(
        "INGESTION_ALLOWED_EXTENSIONS",
        ".csv,.parquet",
    ).split(",")
    if ext.strip()
]

# When True, antivirus scanning is *required* for ingestion. If ClamAV is
# unavailable and this flag is True, uploads will be rejected.
INGESTION_REQUIRE_AV = os.environ.get("INGESTION_REQUIRE_AV", "false").lower() == "true"

# Path to the externalized MMUCC/KABCO schema configuration that non-devs can
# edit without touching Python code.
INGESTION_SCHEMA_CONFIG_PATH = os.environ.get(
    "INGESTION_SCHEMA_CONFIG_PATH",
    str(BASE_DIR / "ingestion" / "config" / "mmucc_schema.yml"),
)

# ---------------------------------------------------------------------------
# Security
# ---------------------------------------------------------------------------

SECURE_PROXY_SSL_HEADER = ("HTTP_X_FORWARDED_PROTO", "https")

SESSION_COOKIE_SECURE = not DEBUG
CSRF_COOKIE_SECURE = not DEBUG
SECURE_SSL_REDIRECT = os.environ.get("DJANGO_SECURE_SSL_REDIRECT", "false").lower() == "true"

# You may wish to fine-tune this further for production deployments.
SECURE_HSTS_SECONDS = int(os.environ.get("DJANGO_SECURE_HSTS_SECONDS", "0"))
SECURE_HSTS_INCLUDE_SUBDOMAINS = os.environ.get(
    "DJANGO_SECURE_HSTS_INCLUDE_SUBDOMAINS", "false"
).lower() == "true"
SECURE_HSTS_PRELOAD = os.environ.get("DJANGO_SECURE_HSTS_PRELOAD", "false").lower() == "true"

================================================================================
===== END FILE: alaska_project\settings.py =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_project\urls.py =====
================================================================================

from django.conf import settings
from django.conf.urls.static import static
from django.contrib import admin
from django.http import JsonResponse
from django.urls import path, include


def healthcheck(_request):
    """Simple readiness/liveness probe used by deployment."""
    return JsonResponse({"status": "ok"})


urlpatterns = [
    path("admin/", admin.site.urls),
    # Upload gateway / ingestion endpoints
    path("api/ingest/", include("ingestion.urls")),
    # Crashdata / visualization endpoints
    path("api/crashdata/", include("crashdata.urls")),
    # Analysis / model endpoints
    path("api/", include("analysis.urls")),
    # Healthcheck
    path("health/", healthcheck, name="healthcheck"),
    # Frontend SPA
    path("", include("frontend.urls")),
]

if settings.DEBUG:
    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)

================================================================================
===== END FILE: alaska_project\urls.py =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_project\wsgi.py =====
================================================================================

import os
from django.core.wsgi import get_wsgi_application

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "alaska_project.settings")

application = get_wsgi_application()

================================================================================
===== END FILE: alaska_project\wsgi.py =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_ui\package-lock.json =====
================================================================================

{
  "name": "alaska-car-crash-analysis-ui",
  "version": "0.1.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "alaska-car-crash-analysis-ui",
      "version": "0.1.0",
      "dependencies": {
        "react": "^18.3.1",
        "react-dom": "^18.3.1"
      },
      "devDependencies": {
        "@types/react": "^18.3.3",
        "@types/react-dom": "^18.3.3",
        "@vitejs/plugin-react-swc": "^3.7.1",
        "typescript": "^5.5.0",
        "vite": "^5.4.0"
      }
    },
    "node_modules/@esbuild/aix-ppc64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.21.5.tgz",
      "integrity": "sha512-1SDgH6ZSPTlggy1yI6+Dbkiz8xzpHJEVAlF/AM1tHPLsf5STom9rwtjE4hKAF20FfXXNTFqEYXyJNWh1GiZedQ==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "aix"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/android-arm": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.21.5.tgz",
      "integrity": "sha512-vCPvzSjpPHEi1siZdlvAlsPxXl7WbOVUBBAowWug4rJHb68Ox8KualB+1ocNvT5fjv6wpkX6o/iEpbDrf68zcg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/android-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.21.5.tgz",
      "integrity": "sha512-c0uX9VAUBQ7dTDCjq+wdyGLowMdtR/GoC2U5IYk/7D1H1JYC0qseD7+11iMP2mRLN9RcCMRcjC4YMclCzGwS/A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/android-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.21.5.tgz",
      "integrity": "sha512-D7aPRUUNHRBwHxzxRvp856rjUHRFW1SdQATKXH2hqA0kAZb1hKmi02OpYRacl0TxIGz/ZmXWlbZgjwWYaCakTA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/darwin-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.21.5.tgz",
      "integrity": "sha512-DwqXqZyuk5AiWWf3UfLiRDJ5EDd49zg6O9wclZ7kUMv2WRFr4HKjXp/5t8JZ11QbQfUS6/cRCKGwYhtNAY88kQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/darwin-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.21.5.tgz",
      "integrity": "sha512-se/JjF8NlmKVG4kNIuyWMV/22ZaerB+qaSi5MdrXtd6R08kvs2qCN4C09miupktDitvh8jRFflwGFBQcxZRjbw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/freebsd-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.21.5.tgz",
      "integrity": "sha512-5JcRxxRDUJLX8JXp/wcBCy3pENnCgBR9bN6JsY4OmhfUtIHe3ZW0mawA7+RDAcMLrMIZaf03NlQiX9DGyB8h4g==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/freebsd-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.21.5.tgz",
      "integrity": "sha512-J95kNBj1zkbMXtHVH29bBriQygMXqoVQOQYA+ISs0/2l3T9/kj42ow2mpqerRBxDJnmkUDCaQT/dfNXWX/ZZCQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-arm": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.21.5.tgz",
      "integrity": "sha512-bPb5AHZtbeNGjCKVZ9UGqGwo8EUu4cLq68E95A53KlxAPRmUyYv2D6F0uUI65XisGOL1hBP5mTronbgo+0bFcA==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.21.5.tgz",
      "integrity": "sha512-ibKvmyYzKsBeX8d8I7MH/TMfWDXBF3db4qM6sy+7re0YXya+K1cem3on9XgdT2EQGMu4hQyZhan7TeQ8XkGp4Q==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-ia32": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.21.5.tgz",
      "integrity": "sha512-YvjXDqLRqPDl2dvRODYmmhz4rPeVKYvppfGYKSNGdyZkA01046pLWyRKKI3ax8fbJoK5QbxblURkwK/MWY18Tg==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-loong64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.21.5.tgz",
      "integrity": "sha512-uHf1BmMG8qEvzdrzAqg2SIG/02+4/DHB6a9Kbya0XDvwDEKCoC8ZRWI5JJvNdUjtciBGFQ5PuBlpEOXQj+JQSg==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-mips64el": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.21.5.tgz",
      "integrity": "sha512-IajOmO+KJK23bj52dFSNCMsz1QP1DqM6cwLUv3W1QwyxkyIWecfafnI555fvSGqEKwjMXVLokcV5ygHW5b3Jbg==",
      "cpu": [
        "mips64el"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-ppc64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.21.5.tgz",
      "integrity": "sha512-1hHV/Z4OEfMwpLO8rp7CvlhBDnjsC3CttJXIhBi+5Aj5r+MBvy4egg7wCbe//hSsT+RvDAG7s81tAvpL2XAE4w==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-riscv64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.21.5.tgz",
      "integrity": "sha512-2HdXDMd9GMgTGrPWnJzP2ALSokE/0O5HhTUvWIbD3YdjME8JwvSCnNGBnTThKGEB91OZhzrJ4qIIxk/SBmyDDA==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-s390x": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.21.5.tgz",
      "integrity": "sha512-zus5sxzqBJD3eXxwvjN1yQkRepANgxE9lgOW2qLnmr8ikMTphkjgXu1HR01K4FJg8h1kEEDAqDcZQtbrRnB41A==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.21.5.tgz",
      "integrity": "sha512-1rYdTpyv03iycF1+BhzrzQJCdOuAOtaqHTWJZCWvijKD2N5Xu0TtVC8/+1faWqcP9iBCWOmjmhoH94dH82BxPQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/netbsd-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.21.5.tgz",
      "integrity": "sha512-Woi2MXzXjMULccIwMnLciyZH4nCIMpWQAs049KEeMvOcNADVxo0UBIQPfSmxB3CWKedngg7sWZdLvLczpe0tLg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/openbsd-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.21.5.tgz",
      "integrity": "sha512-HLNNw99xsvx12lFBUwoT8EVCsSvRNDVxNpjZ7bPn947b8gJPzeHWyNVhFsaerc0n3TsbOINvRP2byTZ5LKezow==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/sunos-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.21.5.tgz",
      "integrity": "sha512-6+gjmFpfy0BHU5Tpptkuh8+uw3mnrvgs+dSPQXQOv3ekbordwnzTVEb4qnIvQcYXq6gzkyTnoZ9dZG+D4garKg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "sunos"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/win32-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.21.5.tgz",
      "integrity": "sha512-Z0gOTd75VvXqyq7nsl93zwahcTROgqvuAcYDUr+vOv8uHhNSKROyU961kgtCD1e95IqPKSQKH7tBTslnS3tA8A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/win32-ia32": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.21.5.tgz",
      "integrity": "sha512-SWXFF1CL2RVNMaVs+BBClwtfZSvDgtL//G/smwAc5oVK/UPu2Gu9tIaRgFmYFFKrmg3SyAjSrElf0TiJ1v8fYA==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/win32-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.21.5.tgz",
      "integrity": "sha512-tQd/1efJuzPC6rCFwEvLtci/xNFcTZknmXs98FYDfGE4wP9ClFV98nyKrzJKVPMhdDnjzLhdUyMX4PsQAPjwIw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@rolldown/pluginutils": {
      "version": "1.0.0-beta.27",
      "resolved": "https://registry.npmjs.org/@rolldown/pluginutils/-/pluginutils-1.0.0-beta.27.tgz",
      "integrity": "sha512-+d0F4MKMCbeVUJwG96uQ4SgAznZNSq93I3V+9NHA4OpvqG8mRCpGdKmK8l/dl02h2CCDHwW2FqilnTyDcAnqjA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@rollup/rollup-android-arm-eabi": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm-eabi/-/rollup-android-arm-eabi-4.53.3.tgz",
      "integrity": "sha512-mRSi+4cBjrRLoaal2PnqH82Wqyb+d3HsPUN/W+WslCXsZsyHa9ZeQQX/pQsZaVIWDkPcpV6jJ+3KLbTbgnwv8w==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ]
    },
    "node_modules/@rollup/rollup-android-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm64/-/rollup-android-arm64-4.53.3.tgz",
      "integrity": "sha512-CbDGaMpdE9sh7sCmTrTUyllhrg65t6SwhjlMJsLr+J8YjFuPmCEjbBSx4Z/e4SmDyH3aB5hGaJUP2ltV/vcs4w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ]
    },
    "node_modules/@rollup/rollup-darwin-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.53.3.tgz",
      "integrity": "sha512-Nr7SlQeqIBpOV6BHHGZgYBuSdanCXuw09hon14MGOLGmXAFYjx1wNvquVPmpZnl0tLjg25dEdr4IQ6GgyToCUA==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ]
    },
    "node_modules/@rollup/rollup-darwin-x64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-x64/-/rollup-darwin-x64-4.53.3.tgz",
      "integrity": "sha512-DZ8N4CSNfl965CmPktJ8oBnfYr3F8dTTNBQkRlffnUarJ2ohudQD17sZBa097J8xhQ26AwhHJ5mvUyQW8ddTsQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ]
    },
    "node_modules/@rollup/rollup-freebsd-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-arm64/-/rollup-freebsd-arm64-4.53.3.tgz",
      "integrity": "sha512-yMTrCrK92aGyi7GuDNtGn2sNW+Gdb4vErx4t3Gv/Tr+1zRb8ax4z8GWVRfr3Jw8zJWvpGHNpss3vVlbF58DZ4w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ]
    },
    "node_modules/@rollup/rollup-freebsd-x64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-x64/-/rollup-freebsd-x64-4.53.3.tgz",
      "integrity": "sha512-lMfF8X7QhdQzseM6XaX0vbno2m3hlyZFhwcndRMw8fbAGUGL3WFMBdK0hbUBIUYcEcMhVLr1SIamDeuLBnXS+Q==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm-gnueabihf": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-gnueabihf/-/rollup-linux-arm-gnueabihf-4.53.3.tgz",
      "integrity": "sha512-k9oD15soC/Ln6d2Wv/JOFPzZXIAIFLp6B+i14KhxAfnq76ajt0EhYc5YPeX6W1xJkAdItcVT+JhKl1QZh44/qw==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm-musleabihf": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-musleabihf/-/rollup-linux-arm-musleabihf-4.53.3.tgz",
      "integrity": "sha512-vTNlKq+N6CK/8UktsrFuc+/7NlEYVxgaEgRXVUVK258Z5ymho29skzW1sutgYjqNnquGwVUObAaxae8rZ6YMhg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-gnu/-/rollup-linux-arm64-gnu-4.53.3.tgz",
      "integrity": "sha512-RGrFLWgMhSxRs/EWJMIFM1O5Mzuz3Xy3/mnxJp/5cVhZ2XoCAxJnmNsEyeMJtpK+wu0FJFWz+QF4mjCA7AUQ3w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm64-musl": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-musl/-/rollup-linux-arm64-musl-4.53.3.tgz",
      "integrity": "sha512-kASyvfBEWYPEwe0Qv4nfu6pNkITLTb32p4yTgzFCocHnJLAHs+9LjUu9ONIhvfT/5lv4YS5muBHyuV84epBo/A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-loong64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-loong64-gnu/-/rollup-linux-loong64-gnu-4.53.3.tgz",
      "integrity": "sha512-JiuKcp2teLJwQ7vkJ95EwESWkNRFJD7TQgYmCnrPtlu50b4XvT5MOmurWNrCj3IFdyjBQ5p9vnrX4JM6I8OE7g==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-ppc64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-ppc64-gnu/-/rollup-linux-ppc64-gnu-4.53.3.tgz",
      "integrity": "sha512-EoGSa8nd6d3T7zLuqdojxC20oBfNT8nexBbB/rkxgKj5T5vhpAQKKnD+h3UkoMuTyXkP5jTjK/ccNRmQrPNDuw==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-riscv64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-gnu/-/rollup-linux-riscv64-gnu-4.53.3.tgz",
      "integrity": "sha512-4s+Wped2IHXHPnAEbIB0YWBv7SDohqxobiiPA1FIWZpX+w9o2i4LezzH/NkFUl8LRci/8udci6cLq+jJQlh+0g==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-riscv64-musl": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-musl/-/rollup-linux-riscv64-musl-4.53.3.tgz",
      "integrity": "sha512-68k2g7+0vs2u9CxDt5ktXTngsxOQkSEV/xBbwlqYcUrAVh6P9EgMZvFsnHy4SEiUl46Xf0IObWVbMvPrr2gw8A==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-s390x-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-s390x-gnu/-/rollup-linux-s390x-gnu-4.53.3.tgz",
      "integrity": "sha512-VYsFMpULAz87ZW6BVYw3I6sWesGpsP9OPcyKe8ofdg9LHxSbRMd7zrVrr5xi/3kMZtpWL/wC+UIJWJYVX5uTKg==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-x64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-gnu/-/rollup-linux-x64-gnu-4.53.3.tgz",
      "integrity": "sha512-3EhFi1FU6YL8HTUJZ51imGJWEX//ajQPfqWLI3BQq4TlvHy4X0MOr5q3D2Zof/ka0d5FNdPwZXm3Yyib/UEd+w==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-x64-musl": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-musl/-/rollup-linux-x64-musl-4.53.3.tgz",
      "integrity": "sha512-eoROhjcc6HbZCJr+tvVT8X4fW3/5g/WkGvvmwz/88sDtSJzO7r/blvoBDgISDiCjDRZmHpwud7h+6Q9JxFwq1Q==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-openharmony-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-openharmony-arm64/-/rollup-openharmony-arm64-4.53.3.tgz",
      "integrity": "sha512-OueLAWgrNSPGAdUdIjSWXw+u/02BRTcnfw9PN41D2vq/JSEPnJnVuBgw18VkN8wcd4fjUs+jFHVM4t9+kBSNLw==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openharmony"
      ]
    },
    "node_modules/@rollup/rollup-win32-arm64-msvc": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-arm64-msvc/-/rollup-win32-arm64-msvc-4.53.3.tgz",
      "integrity": "sha512-GOFuKpsxR/whszbF/bzydebLiXIHSgsEUp6M0JI8dWvi+fFa1TD6YQa4aSZHtpmh2/uAlj/Dy+nmby3TJ3pkTw==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-ia32-msvc": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-ia32-msvc/-/rollup-win32-ia32-msvc-4.53.3.tgz",
      "integrity": "sha512-iah+THLcBJdpfZ1TstDFbKNznlzoxa8fmnFYK4V67HvmuNYkVdAywJSoteUszvBQ9/HqN2+9AZghbajMsFT+oA==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-x64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-x64-gnu/-/rollup-win32-x64-gnu-4.53.3.tgz",
      "integrity": "sha512-J9QDiOIZlZLdcot5NXEepDkstocktoVjkaKUtqzgzpt2yWjGlbYiKyp05rWwk4nypbYUNoFAztEgixoLaSETkg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-x64-msvc": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-x64-msvc/-/rollup-win32-x64-msvc-4.53.3.tgz",
      "integrity": "sha512-UhTd8u31dXadv0MopwGgNOBpUVROFKWVQgAg5N1ESyCz8AuBcMqm4AuTjrwgQKGDfoFuz02EuMRHQIw/frmYKQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@swc/core": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core/-/core-1.15.3.tgz",
      "integrity": "sha512-Qd8eBPkUFL4eAONgGjycZXj1jFCBW8Fd+xF0PzdTlBCWQIV1xnUT7B93wUANtW3KGjl3TRcOyxwSx/u/jyKw/Q==",
      "dev": true,
      "hasInstallScript": true,
      "license": "Apache-2.0",
      "dependencies": {
        "@swc/counter": "^0.1.3",
        "@swc/types": "^0.1.25"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/swc"
      },
      "optionalDependencies": {
        "@swc/core-darwin-arm64": "1.15.3",
        "@swc/core-darwin-x64": "1.15.3",
        "@swc/core-linux-arm-gnueabihf": "1.15.3",
        "@swc/core-linux-arm64-gnu": "1.15.3",
        "@swc/core-linux-arm64-musl": "1.15.3",
        "@swc/core-linux-x64-gnu": "1.15.3",
        "@swc/core-linux-x64-musl": "1.15.3",
        "@swc/core-win32-arm64-msvc": "1.15.3",
        "@swc/core-win32-ia32-msvc": "1.15.3",
        "@swc/core-win32-x64-msvc": "1.15.3"
      },
      "peerDependencies": {
        "@swc/helpers": ">=0.5.17"
      },
      "peerDependenciesMeta": {
        "@swc/helpers": {
          "optional": true
        }
      }
    },
    "node_modules/@swc/core-darwin-arm64": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-darwin-arm64/-/core-darwin-arm64-1.15.3.tgz",
      "integrity": "sha512-AXfeQn0CvcQ4cndlIshETx6jrAM45oeUrK8YeEY6oUZU/qzz0Id0CyvlEywxkWVC81Ajpd8TQQ1fW5yx6zQWkQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "Apache-2.0 AND MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/core-darwin-x64": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-darwin-x64/-/core-darwin-x64-1.15.3.tgz",
      "integrity": "sha512-p68OeCz1ui+MZYG4wmfJGvcsAcFYb6Sl25H9TxWl+GkBgmNimIiRdnypK9nBGlqMZAcxngNPtnG3kEMNnvoJ2A==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "Apache-2.0 AND MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/core-linux-arm-gnueabihf": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-linux-arm-gnueabihf/-/core-linux-arm-gnueabihf-1.15.3.tgz",
      "integrity": "sha512-Nuj5iF4JteFgwrai97mUX+xUOl+rQRHqTvnvHMATL/l9xE6/TJfPBpd3hk/PVpClMXG3Uvk1MxUFOEzM1JrMYg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "Apache-2.0",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/core-linux-arm64-gnu": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-linux-arm64-gnu/-/core-linux-arm64-gnu-1.15.3.tgz",
      "integrity": "sha512-2Nc/s8jE6mW2EjXWxO/lyQuLKShcmTrym2LRf5Ayp3ICEMX6HwFqB1EzDhwoMa2DcUgmnZIalesq2lG3krrUNw==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "Apache-2.0 AND MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/core-linux-arm64-musl": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-linux-arm64-musl/-/core-linux-arm64-musl-1.15.3.tgz",
      "integrity": "sha512-j4SJniZ/qaZ5g8op+p1G9K1z22s/EYGg1UXIb3+Cg4nsxEpF5uSIGEE4mHUfA70L0BR9wKT2QF/zv3vkhfpX4g==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "Apache-2.0 AND MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/core-linux-x64-gnu": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-linux-x64-gnu/-/core-linux-x64-gnu-1.15.3.tgz",
      "integrity": "sha512-aKttAZnz8YB1VJwPQZtyU8Uk0BfMP63iDMkvjhJzRZVgySmqt/apWSdnoIcZlUoGheBrcqbMC17GGUmur7OT5A==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "Apache-2.0 AND MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/core-linux-x64-musl": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-linux-x64-musl/-/core-linux-x64-musl-1.15.3.tgz",
      "integrity": "sha512-oe8FctPu1gnUsdtGJRO2rvOUIkkIIaHqsO9xxN0bTR7dFTlPTGi2Fhk1tnvXeyAvCPxLIcwD8phzKg6wLv9yug==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "Apache-2.0 AND MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/core-win32-arm64-msvc": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-win32-arm64-msvc/-/core-win32-arm64-msvc-1.15.3.tgz",
      "integrity": "sha512-L9AjzP2ZQ/Xh58e0lTRMLvEDrcJpR7GwZqAtIeNLcTK7JVE+QineSyHp0kLkO1rttCHyCy0U74kDTj0dRz6raA==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "Apache-2.0 AND MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/core-win32-ia32-msvc": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-win32-ia32-msvc/-/core-win32-ia32-msvc-1.15.3.tgz",
      "integrity": "sha512-B8UtogMzErUPDWUoKONSVBdsgKYd58rRyv2sHJWKOIMCHfZ22FVXICR4O/VwIYtlnZ7ahERcjayBHDlBZpR0aw==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "Apache-2.0 AND MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/core-win32-x64-msvc": {
      "version": "1.15.3",
      "resolved": "https://registry.npmjs.org/@swc/core-win32-x64-msvc/-/core-win32-x64-msvc-1.15.3.tgz",
      "integrity": "sha512-SpZKMR9QBTecHeqpzJdYEfgw30Oo8b/Xl6rjSzBt1g0ZsXyy60KLXrp6IagQyfTYqNYE/caDvwtF2FPn7pomog==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "Apache-2.0 AND MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@swc/counter": {
      "version": "0.1.3",
      "resolved": "https://registry.npmjs.org/@swc/counter/-/counter-0.1.3.tgz",
      "integrity": "sha512-e2BR4lsJkkRlKZ/qCHPw9ZaSxc0MVUd7gtbtaB7aMvHeJVYe8sOB8DBZkP2DtISHGSku9sCK6T6cnY0CtXrOCQ==",
      "dev": true,
      "license": "Apache-2.0"
    },
    "node_modules/@swc/types": {
      "version": "0.1.25",
      "resolved": "https://registry.npmjs.org/@swc/types/-/types-0.1.25.tgz",
      "integrity": "sha512-iAoY/qRhNH8a/hBvm3zKj9qQ4oc2+3w1unPJa2XvTK3XjeLXtzcCingVPw/9e5mn1+0yPqxcBGp9Jf0pkfMb1g==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "@swc/counter": "^0.1.3"
      }
    },
    "node_modules/@types/estree": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/@types/estree/-/estree-1.0.8.tgz",
      "integrity": "sha512-dWHzHa2WqEXI/O1E9OjrocMTKJl2mSrEolh1Iomrv6U+JuNwaHXsXx9bLu5gG7BUWFIN0skIQJQ/L1rIex4X6w==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/prop-types": {
      "version": "15.7.15",
      "resolved": "https://registry.npmjs.org/@types/prop-types/-/prop-types-15.7.15.tgz",
      "integrity": "sha512-F6bEyamV9jKGAFBEmlQnesRPGOQqS2+Uwi0Em15xenOxHaf2hv6L8YCVn3rPdPJOiJfPiCnLIRyvwVaqMY3MIw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/react": {
      "version": "18.3.27",
      "resolved": "https://registry.npmjs.org/@types/react/-/react-18.3.27.tgz",
      "integrity": "sha512-cisd7gxkzjBKU2GgdYrTdtQx1SORymWyaAFhaxQPK9bYO9ot3Y5OikQRvY0VYQtvwjeQnizCINJAenh/V7MK2w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/prop-types": "*",
        "csstype": "^3.2.2"
      }
    },
    "node_modules/@types/react-dom": {
      "version": "18.3.7",
      "resolved": "https://registry.npmjs.org/@types/react-dom/-/react-dom-18.3.7.tgz",
      "integrity": "sha512-MEe3UeoENYVFXzoXEWsvcpg6ZvlrFNlOQ7EOsvhI3CfAXwzPfO8Qwuxd40nepsYKqyyVQnTdEfv68q91yLcKrQ==",
      "dev": true,
      "license": "MIT",
      "peerDependencies": {
        "@types/react": "^18.0.0"
      }
    },
    "node_modules/@vitejs/plugin-react-swc": {
      "version": "3.11.0",
      "resolved": "https://registry.npmjs.org/@vitejs/plugin-react-swc/-/plugin-react-swc-3.11.0.tgz",
      "integrity": "sha512-YTJCGFdNMHCMfjODYtxRNVAYmTWQ1Lb8PulP/2/f/oEEtglw8oKxKIZmmRkyXrVrHfsKOaVkAc3NT9/dMutO5w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@rolldown/pluginutils": "1.0.0-beta.27",
        "@swc/core": "^1.12.11"
      },
      "peerDependencies": {
        "vite": "^4 || ^5 || ^6 || ^7"
      }
    },
    "node_modules/csstype": {
      "version": "3.2.3",
      "resolved": "https://registry.npmjs.org/csstype/-/csstype-3.2.3.tgz",
      "integrity": "sha512-z1HGKcYy2xA8AGQfwrn0PAy+PB7X/GSj3UVJW9qKyn43xWa+gl5nXmU4qqLMRzWVLFC8KusUX8T/0kCiOYpAIQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/esbuild": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.21.5.tgz",
      "integrity": "sha512-mg3OPMV4hXywwpoDxu3Qda5xCKQi+vCTZq8S9J/EpkhB2HzKXq4SNFZE3+NK93JYxc8VMSep+lOUSC/RVKaBqw==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "bin": {
        "esbuild": "bin/esbuild"
      },
      "engines": {
        "node": ">=12"
      },
      "optionalDependencies": {
        "@esbuild/aix-ppc64": "0.21.5",
        "@esbuild/android-arm": "0.21.5",
        "@esbuild/android-arm64": "0.21.5",
        "@esbuild/android-x64": "0.21.5",
        "@esbuild/darwin-arm64": "0.21.5",
        "@esbuild/darwin-x64": "0.21.5",
        "@esbuild/freebsd-arm64": "0.21.5",
        "@esbuild/freebsd-x64": "0.21.5",
        "@esbuild/linux-arm": "0.21.5",
        "@esbuild/linux-arm64": "0.21.5",
        "@esbuild/linux-ia32": "0.21.5",
        "@esbuild/linux-loong64": "0.21.5",
        "@esbuild/linux-mips64el": "0.21.5",
        "@esbuild/linux-ppc64": "0.21.5",
        "@esbuild/linux-riscv64": "0.21.5",
        "@esbuild/linux-s390x": "0.21.5",
        "@esbuild/linux-x64": "0.21.5",
        "@esbuild/netbsd-x64": "0.21.5",
        "@esbuild/openbsd-x64": "0.21.5",
        "@esbuild/sunos-x64": "0.21.5",
        "@esbuild/win32-arm64": "0.21.5",
        "@esbuild/win32-ia32": "0.21.5",
        "@esbuild/win32-x64": "0.21.5"
      }
    },
    "node_modules/fsevents": {
      "version": "2.3.3",
      "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz",
      "integrity": "sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": "^8.16.0 || ^10.6.0 || >=11.0.0"
      }
    },
    "node_modules/js-tokens": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz",
      "integrity": "sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==",
      "license": "MIT"
    },
    "node_modules/loose-envify": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/loose-envify/-/loose-envify-1.4.0.tgz",
      "integrity": "sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==",
      "license": "MIT",
      "dependencies": {
        "js-tokens": "^3.0.0 || ^4.0.0"
      },
      "bin": {
        "loose-envify": "cli.js"
      }
    },
    "node_modules/nanoid": {
      "version": "3.3.11",
      "resolved": "https://registry.npmjs.org/nanoid/-/nanoid-3.3.11.tgz",
      "integrity": "sha512-N8SpfPUnUp1bK+PMYW8qSWdl9U+wwNWI4QKxOYDy9JAro3WMX7p2OeVRF9v+347pnakNevPmiHhNmZ2HbFA76w==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "bin": {
        "nanoid": "bin/nanoid.cjs"
      },
      "engines": {
        "node": "^10 || ^12 || ^13.7 || ^14 || >=15.0.1"
      }
    },
    "node_modules/picocolors": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/picocolors/-/picocolors-1.1.1.tgz",
      "integrity": "sha512-xceH2snhtb5M9liqDsmEw56le376mTZkEX/jEb/RxNFyegNul7eNslCXP9FDj/Lcu0X8KEyMceP2ntpaHrDEVA==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/postcss": {
      "version": "8.5.6",
      "resolved": "https://registry.npmjs.org/postcss/-/postcss-8.5.6.tgz",
      "integrity": "sha512-3Ybi1tAuwAP9s0r1UQ2J4n5Y0G05bJkpUIO0/bI9MhwmD70S5aTWbXGBwxHrelT+XM1k6dM0pk+SwNkpTRN7Pg==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/postcss/"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/postcss"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "nanoid": "^3.3.11",
        "picocolors": "^1.1.1",
        "source-map-js": "^1.2.1"
      },
      "engines": {
        "node": "^10 || ^12 || >=14"
      }
    },
    "node_modules/react": {
      "version": "18.3.1",
      "resolved": "https://registry.npmjs.org/react/-/react-18.3.1.tgz",
      "integrity": "sha512-wS+hAgJShR0KhEvPJArfuPVN1+Hz1t0Y6n5jLrGQbkb4urgPE/0Rve+1kMB1v/oWgHgm4WIcV+i7F2pTVj+2iQ==",
      "license": "MIT",
      "dependencies": {
        "loose-envify": "^1.1.0"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/react-dom": {
      "version": "18.3.1",
      "resolved": "https://registry.npmjs.org/react-dom/-/react-dom-18.3.1.tgz",
      "integrity": "sha512-5m4nQKp+rZRb09LNH59GM4BxTh9251/ylbKIbpe7TpGxfJ+9kv6BLkLBXIjjspbgbnIBNqlI23tRnTWT0snUIw==",
      "license": "MIT",
      "dependencies": {
        "loose-envify": "^1.1.0",
        "scheduler": "^0.23.2"
      },
      "peerDependencies": {
        "react": "^18.3.1"
      }
    },
    "node_modules/rollup": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/rollup/-/rollup-4.53.3.tgz",
      "integrity": "sha512-w8GmOxZfBmKknvdXU1sdM9NHcoQejwF/4mNgj2JuEEdRaHwwF12K7e9eXn1nLZ07ad+du76mkVsyeb2rKGllsA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/estree": "1.0.8"
      },
      "bin": {
        "rollup": "dist/bin/rollup"
      },
      "engines": {
        "node": ">=18.0.0",
        "npm": ">=8.0.0"
      },
      "optionalDependencies": {
        "@rollup/rollup-android-arm-eabi": "4.53.3",
        "@rollup/rollup-android-arm64": "4.53.3",
        "@rollup/rollup-darwin-arm64": "4.53.3",
        "@rollup/rollup-darwin-x64": "4.53.3",
        "@rollup/rollup-freebsd-arm64": "4.53.3",
        "@rollup/rollup-freebsd-x64": "4.53.3",
        "@rollup/rollup-linux-arm-gnueabihf": "4.53.3",
        "@rollup/rollup-linux-arm-musleabihf": "4.53.3",
        "@rollup/rollup-linux-arm64-gnu": "4.53.3",
        "@rollup/rollup-linux-arm64-musl": "4.53.3",
        "@rollup/rollup-linux-loong64-gnu": "4.53.3",
        "@rollup/rollup-linux-ppc64-gnu": "4.53.3",
        "@rollup/rollup-linux-riscv64-gnu": "4.53.3",
        "@rollup/rollup-linux-riscv64-musl": "4.53.3",
        "@rollup/rollup-linux-s390x-gnu": "4.53.3",
        "@rollup/rollup-linux-x64-gnu": "4.53.3",
        "@rollup/rollup-linux-x64-musl": "4.53.3",
        "@rollup/rollup-openharmony-arm64": "4.53.3",
        "@rollup/rollup-win32-arm64-msvc": "4.53.3",
        "@rollup/rollup-win32-ia32-msvc": "4.53.3",
        "@rollup/rollup-win32-x64-gnu": "4.53.3",
        "@rollup/rollup-win32-x64-msvc": "4.53.3",
        "fsevents": "~2.3.2"
      }
    },
    "node_modules/scheduler": {
      "version": "0.23.2",
      "resolved": "https://registry.npmjs.org/scheduler/-/scheduler-0.23.2.tgz",
      "integrity": "sha512-UOShsPwz7NrMUqhR6t0hWjFduvOzbtv7toDH1/hIrfRNIDBnnBWd0CwJTGvTpngVlmwGCdP9/Zl/tVrDqcuYzQ==",
      "license": "MIT",
      "dependencies": {
        "loose-envify": "^1.1.0"
      }
    },
    "node_modules/source-map-js": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/source-map-js/-/source-map-js-1.2.1.tgz",
      "integrity": "sha512-UXWMKhLOwVKb728IUtQPXxfYU+usdybtUrK/8uGE8CQMvrhOpwvzDBwj0QhSL7MQc7vIsISBG8VQ8+IDQxpfQA==",
      "dev": true,
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/typescript": {
      "version": "5.9.3",
      "resolved": "https://registry.npmjs.org/typescript/-/typescript-5.9.3.tgz",
      "integrity": "sha512-jl1vZzPDinLr9eUt3J/t7V6FgNEw9QjvBPdysz9KfQDD41fQrC2Y4vKQdiaUpFT4bXlb1RHhLpp8wtm6M5TgSw==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "tsc": "bin/tsc",
        "tsserver": "bin/tsserver"
      },
      "engines": {
        "node": ">=14.17"
      }
    },
    "node_modules/vite": {
      "version": "5.4.21",
      "resolved": "https://registry.npmjs.org/vite/-/vite-5.4.21.tgz",
      "integrity": "sha512-o5a9xKjbtuhY6Bi5S3+HvbRERmouabWbyUcpXXUA1u+GNUKoROi9byOJ8M0nHbHYHkYICiMlqxkg1KkYmm25Sw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "esbuild": "^0.21.3",
        "postcss": "^8.4.43",
        "rollup": "^4.20.0"
      },
      "bin": {
        "vite": "bin/vite.js"
      },
      "engines": {
        "node": "^18.0.0 || >=20.0.0"
      },
      "funding": {
        "url": "https://github.com/vitejs/vite?sponsor=1"
      },
      "optionalDependencies": {
        "fsevents": "~2.3.3"
      },
      "peerDependencies": {
        "@types/node": "^18.0.0 || >=20.0.0",
        "less": "*",
        "lightningcss": "^1.21.0",
        "sass": "*",
        "sass-embedded": "*",
        "stylus": "*",
        "sugarss": "*",
        "terser": "^5.4.0"
      },
      "peerDependenciesMeta": {
        "@types/node": {
          "optional": true
        },
        "less": {
          "optional": true
        },
        "lightningcss": {
          "optional": true
        },
        "sass": {
          "optional": true
        },
        "sass-embedded": {
          "optional": true
        },
        "stylus": {
          "optional": true
        },
        "sugarss": {
          "optional": true
        },
        "terser": {
          "optional": true
        }
      }
    }
  }
}

================================================================================
===== END FILE: alaska_ui\package-lock.json =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_ui\package.json =====
================================================================================

{
  "name": "alaska-car-crash-analysis-ui",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "react": "^18.3.1",
    "react-dom": "^18.3.1"
  },
  "devDependencies": {
    "@types/react": "^18.3.3",
    "@types/react-dom": "^18.3.3",
    "@vitejs/plugin-react-swc": "^3.7.1",
    "typescript": "^5.5.0",
    "vite": "^5.4.0"
  }
}

================================================================================
===== END FILE: alaska_ui\package.json =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_ui\src\design\Checklist.md =====
================================================================================


================================================================================
===== END FILE: alaska_ui\src\design\Checklist.md =====
================================================================================

================================================================================
===== BEGIN FILE: alaska_ui\tsconfig.json =====
================================================================================

{
  "compilerOptions": {
    "target": "ESNext",
    "useDefineForClassFields": true,
    "lib": ["DOM", "DOM.Iterable", "ESNext"],
    "allowJs": false,
    "skipLibCheck": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "module": "ESNext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx"
  },
  "include": ["src"]
}

================================================================================
===== END FILE: alaska_ui\tsconfig.json =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\__init__.py =====
================================================================================


================================================================================
===== END FILE: analysis\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\admin.py =====
================================================================================

from django.contrib import admin

from .models import ModelJob


@admin.register(ModelJob)
class ModelJobAdmin(admin.ModelAdmin):
    list_display = (
        "id",
        "model_name",
        "status",
        "owner",
        "upload",
        "created_at",
    )
    list_filter = ("status", "model_name", "created_at")
    search_fields = ("id", "owner__username", "upload__id", "model_name")

================================================================================
===== END FILE: analysis\admin.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\apps.py =====
================================================================================

from django.apps import AppConfig


class AnalysisConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "analysis"

================================================================================
===== END FILE: analysis\apps.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\management\__init__.py =====
================================================================================


================================================================================
===== END FILE: analysis\management\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\management\commands\__init__.py =====
================================================================================


================================================================================
===== END FILE: analysis\management\commands\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\management\commands\run_model_jobs.py =====
================================================================================


from __future__ import annotations

import logging
from typing import Any

from django.core.management.base import BaseCommand

from analysis.ml_core.worker import run_next_queued_job

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = (
        "Process ModelJob instances with status='queued'. "
        "By default this command processes a single job and exits. "
        "Use --loop to keep polling for new jobs."
    )

    def add_arguments(self, parser) -> None:
        parser.add_argument(
            "--loop",
            action="store_true",
            help="Continuously poll for queued jobs instead of exiting "
            "after processing a single job.",
        )
        parser.add_argument(
            "--sleep",
            type=int,
            default=5,
            help="Number of seconds to sleep between polling iterations "
            "when --loop is enabled (default: 5).",
        )

    def handle(self, *args: Any, **options: Any) -> None:
        loop = options["loop"]
        sleep_seconds = options["sleep"]

        import time

        if not loop:
            job_id = run_next_queued_job()
            if job_id:
                self.stdout.write(self.style.SUCCESS(f"Processed ModelJob {job_id}"))
            else:
                self.stdout.write("No queued ModelJob instances to process.")
            return

        self.stdout.write(
            self.style.WARNING(
                f"Entering polling loop (sleep={sleep_seconds}s). "
                "Press Ctrl+C to stop."
            )
        )

        try:
            while True:
                job_id = run_next_queued_job()
                if job_id:
                    self.stdout.write(
                        self.style.SUCCESS(f"Processed ModelJob {job_id}")
                    )
                else:
                    logger.debug("No queued jobs found, sleeping for %s seconds.", sleep_seconds)
                time.sleep(sleep_seconds)
        except KeyboardInterrupt:
            self.stdout.write(self.style.WARNING("Stopping run_model_jobs loop."))

================================================================================
===== END FILE: analysis\management\commands\run_model_jobs.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\ml_core\__init__.py =====
================================================================================

"""Core ML utilities for the analysis app.

This package exposes cleaning utilities, model training helpers,
and worker/management-command glue used by the model API.
"""

================================================================================
===== END FILE: analysis\ml_core\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\ml_core\cleaning.py =====
================================================================================

from __future__ import annotations

import logging
import re
from typing import Any, Dict, Iterable, Mapping, Sequence, Set, Tuple

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Configuration values (adapted from the ML team's DataCleaning/config.py)
# ---------------------------------------------------------------------------

UNKNOWN_THRESHOLD: float = 10.0  # percentage threshold for unknowns in a column
YES_NO_THRESHOLD: float = 1.0    # percentage threshold for yes/no balance in a column

# Base set of tokens that should always be treated as "unknown"
DEFAULT_UNKNOWN_STRINGS: Set[str] = {
    "unknown",
    "missing",
    "unspecified",
    "not specified",
    "not applicable",
    "n/a",
    "na",
    "null",
    "blank",
    "tbd",
    "tba",
    "to be determined",
    "refused",
    "prefer not to say",
    "no data",
    "no value",
}

# Generic substrings we treat as "unknown-like" when scanning free-text values.
GENERIC_UNKNOWN_SUBSTRINGS: Set[str] = {
    "unknown",
    "missing",
    "not specified",
    "not applicable",
    "n/a",
    "none",
    "unspecified",
    "tbd",
    "tba",
    "no data",
    "no value",
    "refused",
    "prefer not to say",
}


def validate_config_values(
    unknown_threshold: float = UNKNOWN_THRESHOLD,
    yes_no_threshold: float = YES_NO_THRESHOLD,
) -> None:
    """Validate percentage thresholds are in the expected 0100 range."""

    def _ok(x: float) -> bool:
        return isinstance(x, (int, float)) and 0.0 <= float(x) <= 100.0

    if not _ok(unknown_threshold):
        raise ValueError(
            f"UNKNOWN_THRESHOLD must be 0100, got {unknown_threshold}"
        )
    if not _ok(yes_no_threshold):
        raise ValueError(
            f"YES_NO_THRESHOLD must be 0100, got {yes_no_threshold}"
        )


# ---------------------------------------------------------------------------
# Unknown discovery (adapted from DataCleaning/unknown_discovery.py)
# ---------------------------------------------------------------------------


def discover_unknown_placeholders(
    df: pd.DataFrame,
    base_unknowns: Iterable[str] | None = None,
    generic_substrings: Iterable[str] | None = None,
    min_count: int = 2,
    max_token_length: int = 80,
) -> Set[str]:
    """Scan a DataFrame and automatically augment the set of 'unknown' tokens.

    The original ML script used simple heuristics:
    - focus on string / object columns,
    - look for values that contain generic unknown-like substrings,
    - only keep tokens that appear at least ``min_count`` times.
    """

    if base_unknowns is None:
        base_unknowns = DEFAULT_UNKNOWN_STRINGS
    if generic_substrings is None:
        generic_substrings = GENERIC_UNKNOWN_SUBSTRINGS

    known_unknowns: Set[str] = {
        str(v).strip().lower() for v in base_unknowns if isinstance(v, str)
    }
    counts: Dict[str, int] = {}

    for col in df.columns:
        series = df[col]
        if not (
            pd.api.types.is_object_dtype(series)
            or pd.api.types.is_string_dtype(series)
        ):
            continue

        for raw in series.dropna().unique():
            text = str(raw).strip()
            if not text:
                continue

            lower = text.lower()
            if lower in known_unknowns:
                continue
            if len(lower) > max_token_length:
                continue

            if any(sub in lower for sub in generic_substrings):
                counts[lower] = counts.get(lower, 0) + 1

    new_unknowns = {tok for tok, c in counts.items() if c >= min_count}
    augmented: Set[str] = set(known_unknowns)
    augmented.update(new_unknowns)
    return augmented


# ---------------------------------------------------------------------------
# Column profiling (adapted from DataCleaning/Script to Clean.py)
# ---------------------------------------------------------------------------


def percent_unknowns_per_column(
    df: pd.DataFrame,
    unknown_strings: Iterable[str],
) -> Dict[str, float]:
    """Return {column -> percent of cells that are 'unknown'}."""
    unknown_set = {str(u).lower() for u in unknown_strings}
    result: Dict[str, float] = {}

    for col in df.columns:
        series = df[col]
        total = len(series)
        if total == 0:
            result[col] = 0.0
            continue

        if (
            pd.api.types.is_object_dtype(series)
            or pd.api.types.is_string_dtype(series)
        ):
            lower = series.astype(str).str.lower()
            mask_unknown = lower.isin(unknown_set)
        else:
            mask_unknown = pd.Series([False] * total, index=series.index)

        pct = 100.0 * mask_unknown.sum() / float(total)
        result[col] = float(pct)

    return result


def yes_no(df: pd.DataFrame, unknown_strings: Iterable[str]) -> Dict[str, Dict[str, float]]:
    """Find columns that behave like Yes/No flags and compute coverage stats.

    Returns a dict of the form::

        {
            "col_name": {
                "yes_pct": float,
                "no_pct": float,
                "yesno_total": int,   # number of non-unknown yes/no values
            },
            ...
        }
    """
    unknown_set = {str(u).lower() for u in unknown_strings}
    stats: Dict[str, Dict[str, float]] = {}

    yes_like = {"yes", "y", "true", "t", "1"}
    no_like = {"no", "n", "false", "f", "0"}

    for col in df.columns:
        series = df[col]
        if not (
            pd.api.types.is_object_dtype(series)
            or pd.api.types.is_string_dtype(series)
        ):
            continue

        lower = series.astype(str).str.lower()
        mask_unknown = lower.isin(unknown_set)
        known = lower[~mask_unknown]

        if known.empty:
            continue

        yes_mask = known.isin(yes_like)
        no_mask = known.isin(no_like)

        yes_count = int(yes_mask.sum())
        no_count = int(no_mask.sum())

        total_yesno = yes_count + no_count
        total_known = len(known)

        if total_yesno == 0:
            continue

        yes_pct = 100.0 * yes_count / float(total_known)
        no_pct = 100.0 * no_count / float(total_known)

        stats[col] = {
            "yes_pct": float(yes_pct),
            "no_pct": float(no_pct),
            "yesno_total": float(total_yesno),
        }

    return stats


def profile_columns(
    df: pd.DataFrame,
    unknown_strings: Iterable[str],
) -> Dict[str, Dict[str, Any]]:
    """Return a rich per-column profile used to decide which features to drop.

    This mirrors the ML group's Script-to-Clean logic in a non-interactive form.
    """
    unknown_percent = percent_unknowns_per_column(df, unknown_strings)
    yes_no_stats = yes_no(df, unknown_strings)

    stats: Dict[str, Dict[str, Any]] = {}
    n_rows = len(df)

    for col in df.columns:
        series = df[col]
        col_stats: Dict[str, Any] = {}
        col_stats["unknown_pct"] = float(unknown_percent.get(col, 0.0))
        col_stats["total"] = int(n_rows)
        col_stats["nunique"] = int(series.nunique(dropna=True))

        non_null = series.dropna()
        col_stats["known"] = int(len(non_null))

        # dominant value ratio (excluding NaNs)
        if not non_null.empty:
            value_counts = non_null.value_counts()
            dominant_count = int(value_counts.iloc[0])
            col_stats["dominant_count"] = dominant_count
            col_stats["dominant_pct"] = 100.0 * dominant_count / float(len(non_null))
        else:
            col_stats["dominant_count"] = 0
            col_stats["dominant_pct"] = 0.0

        # yes/no stats if available
        yn = yes_no_stats.get(col, None)
        if yn is not None:
            col_stats["yes_pct"] = float(yn["yes_pct"])
            col_stats["no_pct"] = float(yn["no_pct"])
            col_stats["yesno_total"] = float(yn["yesno_total"])
        else:
            col_stats["yes_pct"] = 0.0
            col_stats["no_pct"] = 0.0
            col_stats["yesno_total"] = 0.0

        stats[col] = col_stats

    return stats


def suggest_columns_to_drop(
    df: pd.DataFrame,
    column_stats: Mapping[str, Mapping[str, Any]],
    *,
    unknown_threshold: float | None = None,
    yes_no_threshold: float | None = None,
    yesno_coverage_min: float = 50.0,
    protected_columns: Iterable[str] | None = None,
) -> Set[str]:
    """Heuristic feature-pruning logic using the profiling stats.

    This is a direct, non-interactive refactor of the Script-to-Clean rules.
    """
    if unknown_threshold is None:
        unknown_threshold = UNKNOWN_THRESHOLD
    if yes_no_threshold is None:
        yes_no_threshold = YES_NO_THRESHOLD

    validate_config_values(unknown_threshold, yes_no_threshold)

    protected = {c for c in (protected_columns or [])}
    n_rows = len(df)
    to_drop: Set[str] = set()

    for col, st in column_stats.items():
        if col in protected:
            continue

        unknown_pct = float(st.get("unknown_pct", 0.0))
        nunique = int(st.get("nunique", 0))
        yes_pct = float(st.get("yes_pct", 0.0))
        no_pct = float(st.get("no_pct", 0.0))
        yesno_total = float(st.get("yesno_total", 0.0))
        dominant_pct = float(st.get("dominant_pct", 0.0))
        dominant_count = int(st.get("dominant_count", 0))

        # 1) Too many unknowns
        if unknown_pct >= unknown_threshold:
            to_drop.add(col)
            continue

        # 2) Yes/No columns that are massively imbalanced
        if yesno_total >= yesno_coverage_min:
            if yes_pct < yes_no_threshold or no_pct < yes_no_threshold:
                to_drop.add(col)
                continue

        # 3) Columns with too few or too many unique values
        if nunique <= 1 or nunique >= n_rows:
            to_drop.add(col)
            continue

        # 4) Almost-constant columns (non yes/no) with enough minority support
        if yesno_total == 0.0 and dominant_pct >= 99.5:
            # require at least 25 non-dominant values so we do not drop
            # columns that are genuinely tiny.
            minority = max(int(st.get("known", 0)) - dominant_count, 0)
            if minority >= 25:
                to_drop.add(col)
                continue

    return to_drop


# ---------------------------------------------------------------------------
# Severity mapping (adapted from severity_mapping_utils.py)
# ---------------------------------------------------------------------------


def map_numeric_severity(unique_values: Sequence[Any]) -> Dict[Any, int]:
    """Map numeric severities into {0, 1, 2} buckets.

    0 = lowest severity, 2 = highest severity.
    """
    cleaned = []
    for v in unique_values:
        try:
            cleaned.append(float(v))
        except Exception:
            return {}

    if not cleaned:
        return {}

    lo = min(cleaned)
    hi = max(cleaned)
    mid = (lo + hi) / 2.0

    mapping: Dict[Any, int] = {}
    for raw, num in zip(unique_values, cleaned):
        if num <= mid and num != hi:
            mapping[raw] = 0
        elif num >= mid and num != lo:
            mapping[raw] = 2
        else:
            mapping[raw] = 1

    return mapping


def map_text_severity(unique_values: Sequence[Any]) -> Dict[Any, int]:
    """Map textual severities into {0, 1, 2} buckets using keywords."""
    severity_mapping: Dict[Any, int] = {}

    low_keywords = [
        "no injury",
        "property damage only",
        "pdo",
        "no apparent injury",
        "possible injury",
        "minor injury",
        "non-incapacitating",
    ]

    mid_keywords = [
        "suspected minor injury",
        "possible injury",
        "moderate injury",
        "non-serious injury",
    ]

    high_keywords = [
        "fatal",
        "death",
        "killed",
        "serious injury",
        "severe injury",
        "incapacitating injury",
        "hospitalized",
        "life threatening",
        "critical",
    ]

    for raw in unique_values:
        text = str(raw).strip().lower()

        score = None
        if any(k in text for k in high_keywords):
            score = 2
        elif any(k in text for k in mid_keywords):
            score = 1
        elif any(k in text for k in low_keywords):
            score = 0

        if score is not None:
            severity_mapping[raw] = score

    return severity_mapping


def guess_severity_column(
    df: pd.DataFrame,
    candidate_names: Sequence[str] | None = None,
) -> str | None:
    """Best-effort guess of the severity column name in a crash DataFrame."""
    if candidate_names is None:
        candidate_names = [
            "severity",
            "crash_severity",
            "Crash Severity",
            "Crash_Severity",
        ]

    for name in candidate_names:
        if name in df.columns:
            return name

    # fallback: any column containing the word severity
    for col in df.columns:
        if "severity" in col.lower():
            return col

    return None


def find_severity_mapping(df: pd.DataFrame, severity_col: str) -> Dict[Any, int]:
    """Non-interactive version of the ML team's find_severity_mapping.

    For MMUCC-style KABCO codes (K/A/B/C/O) we map to 3 buckets:

        K or A -> 2 (high)
        B or C -> 1 (medium)
        O      -> 0 (low)

    For other datasets we fall back to numeric or keyword-based mappings.
    """
    if severity_col not in df.columns:
        raise KeyError(f"Severity column {severity_col!r} not found in DataFrame.")

    series = df[severity_col].dropna()
    unique_vals = list(series.unique())

    if not unique_vals:
        raise ValueError("Severity column has no non-null values.")

    # Special-case KABCO-style single-letter codes
    normalized = [str(v).strip().upper() for v in unique_vals]
    kabco_set = {"K", "A", "B", "C", "O"}
    if set(normalized).issubset(kabco_set):
        mapping: Dict[Any, int] = {}
        for raw, norm in zip(unique_vals, normalized):
            if norm in {"K", "A"}:
                mapping[raw] = 2
            elif norm in {"B", "C"}:
                mapping[raw] = 1
            elif norm == "O" or norm == "0" or norm == "NO" or norm == "NONE":
                mapping[raw] = 0
        if mapping:
            return mapping

    # Try numeric mapping
    numeric = pd.to_numeric(series, errors="coerce")
    frac_numeric = float(numeric.notna().mean())
    if frac_numeric >= 0.9:
        mapping = map_numeric_severity(unique_vals)
        if mapping:
            return mapping

    # Try keyword-based text mapping
    text_mapping = map_text_severity(unique_vals)
    if text_mapping:
        return text_mapping

    raise ValueError(
        "Could not automatically determine a severity mapping for column "
        f"{severity_col!r}. Provide a cleaner severity column if needed."
    )


# ---------------------------------------------------------------------------
# Leakage detection (adapted from leakage_column_utils.py)
# ---------------------------------------------------------------------------


def suggest_leakage_by_name(columns: Sequence[str]) -> Set[str]:
    """Suggest leakage columns via simple keyword matches on column names."""
    keywords = [
        "fatal",
        "fatalities",
        "death",
        "dead",
        "killed",
        "injury",
        "injuries",
        "injured",
        "severity",
        "severe",
        "serious",
        "k_count",
        "killed_cnt",
        "inj_cnt",
    ]

    suggestions: Set[str] = set()
    for col in columns:
        lower = col.lower()
        if any(kw in lower for kw in keywords):
            suggestions.add(col)
    return suggestions


def find_near_perfect_predictors(
    X: pd.DataFrame,
    y: pd.Series,
    min_accuracy: float = 0.98,
    max_unique: int = 50,
) -> Sequence[Tuple[str, float]]:
    """Return columns that almost perfectly predict y on their own."""
    suspicious: list[Tuple[str, float]] = []

    for col in X.columns:
        series = X[col]

        if series.nunique(dropna=True) > max_unique:
            continue

        df_col = pd.DataFrame({"feature": series, "target": y})
        df_col = df_col.dropna(subset=["feature", "target"])
        if df_col.empty:
            continue

        mapping = (
            df_col.groupby("feature")["target"]
            .agg(lambda s: s.value_counts().idxmax())
        )

        y_hat = df_col["feature"].map(mapping)
        acc = float((y_hat == df_col["target"]).mean())
        if acc >= min_accuracy:
            suspicious.append((col, acc))

    return suspicious


def find_leakage_columns(
    X: pd.DataFrame,
    y: pd.Series,
    use_near_perfect_check: bool = True,
    min_accuracy: float = 0.9,
    max_unique: int = 50,
) -> Set[str]:
    """Non-interactive leakage detection.

    Combines:
      1) name-based suggestions; and
      2) near-perfect single-column predictors.
    """
    name_suggestions = suggest_leakage_by_name(list(X.columns))

    near_perfect: Sequence[Tuple[str, float]] = []
    if use_near_perfect_check:
        near_perfect = find_near_perfect_predictors(
            X,
            y,
            min_accuracy=min_accuracy,
            max_unique=max_unique,
        )

    leak_cols: Set[str] = set(name_suggestions)
    leak_cols.update(col for col, _ in near_perfect)

    if leak_cols:
        logger.info(
            "Detected potential leakage columns: %s",
            ", ".join(sorted(leak_cols)),
        )

    return leak_cols


def warn_suspicious_importances(
    feature_names: Sequence[str],
    importances: Sequence[float],
    importance_threshold: float = 0.2,
    dominance_ratio: float = 2.0,
) -> Sequence[str]:
    """Post-training helper that warns about unusually dominant features."""
    if not feature_names or len(feature_names) != len(importances):
        logger.warning(
            "warn_suspicious_importances: feature_names and importances size mismatch."
        )
        return []

    pairs = sorted(
        zip(feature_names, importances),
        key=lambda x: x[1],
        reverse=True,
    )
    if not pairs:
        return []

    max_name, max_imp = pairs[0]
    second_imp = pairs[1][1] if len(pairs) > 1 else 0.0

    suspicious: list[str] = []

    if second_imp == 0:
        dominance = float("inf") if max_imp > 0 else 0.0
    else:
        dominance = float(max_imp) / float(second_imp)

    if (max_imp >= importance_threshold) or (dominance >= dominance_ratio):
        suspicious.append(max_name)

    if suspicious:
        logger.warning(
            "Potential leakage features due to high importance: %s",
            ", ".join(suspicious),
        )

    return suspicious


# ---------------------------------------------------------------------------
# High-level helpers for ETL and model building
# ---------------------------------------------------------------------------



def clean_crash_dataframe_for_import(
    df: pd.DataFrame,
    *,
    base_unknowns: Iterable[str] | None = None,
    protected_columns: Iterable[str] | None = None,
    unknown_threshold: float | None = None,
    yes_no_threshold: float | None = None,
    columns_to_drop: Iterable[str] | None = None,
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """End-to-end cleaning step used by the import_crash_records command.

    This keeps the DataFrame row-aligned with the original dataset while:
      * normalising unknown tokens to NaN,
      * dropping obviously bad or redundant columns, and
      * optionally honouring per-job cleaning configuration.

    The *threshold* arguments are expressed as percentages in the 0100 range.
    If omitted they fall back to the module-level UNKNOWN_THRESHOLD /
    YES_NO_THRESHOLD defaults so existing callers continue to behave as before.
    """
    if base_unknowns is None:
        base_unknowns = DEFAULT_UNKNOWN_STRINGS

    # Decide effective thresholds, falling back to the module defaults.
    if unknown_threshold is None:
        unknown_threshold = UNKNOWN_THRESHOLD
    if yes_no_threshold is None:
        yes_no_threshold = YES_NO_THRESHOLD

    validate_config_values(
        unknown_threshold=unknown_threshold,
        yes_no_threshold=yes_no_threshold,
    )

    # Discover unknown-like placeholders and normalise them to NaN.
    unknown_values = discover_unknown_placeholders(df, base_unknowns)
    df_clean = df.replace(list(unknown_values), np.nan)

    # Profile columns once so we can drive all heuristics from the same stats.
    column_stats = profile_columns(df_clean, unknown_values)

    # Protect core identifiers and geo columns by default; callers can extend.
    default_protected: Set[str] = {
        # Core MMUCC schema columns
        "crash_id",
        "crash_date",
        "severity",
        "kabco",
        "latitude",
        "longitude",
    }
    if protected_columns is not None:
        default_protected.update(protected_columns)

    # First, use the heuristic suggestions.
    to_drop = suggest_columns_to_drop(
        df_clean,
        column_stats,
        unknown_threshold=unknown_threshold,
        yes_no_threshold=yes_no_threshold,
        protected_columns=default_protected,
    )

    # Then, honour any explicit user-specified drops (still respecting protection).
    user_specified_drops: Set[str] = set()
    if columns_to_drop is not None:
        for col in columns_to_drop:
            if not isinstance(col, str):
                continue
            col_name = col
            if col_name in default_protected:
                # Never drop protected columns, even if requested.
                logger.warning(
                    "Requested to drop protected column %r; ignoring.",
                    col_name,
                )
                continue
            user_specified_drops.add(col_name)

    if user_specified_drops:
        to_drop.update(user_specified_drops)

    cleaned = df_clean.drop(columns=list(to_drop), errors="ignore")

    meta: Dict[str, Any] = {
        "unknown_values": sorted(unknown_values),
        "dropped_columns": sorted(to_drop),
        "protected_columns": sorted(default_protected),
        "column_stats": column_stats,
        "input_shape": (int(df.shape[0]), int(df.shape[1])),
        "output_shape": (int(cleaned.shape[0]), int(cleaned.shape[1])),
        "cleaning_config": {
            "unknown_threshold": float(unknown_threshold),
            "yes_no_threshold": float(yes_no_threshold),
        },
        "user_specified_drops": sorted(user_specified_drops),
    }

    return cleaned, meta



def build_ml_ready_dataset(
    df: pd.DataFrame,
    *,
    severity_col: str | None = None,
    base_unknowns: Iterable[str] | None = None,
    unknown_threshold: float | None = None,
    yes_no_threshold: float | None = None,
    columns_to_drop: Iterable[str] | None = None,
) -> Tuple[pd.DataFrame, pd.Series, Dict[str, Any]]:
    """Produce (X, y, meta) suitable for model training.

    This wraps the ML pipeline:

      - unknown discovery (via clean_crash_dataframe_for_import),
      - severity mapping, and
      - basic leakage detection.

    The same cleaning configuration knobs used by the ETL pipeline are
    exposed so that model-training jobs can see an identical view of the
    data (unknown_threshold, yes_no_threshold, columns_to_drop, etc.).
    """
    if base_unknowns is None:
        base_unknowns = DEFAULT_UNKNOWN_STRINGS

    # Best-effort guess of the severity column if one is not provided.
    if severity_col is None:
        severity_col = guess_severity_column(df)
    if severity_col is None:
        raise ValueError("Could not infer a severity column from the dataset.")

    # Reuse the crash-cleaning pipeline so training matches ETL behaviour.
    cleaned_df, cleaning_meta = clean_crash_dataframe_for_import(
        df,
        base_unknowns=base_unknowns,
        protected_columns={severity_col},
        unknown_threshold=unknown_threshold,
        yes_no_threshold=yes_no_threshold,
        columns_to_drop=columns_to_drop,
    )

    unknown_values = set(cleaning_meta.get("unknown_values", []))

    sev_mapping = find_severity_mapping(cleaned_df, severity_col)
    y_raw = cleaned_df[severity_col]
    y = y_raw.map(sev_mapping)

    mask = y.notna()
    X = cleaned_df.loc[mask].drop(columns=[severity_col], errors="ignore")
    y = y.loc[mask].astype(int)

    leak_cols = find_leakage_columns(X, y)

    X_final = X.drop(columns=list(leak_cols), errors="ignore")

    meta: Dict[str, Any] = {
        "severity_column": severity_col,
        "severity_mapping": sev_mapping,
        "unknown_values": sorted(unknown_values),
        "leakage_columns": sorted(leak_cols),
        "n_rows_before_target_filter": int(df.shape[0]),
        "n_rows_after_target_filter": int(X_final.shape[0]),
        "n_features_before_leakage": int(X.shape[1]),
        "n_features_after_leakage": int(X_final.shape[1]),
        "cleaning_meta": cleaning_meta,
    }

    return X_final, y, meta

================================================================================
===== END FILE: analysis\ml_core\cleaning.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\ml_core\models.py =====
================================================================================


"""
Model training utilities for the analysis app.

This module contains importable training functions for the different
"official" models supported by the API.  Each training function follows
the same highlevel contract:

    train_xxx(df, cleaning_params, model_params) -> dict

Where the returned dict contains at least:

    {
        "model": <fitted model instance>,
        "metrics": {...},               # JSON-serialisable metrics
        "feature_importances": {...},   # mapping feature -> importance
        "cleaning_meta": {...},         # metadata from build_ml_ready_dataset
        "model_params": {...},          # fully-resolved model params
    }

The worker process is responsible for deciding which parts of this
dictionary are persisted to the database (e.g. metrics, top importances,
leakage warnings, etc.).
"""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Mapping, MutableMapping, Optional, Sequence, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    classification_report,
    f1_score,
)
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

try:  # Optional dependency, but expected to be installed via requirements.txt
    from interpret.glassbox import ExplainableBoostingClassifier
except Exception:  # pragma: no cover - handled at runtime
    ExplainableBoostingClassifier = None  # type: ignore

try:  # Optional dependency, but expected to be installed via requirements.txt
    from xgboost import XGBClassifier
except Exception:  # pragma: no cover - handled at runtime
    XGBClassifier = None  # type: ignore

from .cleaning import build_ml_ready_dataset, warn_suspicious_importances

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _ensure_cleaning_params(cleaning_params: Optional[Mapping[str, Any]]) -> Dict[str, Any]:
    """
    Normalise cleaning parameters into kwargs for build_ml_ready_dataset.

    We deliberately accept a very permissive shape here because the API
    exposes "parameters.cleaning" as a free-form JSON object.  Unknown
    keys are simply ignored.
    """
    params = dict(cleaning_params or {})

    return {
        "severity_col": params.get("severity_col"),
        "base_unknowns": params.get("base_unknowns"),
        "unknown_threshold": params.get("unknown_threshold"),
        "yes_no_threshold": params.get("yes_no_threshold"),
        "columns_to_drop": params.get("columns_to_drop"),
    }


def _evaluate_classifier(
    y_true_train: pd.Series,
    y_pred_train: np.ndarray,
    y_true_test: Optional[pd.Series] = None,
    y_pred_test: Optional[np.ndarray] = None,
) -> Dict[str, Any]:
    """
    Compute a small, JSON-safe set of classification metrics.

    If no test set is supplied, we compute "overall_*" metrics only.
    """
    metrics: Dict[str, Any] = {}

    # Train / overall metrics
    metrics["train_accuracy"] = float(accuracy_score(y_true_train, y_pred_train))
    metrics["train_f1_macro"] = float(f1_score(y_true_train, y_pred_train, average="macro"))
    metrics["train_balanced_accuracy"] = float(
        balanced_accuracy_score(y_true_train, y_pred_train)
    )
    metrics["train_classification_report"] = classification_report(
        y_true_train, y_pred_train, output_dict=True
    )

    # Test metrics (optional)
    if y_true_test is not None and y_pred_test is not None:
        metrics["test_accuracy"] = float(accuracy_score(y_true_test, y_pred_test))
        metrics["test_f1_macro"] = float(
            f1_score(y_true_test, y_pred_test, average="macro")
        )
        metrics["test_balanced_accuracy"] = float(
            balanced_accuracy_score(y_true_test, y_pred_test)
        )
        metrics["test_classification_report"] = classification_report(
            y_true_test, y_pred_test, output_dict=True
        )

    # Basic label distribution info (for debugging / sanity checks)
    metrics["label_distribution"] = (
        y_true_train.value_counts(normalize=True).sort_index().to_dict()
    )
    if y_true_test is not None:
        metrics["label_distribution_test"] = (
            y_true_test.value_counts(normalize=True).sort_index().to_dict()
        )

    return metrics


def _extract_feature_importances(
    model: Any, feature_names: Sequence[str]
) -> Dict[str, float]:
    """
    Extract feature importances for a fitted model, falling back to zeros
    if the model does not expose importances.

    All values are normalised to sum to 1.0 to make comparison between
    models easier.
    """
    importances: Optional[np.ndarray] = None

    if hasattr(model, "feature_importances_"):
        importances = np.asarray(getattr(model, "feature_importances_"), dtype="float64")
    elif hasattr(model, "coef_"):
        coef = np.asarray(getattr(model, "coef_"), dtype="float64")
        if coef.ndim == 1:
            importances = np.abs(coef)
        else:
            importances = np.mean(np.abs(coef), axis=0)
    elif hasattr(model, "term_importances_"):
        # Interpret's ExplainableBoostingClassifier exposes term_importances_.
        importances = np.asarray(getattr(model, "term_importances_"), dtype="float64")

    if importances is None:
        # Graceful fallback: no notion of feature importance for this model.
        logger.warning(
            "Model of type %s does not expose feature importances; "
            "falling back to zeros.",
            type(model).__name__,
        )
        return {name: 0.0 for name in feature_names}

    if importances.shape[0] != len(feature_names):
        # Extremely defensive: shape mismatch usually indicates something
        # odd in how the model represents terms.
        logger.warning(
            "Feature importance vector has length %s but there are %s feature names. "
            "Truncating/padding with zeros.",
            importances.shape[0],
            len(feature_names),
        )
        # Truncate or pad with zeros to match.
        if importances.shape[0] > len(feature_names):
            importances = importances[: len(feature_names)]
        else:
            pad_width = len(feature_names) - importances.shape[0]
            importances = np.pad(importances, (0, pad_width), mode="constant")

    total = float(importances.sum())
    if not np.isfinite(total) or total <= 0.0:
        total = 1.0

    normalised = importances / total
    return {name: float(val) for name, val in zip(feature_names, normalised)}


def _train_test_split_if_possible(
    X: pd.DataFrame, y: pd.Series, *, random_state: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """
    Perform a robust train/test split that gracefully falls back to using
    the full dataset as "train" if we don't have enough rows or labels.
    """
    n_samples = len(X)
    n_classes = y.nunique(dropna=True)

    if n_samples < 10 or n_classes < 2:
        # Too small or degenerate  just return the full dataset as train
        # and an empty test split.
        logger.warning(
            "Dataset too small or not enough label variety for a proper "
            "train/test split (n_samples=%s, n_classes=%s). "
            "Using the full dataset as train only.",
            n_samples,
            n_classes,
        )
        return X, pd.DataFrame(columns=X.columns), y, pd.Series(dtype=y.dtype)

    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=0.2,
            random_state=random_state,
            stratify=y if n_classes > 1 else None,
        )
    except ValueError as exc:
        logger.warning(
            "Falling back to no train/test split due to ValueError: %s", exc
        )
        return X, pd.DataFrame(columns=X.columns), y, pd.Series(dtype=y.dtype)

    return X_train, X_test, y_train, y_test


def _base_result(
    model: Any,
    feature_names: Sequence[str],
    cleaning_meta: Mapping[str, Any],
    model_params: Mapping[str, Any],
    *,
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_test: Optional[pd.DataFrame] = None,
    y_test: Optional[pd.Series] = None,
) -> Dict[str, Any]:
    """Assemble the standard result dictionary for all training functions."""
    # Predictions for metrics
    y_pred_train = model.predict(X_train)
    y_pred_test = None
    if X_test is not None and not X_test.empty and y_test is not None and len(y_test) > 0:
        y_pred_test = model.predict(X_test)

    metrics = _evaluate_classifier(y_train, y_pred_train, y_test, y_pred_test)

    feature_importances = _extract_feature_importances(model, feature_names)

    # Detect suspicious / potentially leaky features.
    suspicious_features: List[str] = warn_suspicious_importances(
        list(feature_importances.keys()),
        list(feature_importances.values()),
    )

    leakage_columns: List[str] = list(
        cleaning_meta.get("leakage_columns", [])  # type: ignore[call-arg]
    )

    return {
        "model": model,
        "metrics": metrics,
        "feature_importances": feature_importances,
        "cleaning_meta": dict(cleaning_meta),
        "model_params": dict(model_params),
        "leakage_warnings": {
            "leakage_columns": leakage_columns,
            "suspicious_features": suspicious_features,
        },
    }


# ---------------------------------------------------------------------------
# Training functions
# ---------------------------------------------------------------------------


def train_crash_severity_decision_tree(
    df: pd.DataFrame,
    cleaning_params: Optional[Mapping[str, Any]] = None,
    model_params: Optional[Mapping[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Train the baseline crash severity risk model using a DecisionTreeClassifier.

    This function is intended to back the ``crash_severity_risk_v1`` entry
    in the model registry.
    """
    cleaning_kwargs = _ensure_cleaning_params(cleaning_params)
    X, y, cleaning_meta = build_ml_ready_dataset(df, **cleaning_kwargs)

    default_model_params: Dict[str, Any] = {
        "criterion": "gini",
        "max_depth": 6,
        "min_samples_leaf": 50,
        "random_state": 42,
    }
    final_model_params = {**default_model_params, **(model_params or {})}

    model = DecisionTreeClassifier(**final_model_params)

    X_train, X_test, y_train, y_test = _train_test_split_if_possible(X, y)
    model.fit(X_train, y_train)

    return _base_result(
        model=model,
        feature_names=list(X.columns),
        cleaning_meta=cleaning_meta,
        model_params=final_model_params,
        X_train=X_train,
        y_train=y_train,
        X_test=X_test if not X_test.empty else None,
        y_test=y_test if len(y_test) > 0 else None,
    )


def train_ebm(
    df: pd.DataFrame,
    cleaning_params: Optional[Mapping[str, Any]] = None,
    model_params: Optional[Mapping[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Train an Explainable Boosting Machine (EBM) classifier.

    Backs the ``ebm_v1`` entry in the registry.
    """
    if ExplainableBoostingClassifier is None:
        raise RuntimeError(
            "interpret is not installed or ExplainableBoostingClassifier "
            "is unavailable. Ensure the 'interpret' dependency is installed."
        )

    cleaning_kwargs = _ensure_cleaning_params(cleaning_params)
    X, y, cleaning_meta = build_ml_ready_dataset(df, **cleaning_kwargs)

    default_model_params: Dict[str, Any] = {
        "interactions": 10,
        "max_bins": 256,
        "outer_bags": 8,
        "inner_bags": 0,
        "learning_rate": 0.01,
        "random_state": 42,
    }
    final_model_params = {**default_model_params, **(model_params or {})}

    model = ExplainableBoostingClassifier(**final_model_params)

    X_train, X_test, y_train, y_test = _train_test_split_if_possible(X, y)
    model.fit(X_train, y_train)

    return _base_result(
        model=model,
        feature_names=list(X.columns),
        cleaning_meta=cleaning_meta,
        model_params=final_model_params,
        X_train=X_train,
        y_train=y_train,
        X_test=X_test if not X_test.empty else None,
        y_test=y_test if len(y_test) > 0 else None,
    )


def train_mrf(
    df: pd.DataFrame,
    cleaning_params: Optional[Mapping[str, Any]] = None,
    model_params: Optional[Mapping[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Train a (monotonic) Random Forest classifier.

    The original ML repo refers to this family as "MRF".  Here we
    implement it using sklearn's RandomForestClassifier.
    """
    cleaning_kwargs = _ensure_cleaning_params(cleaning_params)
    X, y, cleaning_meta = build_ml_ready_dataset(df, **cleaning_kwargs)

    default_model_params: Dict[str, Any] = {
        "n_estimators": 200,
        "max_depth": None,
        "min_samples_leaf": 10,
        "n_jobs": -1,
        "random_state": 42,
        "class_weight": "balanced",
    }
    final_model_params = {**default_model_params, **(model_params or {})}

    model = RandomForestClassifier(**final_model_params)

    X_train, X_test, y_train, y_test = _train_test_split_if_possible(X, y)
    model.fit(X_train, y_train)

    return _base_result(
        model=model,
        feature_names=list(X.columns),
        cleaning_meta=cleaning_meta,
        model_params=final_model_params,
        X_train=X_train,
        y_train=y_train,
        X_test=X_test if not X_test.empty else None,
        y_test=y_test if len(y_test) > 0 else None,
    )


def train_xgb(
    df: pd.DataFrame,
    cleaning_params: Optional[Mapping[str, Any]] = None,
    model_params: Optional[Mapping[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Train an XGBoost classifier for crash severity risk.

    Backs the ``xgb_v1`` entry in the registry.
    """
    if XGBClassifier is None:
        raise RuntimeError(
            "xgboost is not installed or XGBClassifier is unavailable. "
            "Ensure the 'xgboost' dependency is installed."
        )

    cleaning_kwargs = _ensure_cleaning_params(cleaning_params)
    X, y, cleaning_meta = build_ml_ready_dataset(df, **cleaning_kwargs)

    default_model_params: Dict[str, Any] = {
        "n_estimators": 300,
        "max_depth": 5,
        "learning_rate": 0.05,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "objective": "multi:softprob",
        "eval_metric": "mlogloss",
        "tree_method": "hist",
        "random_state": 42,
        "n_jobs": -1,
    }
    final_model_params = {**default_model_params, **(model_params or {})}

    model = XGBClassifier(**final_model_params)

    X_train, X_test, y_train, y_test = _train_test_split_if_possible(X, y)
    model.fit(X_train, y_train)

    return _base_result(
        model=model,
        feature_names=list(X.columns),
        cleaning_meta=cleaning_meta,
        model_params=final_model_params,
        X_train=X_train,
        y_train=y_train,
        X_test=X_test if not X_test.empty else None,
        y_test=y_test if len(y_test) > 0 else None,
    )


# ---------------------------------------------------------------------------
# Registry
# ---------------------------------------------------------------------------


TrainFunc = Callable[[pd.DataFrame, Optional[Mapping[str, Any]], Optional[Mapping[str, Any]]], Dict[str, Any]]


@dataclass(frozen=True)
class ModelSpec:
    """Specification for a supported model."""

    name: str
    description: str
    trainer: TrainFunc
    default_model_params: Mapping[str, Any]


MODEL_REGISTRY: Dict[str, ModelSpec] = {
    "crash_severity_risk_v1": ModelSpec(
        name="crash_severity_risk_v1",
        description="Baseline crash severity risk model (DecisionTree v1).",
        trainer=train_crash_severity_decision_tree,
        default_model_params={
            "criterion": "gini",
            "max_depth": 6,
            "min_samples_leaf": 50,
            "random_state": 42,
        },
    ),
    "ebm_v1": ModelSpec(
        name="ebm_v1",
        description="Explainable Boosting Machine (EBM) v1.",
        trainer=train_ebm,
        default_model_params={
            "interactions": 10,
            "max_bins": 256,
            "outer_bags": 8,
            "inner_bags": 0,
            "learning_rate": 0.01,
            "random_state": 42,
        },
    ),
    "mrf_v1": ModelSpec(
        name="mrf_v1",
        description="Monotonic Random Forest (MRF) v1.",
        trainer=train_mrf,
        default_model_params={
            "n_estimators": 200,
            "max_depth": None,
            "min_samples_leaf": 10,
            "n_jobs": -1,
            "random_state": 42,
            "class_weight": "balanced",
        },
    ),
    "xgb_v1": ModelSpec(
        name="xgb_v1",
        description="XGBoost crash risk model v1.",
        trainer=train_xgb,
        default_model_params={
            "n_estimators": 300,
            "max_depth": 5,
            "learning_rate": 0.05,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "objective": "multi:softprob",
            "eval_metric": "mlogloss",
            "tree_method": "hist",
            "random_state": 42,
            "n_jobs": -1,
        },
    ),
}

================================================================================
===== END FILE: analysis\ml_core\models.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\ml_core\worker.py =====
================================================================================


"""
ModelJob worker process utilities.

This module provides the core "worker" implementation that can be used
from:

* a background thread (see enqueue_model_job),
* a Django management command, or
* an external task runner (Celery / RQ) that simply calls run_model_job.

The responsibilities here are:

* Load the UploadedDataset into a pandas DataFrame.
* Look up the correct training function from MODEL_REGISTRY.
* Invoke the training function with cleaning + model parameters.
* Persist metrics, top feature importances, and leakage warnings into
  ModelJob.result_metadata.
* Update the ModelJob status to succeeded/failed.
"""

from __future__ import annotations

import logging
import os
import threading
import time
import traceback
from typing import Any, Dict, Optional
from uuid import UUID

import pandas as pd
from django.db import transaction
from django.utils import timezone

from ingestion.models import UploadedDataset
from ingestion.validation import load_dataframe_from_bytes
from analysis.models import ModelJob

from .models import MODEL_REGISTRY, ModelSpec

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Core job runner
# ---------------------------------------------------------------------------


def _load_dataframe_for_job(upload: UploadedDataset) -> pd.DataFrame:
    """Load the raw_file from an UploadedDataset into a DataFrame."""
    file_field = upload.raw_file
    if not file_field:
        raise ValueError("UploadedDataset has no raw_file attached.")

    # Reset pointer and read bytes.
    file_field.open("rb")
    try:
        raw_bytes = file_field.read()
    finally:
        file_field.close()

    # Infer extension from the filename when possible.
    filename = file_field.name or upload.original_filename or ""
    _, ext = os.path.splitext(filename)
    ext = ext.lower() or ".csv"

    logger.info("Loading dataframe for UploadedDataset %s (ext=%s)", upload.pk, ext)

    return load_dataframe_from_bytes(raw_bytes, ext)


def _build_result_metadata(
    job: ModelJob,
    spec: ModelSpec,
    trainer_output: Dict[str, Any],
    *,
    started_at,
    finished_at,
) -> Dict[str, Any]:
    """
    Normalise the training output into a JSON-serialisable metadata
    structure suitable for storing in ModelJob.result_metadata.
    """
    metrics = trainer_output.get("metrics") or {}
    feature_importances = trainer_output.get("feature_importances") or {}
    cleaning_meta = trainer_output.get("cleaning_meta") or {}
    model_params = trainer_output.get("model_params") or {}
    leakage_warnings = trainer_output.get("leakage_warnings") or {}

    # Sort and capture the top N feature importances.
    items = list(feature_importances.items())
    items.sort(key=lambda kv: kv[1], reverse=True)
    top_n = [
        {"feature": name, "importance": float(score)}
        for name, score in items[:50]
    ]

    duration_seconds: Optional[float] = None
    try:
        duration_seconds = (finished_at - started_at).total_seconds()
    except Exception:  # pragma: no cover - extremely defensive
        duration_seconds = None

    metadata: Dict[str, Any] = {
        "model_name": job.model_name,
        "model_label": spec.description,
        "model_params": model_params,
        "metrics": metrics,
        "cleaning_meta": cleaning_meta,
        "feature_importances": {
            "top_n": top_n,
            # We also keep the full mapping for offline analysis, but this
            # can be removed if storage becomes an issue.
            "all": {k: float(v) for k, v in feature_importances.items()},
        },
        "leakage_warnings": leakage_warnings,
        "worker": {
            "started_at": started_at.isoformat(),
            "finished_at": finished_at.isoformat(),
            "duration_seconds": duration_seconds,
        },
    }

    return metadata


def run_model_job(job_id: str | UUID) -> None:
    """
    Run a single ModelJob end-to-end.

    This is the main entry point that can be invoked by a Celery task,
    RQ job, Django management command, or the lightweight in-process
    enqueue_model_job helper below.
    """
    job_id_str = str(job_id)
    logger.info("Starting ModelJob worker for job_id=%s", job_id_str)

    with transaction.atomic():
        try:
            job = (
                ModelJob.objects.select_for_update()
                .select_related("upload")
                .get(pk=job_id)
            )
        except ModelJob.DoesNotExist:
            logger.error("ModelJob %s does not exist; aborting.", job_id_str)
            return

        if job.status != ModelJob.Status.QUEUED:
            logger.warning(
                "ModelJob %s has status=%s, not 'queued'; refusing to run.",
                job_id_str,
                job.status,
            )
            return

        job.status = ModelJob.Status.RUNNING
        job.updated_at = timezone.now()
        job.save(update_fields=["status", "updated_at"])

    # From this point onwards, we operate outside the transaction and
    # update the job record as we go.
    started_at = timezone.now()

    try:
        df = _load_dataframe_for_job(job.upload)

        try:
            spec = MODEL_REGISTRY[job.model_name]
        except KeyError:
            raise ValueError(
                f"Unknown model_name '{job.model_name}'. "
                f"Supported: {sorted(MODEL_REGISTRY.keys())}"
            )

        # Split parameters into cleaning + model-specific parameters.
        parameters = job.parameters or {}
        cleaning_params = parameters.get("cleaning") or {}
        model_params = parameters.get("model_params") or parameters.get("model") or {}
        # Merge user-supplied params over the registry defaults.
        merged_model_params = {
            **spec.default_model_params,
            **(model_params or {}),
        }

        logger.info(
            "Running model '%s' for job %s with %d cleaning params and %d model params.",
            job.model_name,
            job_id_str,
            len(cleaning_params),
            len(merged_model_params),
        )

        trainer_output = spec.trainer(
            df,
            cleaning_params=cleaning_params,
            model_params=merged_model_params,
        )

        finished_at = timezone.now()
        metadata = _build_result_metadata(
            job=job,
            spec=spec,
            trainer_output=trainer_output,
            started_at=started_at,
            finished_at=finished_at,
        )

        job.status = ModelJob.Status.SUCCEEDED
        job.result_metadata = metadata
        job.updated_at = finished_at
        job.save(update_fields=["status", "result_metadata", "updated_at"])

        logger.info("ModelJob %s completed successfully.", job_id_str)

    except Exception as exc:  # pragma: no cover - error path
        finished_at = timezone.now()
        tb_str = traceback.format_exc()
        logger.exception("ModelJob %s failed: %s", job_id_str, exc)

        # Best-effort update of the job record with failure details.
        try:
            job = ModelJob.objects.get(pk=job_id)
            job.status = ModelJob.Status.FAILED
            job.result_metadata = {
                "error": str(exc),
                "traceback": tb_str,
                "failed_at": finished_at.isoformat(),
            }
            job.updated_at = finished_at
            job.save(update_fields=["status", "result_metadata", "updated_at"])
        except Exception:  # pragma: no cover - last resort
            logger.error(
                "Failed to update ModelJob %s with failure metadata.", job_id_str
            )


# ---------------------------------------------------------------------------
# Queue helpers / management-command friendly API
# ---------------------------------------------------------------------------


def run_next_queued_job() -> Optional[str]:
    """
    Pop the next queued ModelJob (if any) and run it synchronously.

    Returns the job ID that was processed, or None if there were no
    queued jobs.
    """
    job = (
        ModelJob.objects.filter(status=ModelJob.Status.QUEUED)
        .select_related("upload")
        .order_by("created_at")
        .first()
    )
    if not job:
        logger.info("No queued ModelJob instances found.")
        return None

    job_id = str(job.id)
    run_model_job(job_id)
    return job_id


def enqueue_model_job(job_id: str | UUID) -> None:
    """
    Lightweight "enqueue" helper used by the /api/models/run/ view.

    In a production deployment this function can be swapped out to call
    a real background task system (e.g. Celery or RQ).  For now we
    implement it as a simple daemon thread to avoid blocking the HTTP
    request while still honouring the API contract that model training
    happens asynchronously.
    """

    def _target():
        # Small delay so that the HTTP response has a chance to be sent
        # before heavy work begins (helpful for local dev).
        time.sleep(0.1)
        run_model_job(job_id)

    thread = threading.Thread(target=_target, daemon=True)
    thread.start()

================================================================================
===== END FILE: analysis\ml_core\worker.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\models.py =====
================================================================================

import uuid

from django.conf import settings
from django.db import models

from ingestion.models import UploadedDataset


class ModelJob(models.Model):
    """Track long-running model / analysis jobs.

    This model intentionally does *not* contain raw prediction payloads; those
    should live in separate, model-specific tables managed by the ML team.
    """

    class Status(models.TextChoices):
        QUEUED = "queued", "Queued"
        RUNNING = "running", "Running"
        SUCCEEDED = "succeeded", "Succeeded"
        FAILED = "failed", "Failed"

    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    upload = models.ForeignKey(
        UploadedDataset,
        on_delete=models.CASCADE,
        related_name="model_jobs",
    )
    owner = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.CASCADE,
        related_name="model_jobs",
    )
    model_name = models.CharField(max_length=128)
    status = models.CharField(
        max_length=16,
        choices=Status.choices,
        default=Status.QUEUED,
    )
    parameters = models.JSONField(blank=True, default=dict)
    result_metadata = models.JSONField(blank=True, default=dict)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

    class Meta:
        ordering = ["-created_at"]
        indexes = [
            models.Index(fields=["owner"]),
            models.Index(fields=["upload"]),
            models.Index(fields=["status"]),
        ]

    def __str__(self) -> str:
        return f"{self.model_name} on {self.upload_id} ({self.status})"

================================================================================
===== END FILE: analysis\models.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\urls.py =====
================================================================================

from django.urls import path
from . import views

urlpatterns = [
    # Legacy/simple upload + summary endpoint
    path("upload/", views.upload_and_analyze, name="upload-and-analyze"),
    # Static example payload for frontend smoke tests
    path("summary/", views.summary_example, name="summary-example"),
    # Abstract ML/model endpoints; to be implemented by the ML team.
    path("models/run/", views.model_run, name="models-run"),
    path("models/results/<uuid:job_id>/", views.model_results, name="models-results"),
]

================================================================================
===== END FILE: analysis\urls.py =====
================================================================================

================================================================================
===== BEGIN FILE: analysis\views.py =====
================================================================================


import io
import logging
from typing import Any, Dict, List

import pandas as pd
from django.http import JsonResponse
from django.shortcuts import get_object_or_404
from rest_framework.decorators import api_view, permission_classes, throttle_classes
from rest_framework.permissions import IsAuthenticated

from ingestion.models import UploadedDataset
from ingestion.validation import validate_dataframe_from_upload, load_dataframe_from_bytes
from .models import ModelJob
from .throttling import BurstRateThrottle
from .ml_core.models import MODEL_REGISTRY
from .ml_core.worker import enqueue_model_job

logger = logging.getLogger(__name__)


def build_summary_from_dataframe(df: pd.DataFrame) -> Dict[str, Any]:
    """Compute a lightweight statistical summary of a dataframe."""
    buffer = io.StringIO()
    df.info(buf=buffer)
    info_str = buffer.getvalue()

    head = df.head().to_dict(orient="list")
    describe = df.describe(include="all").to_dict()

    column_samples: Dict[str, List[Any]] = {}
    for col in df.columns:
        column_samples[col] = df[col].dropna().unique()[:10].tolist()

    return {
        "shape": df.shape,
        "info": info_str,
        "head": head,
        "describe": describe,
        "column_samples": column_samples,
    }


@api_view(["POST"])
@permission_classes([IsAuthenticated])
@throttle_classes([BurstRateThrottle])
def upload_and_analyze(request):
    """Upload a CSV/Excel file and return a basic profile for quick inspection."""
    upload = request.FILES.get("file")
    if not upload:
        return JsonResponse({"detail": "No file uploaded."}, status=400)

    try:
        df = validate_dataframe_from_upload(upload)
    except ValueError as exc:
        return JsonResponse({"detail": str(exc)}, status=400)

    summary = build_summary_from_dataframe(df)
    return JsonResponse(summary, status=200)


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def summary_example(request):
    """Return a canned dataframe summary useful for front-end mocks."""
    from pathlib import Path

    example_path = Path(__file__).resolve().parent / "example_data" / "crash_sample.csv"
    try:
        df = pd.read_csv(example_path)
    except FileNotFoundError:
        return JsonResponse(
            {"detail": f"Example file not found at {example_path}."}, status=500
        )

    summary = build_summary_from_dataframe(df)
    return JsonResponse(summary, status=200)


def _user_is_admin(user) -> bool:
    return user.is_superuser or user.groups.filter(name="admin").exists()


# Keep a lightweight, description-only mapping for backwards compatibility.
SUPPORTED_MODELS: Dict[str, str] = {
    name: spec.description for name, spec in MODEL_REGISTRY.items()
}


@api_view(["POST"])
@permission_classes([IsAuthenticated])
@api_view(["POST"])
@permission_classes([IsAuthenticated])
def model_run(request):
    """
    Start a model training run for an existing UploadedDataset.

    Expected JSON body:
    {
        "dataset_id": "<uuid of UploadedDataset>",
        "model_name": "ebm_v1",
        "parameters": {...}  # optional, free-form
    }

    Behavioural notes:

    * A ModelJob row is created in the database with status="queued".
    * The worker is enqueued (via a lightweight background thread) to
      process the job asynchronously.
    * The response includes the job id and a URL for polling the results.
    """

    if not _user_is_admin(request.user):
        return JsonResponse(
            {"detail": "Only admin users can start model runs."}, status=403
        )

    try:
        payload = request.data
    except Exception:
        payload = {}

    dataset_id = payload.get("dataset_id")
    model_name = payload.get("model_name")
    parameters = payload.get("parameters") or {}

    errors = {}
    if not dataset_id:
        errors["dataset_id"] = "This field is required."
    if not model_name:
        errors["model_name"] = "This field is required."
    elif model_name not in MODEL_REGISTRY:
        errors["model_name"] = (
            f"Unsupported model_name. Supported: {sorted(MODEL_REGISTRY.keys())}"
        )

    if errors:
        return JsonResponse({"errors": errors}, status=400)

    dataset = get_object_or_404(UploadedDataset, pk=dataset_id)

    # Basic validation of parameters.cleaning.* to avoid confusing the ML pipeline
    cleaning_params = parameters.get("cleaning") or {}
    if not isinstance(cleaning_params, dict):
        return JsonResponse(
            {"errors": {"parameters.cleaning": "Must be an object/dict."}},
            status=400,
        )

    for key in ["unknown_threshold", "yes_no_threshold"]:
        if key in cleaning_params and not isinstance(cleaning_params[key], (int, float)):
            return JsonResponse(
                {
                    "errors": {
                        "parameters.cleaning": f'"{key}" must be numeric (int/float).'
                    }
                },
                status=400,
            )

    if "columns_to_drop" in cleaning_params and not isinstance(
        cleaning_params["columns_to_drop"], list
    ):
        return JsonResponse(
            {
                "errors": {
                    "parameters.cleaning": '"columns_to_drop" must be a list of strings.'
                }
            },
            status=400,
        )

    job = ModelJob.objects.create(
        upload=dataset,
        model_name=model_name,
        parameters=parameters,
        status=ModelJob.Status.QUEUED,
    )

    # Enqueue the worker.  The current implementation uses a lightweight
    # background thread so that the HTTP request can return quickly.  In
    # a production deployment this can be replaced with a Celery task,
    # RQ job, or other task runner.
    try:
        enqueue_model_job(job.id)
    except Exception as exc:  # pragma: no cover - failure is recorded on the job
        logger.exception("Failed to enqueue worker for ModelJob %s: %s", job.id, exc)
        job.status = ModelJob.Status.FAILED
        job.result_metadata = {
            "error": f"Failed to enqueue worker: {exc}",
        }
        job.save(update_fields=["status", "result_metadata"])

    # Provide a canonical URL for polling results.  We avoid depending on
    # URL names here to keep this view decoupled from URLconf details.
    results_path = f"/api/models/results/{job.id}/"
    results_url = request.build_absolute_uri(results_path)

    return JsonResponse(
        {
            "job_id": str(job.id),
            "status": job.status,
            "dataset_id": str(dataset.id),
            "model_name": job.model_name,
            "results_url": results_url,
        },
        status=202,
    )


model_run.throttle_scope = "model_run"


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def model_results(request, job_id: str):
    """
    Fetch the JSON result_metadata for a ModelJob.

    This endpoint is read-only and returns whatever the ML worker stored
    into ModelJob.result_metadata, along with basic job bookkeeping fields.
    """
    job = get_object_or_404(ModelJob, pk=job_id)

    if not _user_is_admin(request.user) and job.upload.owner != request.user:
        return JsonResponse(
            {"detail": "You do not have permission to view this job."}, status=403
        )

    payload = {
        "job_id": str(job.id),
        "status": job.status,
        "model_name": job.model_name,
        "dataset_id": str(job.upload_id),
        "created_at": job.created_at.isoformat(),
        "updated_at": job.updated_at.isoformat(),
        "result_metadata": job.result_metadata or {},
    }
    return JsonResponse(payload, status=200)

================================================================================
===== END FILE: analysis\views.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\__init__.py =====
================================================================================


================================================================================
===== END FILE: crashdata\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\admin.py =====
================================================================================

from django.contrib import admin

from .models import CrashRecord


@admin.register(CrashRecord)
class CrashRecordAdmin(admin.ModelAdmin):
    list_display = (
        "id",
        "dataset",
        "crash_id",
        "crash_datetime",
        "severity",
        "municipality",
    )
    list_filter = ("severity", "crash_datetime", "municipality")
    search_fields = ("crash_id", "dataset__original_filename", "municipality", "roadway_name")

================================================================================
===== END FILE: crashdata\admin.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\apps.py =====
================================================================================

from django.apps import AppConfig


class CrashdataConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "crashdata"

================================================================================
===== END FILE: crashdata\apps.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\management\__init__.py =====
================================================================================


================================================================================
===== END FILE: crashdata\management\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\management\commands\__init__.py =====
================================================================================


================================================================================
===== END FILE: crashdata\management\commands\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\management\commands\import_crash_records.py =====
================================================================================

from __future__ import annotations

import os
from datetime import datetime
from typing import Any

import pandas as pd
from django.contrib.gis.geos import Point
from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone

from crashdata.models import CrashRecord
from ingestion.models import UploadedDataset
from ingestion.validation import load_dataframe_from_bytes
from analysis.ml_core import cleaning as ml_cleaning


class Command(BaseCommand):
    help = "Import cleaned crash records for a given UploadedDataset using the ML cleaning pipeline."

    def add_arguments(self, parser) -> None:
        parser.add_argument(
            "upload_id",
            type=str,
            help="ID of the UploadedDataset to import.",
        )
        parser.add_argument(
            "--dry-run",
            action="store_true",
            help="Run the cleaning pipeline and show a summary without writing CrashRecord rows.",
        )

    def handle(self, *args: Any, **options: Any) -> None:
        upload_id = options["upload_id"]
        dry_run = bool(options["dry_run"])

        try:
            dataset = UploadedDataset.objects.get(id=upload_id)
        except UploadedDataset.DoesNotExist:
            raise CommandError(f"UploadedDataset with id={upload_id!r} does not exist.")

        if not dataset.raw_file:
            raise CommandError("UploadedDataset has no raw_file attached.")

        file_field = dataset.raw_file
        file_name = file_field.name or dataset.original_filename
        ext = os.path.splitext(file_name)[1].lower() or ".csv"

        raw_bytes = file_field.read()
        if not raw_bytes:
            raise CommandError("UploadedDataset.raw_file is empty.")

        # Re-parse using the same helper as the ingestion gateway so semantics match.
        df = load_dataframe_from_bytes(raw_bytes, ext)

        # Run the refactored, non-interactive cleaning pipeline.
        cleaned_df, meta = ml_cleaning.clean_crash_dataframe_for_import(df)

        if cleaned_df.empty:
            raise CommandError("Cleaning pipeline produced an empty DataFrame; nothing to import.")

        # Log a small summary to the console so ML teammates can sanity-check.
        self.stdout.write(
            self.style.SUCCESS(
                f"Cleaned dataframe shape: {cleaned_df.shape[0]} rows  {cleaned_df.shape[1]} columns"
            )
        )
        dropped = meta.get("dropped_columns", [])
        if dropped:
            self.stdout.write("Dropped columns (likely high-unknown / low-signal / near-constant):")
            for name in dropped:
                self.stdout.write(f"  - {name}")

        if dry_run:
            self.stdout.write(self.style.SUCCESS("Dry run complete; no CrashRecord rows were written."))
            return

        # Basic mapping assumptions:
        #   * crash_id       -> CrashRecord.crash_id
        #   * crash_date     -> CrashRecord.crash_datetime (midnight in the current timezone)
        #   * severity       -> CrashRecord.severity (MMUCC KABCO codes)
        #   * latitude/longitude -> CrashRecord.location (WGS84 point)
        #   * posted_speed_limit, vehicle_count, person_count if available
        required_cols = {"crash_id", "crash_date", "severity"}
        missing = [c for c in required_cols if c not in cleaned_df.columns]
        if missing:
            raise CommandError(
                "Cleaned dataset is missing required columns for CrashRecord mapping: "
                + ", ".join(missing)
            )

        # If we re-import the same dataset, replace its CrashRecords to avoid duplicates.
        existing_qs = CrashRecord.objects.filter(dataset=dataset)
        existing_count = existing_qs.count()
        if existing_count:
            self.stdout.write(
                self.style.WARNING(
                    f"Dataset {dataset.id} already has {existing_count} CrashRecord rows; "
                    "they will be deleted and replaced."
                )
            )
            existing_qs.delete()

        records: list[CrashRecord] = []

        for _, row in cleaned_df.iterrows():
            try:
                crash_id = str(row["crash_id"])
            except KeyError:
                # Should not happen because we already checked required_cols above.
                continue

            crash_date_raw = row.get("crash_date")
            if isinstance(crash_date_raw, pd.Timestamp):
                crash_dt = crash_date_raw.to_pydatetime()
            elif isinstance(crash_date_raw, datetime):
                crash_dt = crash_date_raw
            else:
                # Let pandas parse strings or date-like objects.
                crash_dt = pd.to_datetime(crash_date_raw, errors="coerce")
                if isinstance(crash_dt, pd.Series):
                    # to_datetime on scalar sometimes returns Series in older pandas;
                    # guard against that by taking the first element.
                    crash_dt = crash_dt.iloc[0]

            if pd.isna(crash_dt):
                # If we cannot parse the date, skip this row rather than crashing the import.
                continue

            if timezone.is_naive(crash_dt):
                crash_dt = timezone.make_aware(crash_dt, timezone.get_current_timezone())

            severity_raw = row.get("severity")
            severity = str(severity_raw).strip().upper() if severity_raw is not None else ""

            # Only accept MMUCC KABCO severity codes; skip rows with invalid codes.
            if severity not in {"K", "A", "B", "C", "O"}:
                continue

            lat = row.get("latitude")
            lon = row.get("longitude")
            location = None
            try:
                if lat is not None and lon is not None and not (pd.isna(lat) or pd.isna(lon)):
                    location = Point(float(lon), float(lat), srid=4326)
            except Exception:
                # Leave location as None if parsing fails.
                location = None

            posted_speed_limit = None
            if "posted_speed_limit" in cleaned_df.columns:
                val = row.get("posted_speed_limit")
                if val is not None and not pd.isna(val):
                    try:
                        posted_speed_limit = int(val)
                    except (TypeError, ValueError):
                        posted_speed_limit = None

            vehicle_count = None
            if "vehicle_count" in cleaned_df.columns:
                val = row.get("vehicle_count")
                if val is not None and not pd.isna(val):
                    try:
                        vehicle_count = int(val)
                    except (TypeError, ValueError):
                        vehicle_count = None

            person_count = None
            if "person_count" in cleaned_df.columns:
                val = row.get("person_count")
                if val is not None and not pd.isna(val):
                    try:
                        person_count = int(val)
                    except (TypeError, ValueError):
                        person_count = None

            roadway_name = ""
            if "roadway_name" in cleaned_df.columns:
                rn = row.get("roadway_name")
                roadway_name = "" if rn is None or pd.isna(rn) else str(rn)

            municipality = ""
            if "municipality" in cleaned_df.columns:
                m = row.get("municipality")
                municipality = "" if m is None or pd.isna(m) else str(m)

            record = CrashRecord(
                dataset=dataset,
                crash_id=crash_id,
                crash_datetime=crash_dt,
                severity=severity,
                location=location,
                roadway_name=roadway_name,
                municipality=municipality,
                posted_speed_limit=posted_speed_limit,
                vehicle_count=vehicle_count,
                person_count=person_count,
            )
            records.append(record)

        if not records:
            raise CommandError(
                "No CrashRecord rows were constructed from the cleaned dataset. "
                "Check that severity codes and crash_date values are valid."
            )

        CrashRecord.objects.bulk_create(records, batch_size=1000)

        self.stdout.write(
            self.style.SUCCESS(
                f"Imported {len(records)} CrashRecord rows for UploadedDataset {dataset.id}."
            )
        )

================================================================================
===== END FILE: crashdata\management\commands\import_crash_records.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\models.py =====
================================================================================

from django.contrib.gis.db import models as gis_models
from django.contrib.gis.db.models import indexes as gis_indexes
from django.db import models

from ingestion.models import UploadedDataset


class CrashRecord(gis_models.Model):
    """Crash-level record aligned with core MMUCC / KABCO fields.

    This model is deliberately minimal; additional fields can be added by the
    data engineering or ML teams as needed.
    """

    SEVERITY_CHOICES = [
        ("K", "Fatal (K)"),
        ("A", "Suspected serious injury (A)"),
        ("B", "Suspected minor injury (B)"),
        ("C", "Possible injury (C)"),
        ("O", "Property damage only (O)"),
    ]

    dataset = models.ForeignKey(
        UploadedDataset,
        on_delete=models.CASCADE,
        related_name="crash_records",
    )
    crash_id = models.CharField(max_length=64)
    crash_datetime = models.DateTimeField()
    severity = models.CharField(max_length=1, choices=SEVERITY_CHOICES)

    # Geospatial location; stored in PostGIS.
    location = gis_models.PointField(
        geography=True,
        null=True,
        blank=True,
        help_text="Crash location in WGS84 (lon/lat).",
    )

    roadway_name = models.CharField(max_length=128, blank=True)
    municipality = models.CharField(max_length=128, blank=True)

    posted_speed_limit = models.PositiveIntegerField(null=True, blank=True)
    vehicle_count = models.PositiveIntegerField(null=True, blank=True)
    person_count = models.PositiveIntegerField(null=True, blank=True)

    created_at = models.DateTimeField(auto_now_add=True)

    class Meta:
        indexes = [
            models.Index(fields=["crash_datetime"]),
            models.Index(fields=["severity"]),
            models.Index(fields=["dataset"]),
            gis_indexes.GiSTIndex(fields=["location"]),
        ]

    def __str__(self) -> str:
        return f"Crash {self.crash_id} ({self.get_severity_display()})"

================================================================================
===== END FILE: crashdata\models.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\queries.py =====
================================================================================

from datetime import datetime
from typing import Iterable, Optional

from django.contrib.gis.geos import Polygon
from django.db.models import Count, QuerySet

from ingestion.models import UploadedDataset

from .models import CrashRecord


def _apply_common_filters(
    qs: QuerySet,
    severity: Optional[Iterable[str]] = None,
    municipality: str | None = None,
    start_datetime: datetime | None = None,
    end_datetime: datetime | None = None,
) -> QuerySet:
    if severity:
        if isinstance(severity, (list, tuple, set)):
            qs = qs.filter(severity__in=list(severity))
        else:
            qs = qs.filter(severity=severity)
    if municipality:
        qs = qs.filter(municipality__iexact=municipality)
    if start_datetime is not None:
        qs = qs.filter(crash_datetime__gte=start_datetime)
    if end_datetime is not None:
        qs = qs.filter(crash_datetime__lt=end_datetime)
    return qs


def severity_histogram(
    dataset: Optional[UploadedDataset] = None,
    *,
    municipality: str | None = None,
    start_datetime: datetime | None = None,
    end_datetime: datetime | None = None,
):
    """Aggregate crashes by KABCO severity for a given dataset.

    Returns a queryset of dicts: [{"severity": "K", "count": 123}, ...]
    """
    qs: QuerySet = CrashRecord.objects.all()
    if dataset is not None:
        qs = qs.filter(dataset=dataset)

    qs = _apply_common_filters(
        qs,
        municipality=municipality,
        start_datetime=start_datetime,
        end_datetime=end_datetime,
    )
    return qs.values("severity").annotate(count=Count("id")).order_by("severity")


def crashes_within_bbox(
    dataset: Optional[UploadedDataset],
    min_lon: float,
    min_lat: float,
    max_lon: float,
    max_lat: float,
    *,
    severity: Optional[Iterable[str]] = None,
    municipality: str | None = None,
    start_datetime: datetime | None = None,
    end_datetime: datetime | None = None,
) -> QuerySet:
    """Return crash records that fall within the provided bounding box."""
    bbox = Polygon.from_bbox((min_lon, min_lat, max_lon, max_lat))
    qs: QuerySet = CrashRecord.objects.filter(location__within=bbox)
    if dataset is not None:
        qs = qs.filter(dataset=dataset)

    qs = _apply_common_filters(
        qs,
        severity=severity,
        municipality=municipality,
        start_datetime=start_datetime,
        end_datetime=end_datetime,
    )
    return qs

================================================================================
===== END FILE: crashdata\queries.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\urls.py =====
================================================================================

from django.urls import path

from . import views

urlpatterns = [
    path(
        "severity-histogram/",
        views.severity_histogram_view,
        name="crashdata-severity-histogram",
    ),
    path(
        "crashes-within-bbox/",
        views.crashes_within_bbox_view,
        name="crashdata-crashes-within-bbox",
    ),
    path(
        "heatmap/",
        views.heatmap_view,
        name="crashdata-heatmap",
    ),
    path(
        "exports/crashes.csv",
        views.export_crashes_csv,
        name="crashdata-export-crashes-csv",
    ),
]

================================================================================
===== END FILE: crashdata\urls.py =====
================================================================================

================================================================================
===== BEGIN FILE: crashdata\views.py =====
================================================================================

import csv
import io
import logging
from datetime import datetime, time

from django.http import Http404, HttpResponse, JsonResponse
from django.shortcuts import get_object_or_404
from django.utils import timezone
from django.utils.dateparse import parse_date, parse_datetime
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated

from ingestion.models import UploadedDataset

from . import queries
from .models import CrashRecord

logger = logging.getLogger(__name__)

DANGEROUS_CSV_PREFIXES = ("=", "+", "-", "@")


def _safe_csv_value(value) -> str:
    if value is None:
        return ""
    if not isinstance(value, str):
        value = str(value)
    if value.startswith(DANGEROUS_CSV_PREFIXES):
        return "'" + value
    return value


def _user_is_admin(user) -> bool:
    return bool(
        getattr(user, "is_superuser", False)
        or getattr(user, "is_staff", False)
        or user.groups.filter(name="Admin").exists()
    )


def _get_dataset_for_user(upload_id, user) -> UploadedDataset:
    dataset = get_object_or_404(UploadedDataset, id=upload_id)
    if dataset.owner_id != getattr(user, "id", None) and not _user_is_admin(user):
        raise Http404("No such upload.")
    return dataset


def _parse_datetime_param(raw: str | None):
    if not raw:
        return None

    dt = parse_datetime(raw)
    if dt is not None:
        if timezone.is_naive(dt):
            dt = timezone.make_aware(dt, timezone=timezone.utc)
        return dt

    d = parse_date(raw)
    if d is not None:
        dt = datetime.combine(d, time.min)
        return timezone.make_aware(dt, timezone=timezone.utc)

    return None


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def severity_histogram_view(request):
    """Return a severity histogram for a given upload (or all uploads)."""
    upload_id = request.query_params.get("upload_id")
    municipality = request.query_params.get("municipality") or None
    start_raw = request.query_params.get("start_datetime")
    end_raw = request.query_params.get("end_datetime")

    start_dt = _parse_datetime_param(start_raw)
    end_dt = _parse_datetime_param(end_raw)
    if (start_raw and start_dt is None) or (end_raw and end_dt is None):
        return JsonResponse(
            {
                "detail": "start_datetime and end_datetime must be ISO-8601 datetimes or dates."
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    dataset = None
    if upload_id:
        dataset = _get_dataset_for_user(upload_id, request.user)

    histogram_qs = queries.severity_histogram(
        dataset=dataset,
        municipality=municipality,
        start_datetime=start_dt,
        end_datetime=end_dt,
    )
    data = [{"severity": row["severity"], "count": row["count"]} for row in histogram_qs]

    return JsonResponse(
        {
            "upload_id": str(dataset.id) if dataset else None,
            "filters": {
                "municipality": municipality,
                "start_datetime": start_dt.isoformat() if start_dt else None,
                "end_datetime": end_dt.isoformat() if end_dt else None,
            },
            "histogram": data,
        }
    )


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def crashes_within_bbox_view(request):
    """Return crashes inside a bounding box as a GeoJSON FeatureCollection."""
    upload_id = request.query_params.get("upload_id")
    required_params = ["min_lon", "min_lat", "max_lon", "max_lat"]
    try:
        min_lon = float(request.query_params["min_lon"])
        min_lat = float(request.query_params["min_lat"])
        max_lon = float(request.query_params["max_lon"])
        max_lat = float(request.query_params["max_lat"])
    except KeyError as missing:
        return JsonResponse(
            {"detail": f"Missing required parameter: {missing.args[0]}", "required": required_params},
            status=status.HTTP_400_BAD_REQUEST,
        )
    except ValueError:
        return JsonResponse(
            {"detail": "Bounding box parameters must be valid floats."},
            status=status.HTTP_400_BAD_REQUEST,
        )

    dataset = None
    if upload_id:
        dataset = _get_dataset_for_user(upload_id, request.user)

    severity_param = request.query_params.get("severity") or ""
    if severity_param:
        severity = [s.strip() for s in severity_param.split(",") if s.strip()]
    else:
        severity = None

    municipality = request.query_params.get("municipality") or None
    start_dt = _parse_datetime_param(request.query_params.get("start_datetime"))
    end_dt = _parse_datetime_param(request.query_params.get("end_datetime"))
    if (
        request.query_params.get("start_datetime")
        and start_dt is None
    ) or (
        request.query_params.get("end_datetime")
        and end_dt is None
    ):
        return JsonResponse(
            {
                "detail": "start_datetime and end_datetime must be ISO-8601 datetimes or dates."
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    try:
        limit = int(request.query_params.get("limit", "5000"))
    except ValueError:
        return JsonResponse(
            {"detail": "limit must be an integer."},
            status=status.HTTP_400_BAD_REQUEST,
        )
    if limit <= 0 or limit > 50000:
        limit = 5000

    qs = queries.crashes_within_bbox(
        dataset=dataset,
        min_lon=min_lon,
        min_lat=min_lat,
        max_lon=max_lon,
        max_lat=max_lat,
        severity=severity,
        municipality=municipality,
        start_datetime=start_dt,
        end_datetime=end_dt,
    ).select_related("dataset")

    features = []
    for crash in qs[:limit]:
        if not crash.location:
            continue
        lon, lat = crash.location.coords
        features.append(
            {
                "type": "Feature",
                "geometry": {"type": "Point", "coordinates": [lon, lat]},
                "properties": {
                    "id": crash.id,
                    "crash_id": crash.crash_id,
                    "severity": crash.severity,
                    "crash_datetime": crash.crash_datetime.isoformat(),
                    "roadway_name": crash.roadway_name,
                    "municipality": crash.municipality,
                    "posted_speed_limit": crash.posted_speed_limit,
                    "dataset_id": str(crash.dataset_id),
                },
            }
        )

    return JsonResponse(
        {
            "type": "FeatureCollection",
            "count": len(features),
            "limit": limit,
            "bbox": [min_lon, min_lat, max_lon, max_lat],
            "features": features,
        }
    )


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def heatmap_view(request):
    """Return a lightweight grid-aggregated heatmap for crashes in a bbox."""
    upload_id = request.query_params.get("upload_id")
    try:
        min_lon = float(request.query_params["min_lon"])
        min_lat = float(request.query_params["min_lat"])
        max_lon = float(request.query_params["max_lon"])
        max_lat = float(request.query_params["max_lat"])
    except KeyError as missing:
        return JsonResponse(
            {
                "detail": f"Missing required parameter: {missing.args[0]}",
                "required": ["min_lon", "min_lat", "max_lon", "max_lat"],
            },
            status=status.HTTP_400_BAD_REQUEST,
        )
    except ValueError:
        return JsonResponse(
            {"detail": "Bounding box parameters must be valid floats."},
            status=status.HTTP_400_BAD_REQUEST,
        )

    dataset = None
    if upload_id:
        dataset = _get_dataset_for_user(upload_id, request.user)

    try:
        grid_size = float(request.query_params.get("grid_size", "0.05"))
    except ValueError:
        return JsonResponse(
            {"detail": "grid_size must be a float representing degrees."},
            status=status.HTTP_400_BAD_REQUEST,
        )
    if grid_size <= 0:
        grid_size = 0.05

    severity_param = request.query_params.get("severity") or ""
    if severity_param:
        severity = [s.strip() for s in severity_param.split(",") if s.strip()]
    else:
        severity = None

    municipality = request.query_params.get("municipality") or None
    start_dt = _parse_datetime_param(request.query_params.get("start_datetime"))
    end_dt = _parse_datetime_param(request.query_params.get("end_datetime"))

    qs = queries.crashes_within_bbox(
        dataset=dataset,
        min_lon=min_lon,
        min_lat=min_lat,
        max_lon=max_lon,
        max_lat=max_lat,
        severity=severity,
        municipality=municipality,
        start_datetime=start_dt,
        end_datetime=end_dt,
    ).only("location")

    # Aggregate into simple grid cells.
    buckets: dict[tuple[int, int], int] = {}
    for crash in qs:
        if not crash.location:
            continue
        lon, lat = crash.location.coords
        x = int(lon // grid_size)
        y = int(lat // grid_size)
        key = (x, y)
        buckets[key] = buckets.get(key, 0) + 1

    cells = []
    for (x, y), count in buckets.items():
        center_lon = (x + 0.5) * grid_size
        center_lat = (y + 0.5) * grid_size
        cells.append(
            {
                "count": count,
                "center": [center_lon, center_lat],
                "grid_size": grid_size,
            }
        )

    return JsonResponse(
        {
            "bbox": [min_lon, min_lat, max_lon, max_lat],
            "grid_size": grid_size,
            "cells": cells,
        }
    )


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def export_crashes_csv(request):
    """Export a filtered subset of crashes as CSV, with a hard row cap."""
    upload_id = request.query_params.get("upload_id")
    if not upload_id:
        return JsonResponse(
            {"detail": "upload_id is required for crash exports."},
            status=status.HTTP_400_BAD_REQUEST,
        )

    dataset = _get_dataset_for_user(upload_id, request.user)

    severity_param = request.query_params.get("severity") or ""
    if severity_param:
        severity = [s.strip() for s in severity_param.split(",") if s.strip()]
    else:
        severity = None

    municipality = request.query_params.get("municipality") or None
    start_dt = _parse_datetime_param(request.query_params.get("start_datetime"))
    end_dt = _parse_datetime_param(request.query_params.get("end_datetime"))

    qs = CrashRecord.objects.filter(dataset=dataset)
    if severity:
        qs = qs.filter(severity__in=severity)
    if municipality:
        qs = qs.filter(municipality__iexact=municipality)
    if start_dt is not None:
        qs = qs.filter(crash_datetime__gte=start_dt)
    if end_dt is not None:
        qs = qs.filter(crash_datetime__lt=end_dt)

    try:
        max_rows = int(request.query_params.get("max_rows", "100000"))
    except ValueError:
        return JsonResponse(
            {"detail": "max_rows must be an integer."},
            status=status.HTTP_400_BAD_REQUEST,
        )
    if max_rows <= 0 or max_rows > 500000:
        max_rows = 100000

    row_count = qs.count()
    if row_count > max_rows:
        return JsonResponse(
            {
                "detail": (
                    "Refusing to export more than max_rows rows in a single request."
                ),
                "row_count": row_count,
                "max_rows": max_rows,
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    buffer = io.StringIO()
    writer = csv.writer(buffer)

    header = [
        "id",
        "dataset_id",
        "crash_id",
        "crash_datetime",
        "severity",
        "roadway_name",
        "municipality",
        "posted_speed_limit",
        "vehicle_count",
        "person_count",
        "lon",
        "lat",
    ]
    writer.writerow(header)

    for crash in qs.iterator():
        lon = lat = ""
        if crash.location:
            lon, lat = crash.location.coords
        writer.writerow(
            [
                crash.id,
                crash.dataset_id,
                _safe_csv_value(crash.crash_id),
                crash.crash_datetime.isoformat(),
                crash.severity,
                _safe_csv_value(crash.roadway_name),
                _safe_csv_value(crash.municipality),
                crash.posted_speed_limit,
                crash.vehicle_count,
                crash.person_count,
                lon,
                lat,
            ]
        )

    csv_bytes = buffer.getvalue().encode("utf-8")
    response = HttpResponse(csv_bytes, content_type="text/csv")
    response["Content-Disposition"] = (
        f"attachment; filename=crashes_{dataset.id}.csv"
    )
    return response


# Throttle scope for exports
export_crashes_csv.throttle_scope = "exports"  # type: ignore[attr-defined]

================================================================================
===== END FILE: crashdata\views.py =====
================================================================================

================================================================================
===== BEGIN FILE: docker-compose.yml =====
================================================================================

version: "3.9"

services:
  db:
    image: postgis/postgis:16-3.4
    environment:
      POSTGRES_DB: alaska_crash_analysis
      POSTGRES_USER: alaska
      POSTGRES_PASSWORD: alaska
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  clamav:
    image: clamav/clamav:latest
    ports:
      - "3310:3310"

  web:
    build: .
    command: gunicorn alaska_project.wsgi:application --bind 0.0.0.0:8000
    env_file:
      - .env
    environment:
      POSTGRES_DB: alaska_crash_analysis
      POSTGRES_USER: alaska
      POSTGRES_PASSWORD: alaska
      POSTGRES_HOST: db
      DJANGO_DEBUG: "false"
      CLAMAV_TCP_HOST: clamav
      CLAMAV_TCP_PORT: 3310
    depends_on:
      - db
      - clamav
    ports:
      - "8000:8000"
    volumes:
      - .:/app
      - media_data:/app/media

volumes:
  postgres_data:
  media_data:

================================================================================
===== END FILE: docker-compose.yml =====
================================================================================

================================================================================
===== BEGIN FILE: Dockerfile =====
================================================================================

FROM python:3.12-slim

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

WORKDIR /app

# System dependencies for Postgres + GeoDjango
RUN apt-get update && apt-get install -y --no-install-recommends         build-essential         libpq-dev         binutils         libproj-dev         gdal-bin         && rm -rf /var/lib/apt/lists/*

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# Collect static files if the frontend has been built into the expected path.
RUN python manage.py collectstatic --noinput || true

CMD ["gunicorn", "alaska_project.wsgi:application", "--bind", "0.0.0.0:8000"]

================================================================================
===== END FILE: Dockerfile =====
================================================================================

================================================================================
===== BEGIN FILE: docs\crash_etl.md =====
================================================================================

# Crash ETL and ML workflow

This document ties together the ingestion app, the crash ETL step, and the
machinelearning worker. It assumes the Alaska MMUCCstyle schema defined in
`ingestion/config/mmucc_schema.yml`.

The highlevel flow is:

1. **Raw CSV/Parquet upload  `UploadedDataset` (ingestion app).**
2. **`import_crash_records` management command  cleaned, normalized `CrashRecord`.**
3. **Model worker  uses those cleaned records (and the same cleaning utilities)
   to build MLready feature matrices and train/evaluate models.**

---

## 1. Ingestion: raw file  `UploadedDataset`

The ingestion gateway (FR1) is responsible for:

* Accepting an uploaded CSV or Parquet file.
* Validating it against the configured schema (e.g. `mmucc-alaska-v1`).
* Persisting the file and validation metadata into an `UploadedDataset` row.

Key pieces:

* **Model:** `ingestion.models.UploadedDataset`
* **Parser:** `ingestion.validation.load_dataframe_from_bytes`

The `UploadedDataset.raw_file` field always contains the original bytes; the
cleaning/ML stack never mutates this file in place.

---

## 2. Crash ETL: `python manage.py import_crash_records <upload_id>`

The new management command lives at
`crashdata/management/commands/import_crash_records.py`.

### What the command does

1. **Lookup the dataset** by `upload_id` using `UploadedDataset.objects.get(id=...)`.
2. **Reparse the raw file** using
   `ingestion.validation.load_dataframe_from_bytes` so the ETL step sees the
   same column types that the ingestion endpoint validated.
3. **Run the refactored cleaning pipeline** from
   `analysis.ml_core.cleaning.clean_crash_dataframe_for_import`:

   * `discover_unknown_placeholders(df, base_unknowns)`
      * Scans string/object columns for terms that look like "unknown" values
        using the configured `DEFAULT_UNKNOWN_STRINGS` and
        `GENERIC_UNKNOWN_SUBSTRINGS` sets.
      * Returns an augmented set of tokens that should be treated as missing.

   * `profile_columns(df, unknown_strings)`
      * Computes rich percolumn diagnostics:
        * `% unknown` values,
        * number of unique values,
        * dominant value frequency,
        * whether a column behaves like a Yes/No flag and how balanced it is.

   * `suggest_columns_to_drop(df, column_stats, ...)`
      * Applies heuristic rules to mark columns as lowvalue for modeling:
        * drop columns where the share of unknowns exceeds `UNKNOWN_THRESHOLD`,
        * drop Yes/No columns that are extremely imbalanced based on
          `YES_NO_THRESHOLD`,
        * drop columns with either only one unique value or as many uniques as
          there are rows,
        * drop nearlyconstant columns where one value dominates 99.5% of
          nonmissing rows but there are still at least 25 nondominant values.
      * A core set of MMUCC columns is always **protected** from being dropped:
        `crash_id`, `crash_date`, `severity`, `kabco`, `latitude`, `longitude`.

   * The command uses the cleaned DataFrame returned by
     `clean_crash_dataframe_for_import` to map into `CrashRecord` rows and logs
     which columns were dropped. This cleaned DataFrame is also "modelready"
     in the sense that obviously bad or useless columns have been stripped out
     while keeping row alignment with the original data.

4. **Map the cleaned rows into `CrashRecord` objects** and bulkcreate them.

### Field mapping

For each row in the cleaned DataFrame, `import_crash_records` creates a
`CrashRecord` with:

* `dataset`  the `UploadedDataset` instance corresponding to `<upload_id>`.
* `crash_id`  from the `crash_id` column (coerced to string).
* `crash_datetime`  parsed from `crash_date`. If only a date is present it is
  interpreted as midnight in the Django project's current timezone.
* `severity`  from the `severity` column, **normalized to uppercase
  MMUCC KABCO codes**. Rows where the severity is not one of `{"K","A","B","C","O"}`
  are skipped so that the database only contains valid records.
* `location`  built from `longitude`/`latitude` (WGS84, SRID 4326). If either
  coordinate is missing or invalid, the `CrashRecord` is still created with
  `location=None`.
* `roadway_name`  optional, from the `roadway_name` column if present,
  otherwise empty string.
* `municipality`  optional, from the `municipality` column if present,
  otherwise empty string.
* `posted_speed_limit`  optional integer from `posted_speed_limit` if present
  and parsable.
* `vehicle_count`  optional integer from `vehicle_count` if present and
  parsable.
* `person_count`  optional integer from `person_count` if present and
  parsable.

Idempotency / overwrite behaviour:

* If a dataset already has `CrashRecord` rows, the command **deletes** them
  before bulkcreating the new set. This makes repeated imports for the same
  upload safe and avoids duplicate rows.

### Running the command

From the Django project root:

```bash
python manage.py import_crash_records <upload_id>
```

You can inspect the ingestion tests (`ingestion/tests/test_ingestion_gateway.py`)
for an example of a minimal, valid CSV; the same shape will work with this
command.

To see what would happen without writing to the database, use:

```bash
python manage.py import_crash_records <upload_id> --dry-run
```

This runs the cleaning pipeline and prints a summary (including dropped
columns), but does not create any `CrashRecord` rows.

---

## 3. Model worker: building MLready datasets

The model API and worker are described in more detail in `docs/model_api.md`.
This section explains how the **same cleaning stack** is intended to be used
from a training / inference worker.

The central entrypoint for modeling is
`analysis.ml_core.cleaning.build_ml_ready_dataset(df, ...)`, which wraps four
key steps:

1. **Unknown discovery**  `discover_unknown_placeholders`
2. **Severity mapping**  `find_severity_mapping`
3. **Leakage handling**  `find_leakage_columns`
4. (Optionally) posthoc leakage inspection  `warn_suspicious_importances`

### 3.1. Building `(X, y)` from a cleaned crash table

Given a pandas DataFrame `df` (for example built from a queryset against
`CrashRecord` or by rereading `UploadedDataset.raw_file`), the worker calls:

```python
from analysis.ml_core import cleaning

X, y, meta = cleaning.build_ml_ready_dataset(df)
```

Under the hood this performs:

1. **Unknown normalisation**

   ```python
   unknown_values = cleaning.discover_unknown_placeholders(df, cleaning.DEFAULT_UNKNOWN_STRINGS)
   df_clean = df.replace(list(unknown_values), pandas.NA)
   ```

2. **Severity column selection + mapping**

   * `guess_severity_column(df_clean)` tries a list of common names
     (`"severity"`, `"Crash Severity"`, `"crash_severity"`, etc.) and falls
     back to any column containing `"severity"`.
   * `find_severity_mapping(df_clean, severity_col)` then maps values in that
     column to the numeric labels `{0, 1, 2}`:
       * For MMUCC KABCO codes:
         * `K` or `A`  `2` (high severity),
         * `B` or `C`  `1` (medium),
         * `O`  `0` (low / propertydamageonly).
       * For purely numeric severities it buckets into low/medium/high based on
         the observed range.
       * For textual severities it searches for fatal/serious/minor keywords.

   Rows where the severity cannot be mapped are dropped from the modeling
   dataset (but not from the `CrashRecord` table).

3. **Feature matrix construction**

   * The raw severity column is dropped from the feature matrix `X`.
   * The mapped target values become `y` (integer dtype).

4. **Leakage detection and removal**

   * `find_leakage_columns(X, y)` performs two noninteractive checks:
       * **Namebased scan** via `suggest_leakage_by_name`, looking for column
         names that contain keywords like `"fatal"`, `"injury"`, `"severity"`,
         etc.
       * **Nearperfect predictors** via `find_near_perfect_predictors`, which
         looks for lowcardinality features that almost deterministically
         predict the severity labels on their own.
   * The union of these suggestions is dropped from `X` before training.

5. **Metadata for reproducibility**

   * The returned `meta` dictionary includes:
       * the chosen severity column and mapping,
       * the full list of discovered unknown tokens,
       * the set of dropped leakage columns,
       * row/column counts before and after filtering.

A typical model worker implementation would stash `meta` into
`ModelJob.result_metadata` (see `docs/model_api.md`) alongside the usual
metrics so that future teammates can reconstruct exactly how the dataset was
formed.

### 3.2. Posttraining leakage sanitycheck

After fitting a model that exposes feature importances, the worker can call:

```python
suspicious = cleaning.warn_suspicious_importances(feature_names, importances)
```

If a single feature dominates the importance scores (either absolutely or
relative to the runnerup), this helper logs a warning suggesting that the
feature may be a leakage column (for example, `"Number of Fatalities"` or a
duplicate severity code).

---

## 4. Putting it together

* **Where does cleaning happen?**  
  In the shared `analysis.ml_core.cleaning` module. Both the
  `import_crash_records` ETL step and the model worker call into the same
  functions (`discover_unknown_placeholders`, `profile_columns`,
  `find_severity_mapping`, `find_leakage_columns`).

* **Where is normalization to `CrashRecord` defined?**  
  In `crashdata/management/commands/import_crash_records.py`. That command
  takes the cleaned, unknownnormalised DataFrame and maps it to the
  `CrashRecord` schema (one row per crash, with coordinates, severity, and a
  small set of core attributes).

* **How does this feed modeling?**  
  A model worker can either:
  * read from the `CrashRecord` table, convert to a DataFrame, and then call
    `build_ml_ready_dataset`, or
  * reread `UploadedDataset.raw_file`, call
    `clean_crash_dataframe_for_import`, and then `build_ml_ready_dataset` on
    the result.

In both cases the worker relies on the same refactored, noninteractive
cleaning utilities, so any future teammate can follow the pipeline endtoend
from raw CSV  `UploadedDataset`  `CrashRecord`  `(X, y)` without having to
reverseengineer adhoc scripts.

================================================================================
===== END FILE: docs\crash_etl.md =====
================================================================================

================================================================================
===== BEGIN FILE: docs\deployment.md =====
================================================================================

# Deployment Guide

This document describes how to run the Alaska Crash Analysis stack in local
development and in a simple containerized (Docker) setup.

## 1. Environment variables

The Django settings module reads all sensitive values from the environment.

Required in non-DEBUG environments:

- `DJANGO_SECRET_KEY`
- `POSTGRES_DB`
- `POSTGRES_USER`
- `POSTGRES_PASSWORD`
- `POSTGRES_HOST`
- `POSTGRES_PORT`

Recommended (but optional) variables:

- `DJANGO_DEBUG`
- `DJANGO_ALLOWED_HOSTS`
- `CORS_ALLOW_ALL_ORIGINS`
- `DJANGO_SECURE_SSL_REDIRECT`
- `DJANGO_SECURE_HSTS_SECONDS`
- `DJANGO_SECURE_HSTS_INCLUDE_SUBDOMAINS`
- `DJANGO_SECURE_HSTS_PRELOAD`
- `INGESTION_MAX_FILE_SIZE_BYTES`
- `INGESTION_ALLOWED_EXTENSIONS`
- `INGESTION_SCHEMA_CONFIG_PATH`
- `INGESTION_REQUIRE_AV`
- `CLAMAV_UNIX_SOCKET` or `CLAMAV_TCP_HOST` / `CLAMAV_TCP_PORT`

For ingestion, a typical production configuration is:

```bash
INGESTION_ALLOWED_EXTENSIONS=".csv,.parquet"
INGESTION_MAX_FILE_SIZE_BYTES=10485760  # 10 MB
```

Adjust these as needed for your environment (e.g. to restrict uploads to CSV
only, set `INGESTION_ALLOWED_EXTENSIONS=".csv"`).

## 2. Local development (without Docker)

1. Start Postgres/PostGIS locally and create a database:

   ```bash
   createdb alaska_crash_analysis
   ```

2. Create a virtual environment and install dependencies:

   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Windows: .venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. Configure environment variables (e.g. in a `.env` file or by exporting
   them in your shell).

   At minimum, you likely want:

   ```bash
   export DJANGO_DEBUG=true
   export POSTGRES_DB=alaska_crash_analysis
   export POSTGRES_USER=postgres
   export POSTGRES_PASSWORD=postgres
   export POSTGRES_HOST=localhost
   export POSTGRES_PORT=5432
   # Optional: ingestion tuning
   export INGESTION_ALLOWED_EXTENSIONS=".csv,.parquet"
   export INGESTION_MAX_FILE_SIZE_BYTES=10485760
   ```

4. Run migrations and start the dev server:

   ```bash
   python manage.py migrate
   python manage.py runserver
   ```

5. In another terminal, start the frontend dev server:

   ```bash
   cd alaska_ui
   npm install
   npm run dev
   ```

## 3. Docker / docker-compose

A minimal `docker-compose.yml` is provided with three services:

- `db`  Postgres + PostGIS
- `clamav`  ClamAV daemon for antivirus scanning
- `web`  Django

================================================================================
===== END FILE: docs\deployment.md =====
================================================================================

================================================================================
===== BEGIN FILE: docs\ingestion_errors.md =====
================================================================================

# Ingestion error codes

The upload gateway uses **stable, machinereadable error codes** for any
hardfail condition. These codes appear:

- As the toplevel `error_code` field when `overall_status == "rejected"`.
- On one of the `steps[i].code` entries for the step that caused the failure.

This document is the source of truth for codes used by FR1 (data ingestion &
validation). When adding new hardfail conditions in the future, extend this
table and keep codes stable.

---

## Error code reference

| error_code              | Step            | Typical severity | Description (for developers)                                              | Suggested user action                                                  |
| ----------------------- | --------------- | ---------------- | ------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `PAYLOAD_MISSING_FILE`  | `PAYLOAD`       | error            | No file was sent in the `file` field of the multipart/form-data request. | Reupload with a file attached under the `file` field.                 |
| `EXTENSION_NOT_ALLOWED` | `EXTENSION_CHECK` | error          | File extension is not in `INGESTION_ALLOWED_EXTENSIONS`.                  | Convert the file to an allowed type (e.g. `.csv` / `.parquet`) or ask an admin to update the configuration. |
| `READ_FAILED`           | `READ_FILE`     | error            | Django could not read the uploaded file stream.                           | Retry the upload; if it recurs, check client/network and server logs.  |
| `FILE_TOO_LARGE`        | `FILE_SIZE`     | error            | File size exceeds `INGESTION_MAX_FILE_SIZE_BYTES`.                        | Split the file or lower its size, or have an admin raise the limit.    |
| `MIME_MISMATCH`         | `MIME_SNIFF`    | error            | Declared `Content-Type` does not match the detected MIME type.           | Save/export the file again so the MIME type matches its actual content, then reupload. |
| `UPLOAD_INFECTED`       | `AV_SCAN`       | error            | ClamAV reports the file as infected (malware detected).                   | Clean or regenerate the file, verify it is virusfree, then reupload. |
| `AV_UNAVAILABLE_REQUIRED` | `AV_SCAN`     | error            | Antivirus scanning is required but was skipped (e.g., AV offline).       | Retry later or contact an administrator to restore AV scanning.        |
| `PARSE_FAILED`          | `PARSE_TABLE`   | error            | Pandas failed to parse the file into a tabular dataset.                  | Ensure the file is valid CSV/Parquet (no truncation or mixed formats) and try again. |
| `SCHEMA_MISSING_COLUMNS`| `SCHEMA_CHECK`  | error            | One or more `required: true` columns from `mmucc_schema.yml` are missing. | Update the dataset to include all required columns, then reupload.    |

---

## Notes

- All of the codes above correspond to **hardfail** conditions in the pipeline.
  When one of these occurs:
  - `overall_status` is `"rejected"`.
  - The upload is **not** persisted as an `UploadedDataset`.
- Other validation steps (type/range and geo checks) are **softfail**:
  - They only affect perstep `severity` (`"warning"`) and row counts.
  - They never set a toplevel `error_code` or cause the upload to be rejected.
- The UI can rely on:
  - `steps[i].severity == "error"` and `steps[i].is_hard_fail == true` to mark
    blocking issues.
  - The toplevel `error_code` to render userfriendly error screens or
    contextual documentation.

When adding new hardfail conditions, make sure to:

1. Introduce a new, descriptive, UPPER_SNAKE_CASE error code.
2. Set it both on the failing step (`code`) and at the top level (`error_code`).
3. Document it in the table above.

================================================================================
===== END FILE: docs\ingestion_errors.md =====
================================================================================

================================================================================
===== BEGIN FILE: docs\model_api.md =====
================================================================================

# Model API Contract

This document defines the non-ML interface for running crash analysis models via
the Django backend. The ML / serving team is responsible for wiring these
endpoints into their own pipelines.

## Endpoints

### `POST /api/models/run/`

Start a new model job.

**Authentication:** required (session or token).

**Request body (JSON):**

```json
{
  "upload_id": "uuid-of-UploadedDataset",
  "model": "crash_severity_risk_v1",
  "parameters": {
    "exclude_invalid_rows": true,
    "geography_filter": {
      "municipality": "Anchorage"
    }
  }
}
```

- `upload_id` (string, required)
  - UUID of an `ingestion.UploadedDataset` with `status="accepted"`.
- `model` (string, required)
  - Identifier of the model to run. Must be one of:
    - `crash_severity_risk_v1`
    - `ebm_v1`
  - This list is defined in `analysis.views.SUPPORTED_MODELS` and can be extended
    in a backwards-compatible way.
- `parameters` (object, optional)
  - Free-form JSON object interpreted by the ML pipeline. The backend stores
    this verbatim in `ModelJob.parameters`, with light validation for an
    optional `cleaning` sub-object:
    - `cleaning.unknown_threshold` (number, optional, 0100)
    - `cleaning.yes_no_threshold` (number, optional, 0100)
    - `cleaning.columns_to_drop` (array of strings, optional)

**Responses:**

- `202 Accepted`  job created

  ```json
  {
    "job_id": "00000000-0000-0000-0000-000000000000",
    "status": "queued",
    "upload_id": "uuid-of-UploadedDataset",
    "model": "crash_severity_risk_v1",
    "parameters": { "...": "..." },
    "detail": "Job has been created and queued."
  }
  ```

- `400 Bad Request`  validation error (missing fields, unsupported model, non-object parameters)
- `403 Forbidden`  upload is not owned by the caller
- `404 Not Found`  upload does not exist

### `GET /api/models/results/<job_id>/`

Retrieve the status and high-level metadata for a model job.

**Authentication:** required.

**Path parameters:**

- `job_id`  UUID of the `ModelJob` row created by the `run` endpoint.

**Response body (JSON):**

```json
{
  "job_id": "00000000-0000-0000-0000-000000000000",
  "upload_id": "uuid-of-UploadedDataset",
  "model": "crash_severity_risk_v1",
  "status": "queued",
  "parameters": { "...": "..." },
  "result_metadata": {
    "message": "Job created and queued. Hook this into your task queue or model-serving pipeline."
  },
  "created_at": "2025-01-01T00:00:00Z",
  "updated_at": "2025-01-01T00:00:00Z"
}
```

**Status codes:**

- `202 Accepted`  job is `queued` or `running`
- `200 OK`  job is `succeeded` or `failed`
- `403 Forbidden`  job is not visible to the caller
- `404 Not Found`  job id unknown

## Responsibilities

- **App / API team (this repo):**
  - Maintain `ModelJob` schema and migrations.
  - Enforce permissions (only owners/Admin can see jobs for a given upload).
  - Provide stable request/response contracts for the ML team.

- **ML / data-cleaning team:**
  - Attach a task queue / worker that consumes `ModelJob` rows with status
    `queued`, runs models, writes results to storage of their choice, and
    updates `ModelJob.status` and `ModelJob.result_metadata` (e.g., summary
    metrics, links to result tables, etc.).
  - Keep the logic that interprets `parameters` in ML-owned code, not in
    the Django views.

================================================================================
===== END FILE: docs\model_api.md =====
================================================================================

================================================================================
===== BEGIN FILE: docs\schema_config.md =====
================================================================================

# Ingestion schema configuration (`mmucc_schema.yml`)

The upload gateway uses a configurable schema file at:

```text
ingestion/config/mmucc_schema.yml
```

This file describes the MMUCC / KABCOaligned schema that is enforced at **upload
time**. Non-developers can safely adjust required columns, labels, and basic
ranges by editing this YAML file and restarting Django  no Python changes are
needed.

>  **Important:** Invalid YAML syntax will prevent Django from starting.
> Always keep a backup of the previous version and make small, incremental
> changes.

---

## Toplevel structure

The file has three main sections:

```yaml
schema_version: "2024.1"
columns:
  crash_id:
    label: "Crash ID"
    required: true
    type: string
  crash_date:
    label: "Crash Date"
    required: true
    type: date
  # ...
geo_bounds:
  latitude:
    min: 51.0
    max: 72.0
  longitude:
    min: -170.0
    max: -130.0
```

### `schema_version`

- A humanreadable version string (e.g. `"2024.1"`).
- Returned in the upload response and stored on `UploadedDataset.schema_version`.
- Changing this string is a simple way to track schema revisions.

### `columns`

A mapping from **column key**  configuration object. The column key must match
the exact column name in the uploaded CSV / Parquet file (case sensitive).

Each column has the following fields:

- `label` (string)
  - A friendly, humanreadable name used in UI and reports.
- `required` (boolean)
  - `true`  this column **must** be present in uploads.
  - If any `required: true` columns are missing, the `SCHEMA_CHECK` step fails,
    the upload is **rejected** with error code `SCHEMA_MISSING_COLUMNS`, and
    `overall_status` is `"rejected"`.
  - `false`  optional column. Missing optional columns are allowed; they will
    be recorded in the `unknown_columns` / `missing_columns` metadata but do
    not block ingestion.
- `type` (string)
  - Controls how type / range validation interprets the values.
  - Supported values include:
    - `string`
    - `int`
    - `float`
    - `date`
    - `category`
  - If the type does not match (e.g. a nonnumeric value for an `int` column),
    the row is counted as invalid in the value/type checks. These issues are
    **softfail**: the upload is still accepted, but the validation report
    will contain warnings.
- Optional `min` / `max` (numeric)
  - For numeric types (`int`, `float`), define allowed value ranges.
  - Rows with values outside this range increase `invalid_row_count` in the
    value/type checks but do **not** cause a hard rejection.

The validation report exposes these schema details under:

```json
"schema": {
  "schema_version": "...",
  "columns": { ... },
  "missing_columns": [ ... ],
  "unknown_columns": [ ... ]
}
```

### `geo_bounds`

Defines latitude / longitude ranges used by the GEO_CHECKS step:

```yaml
geo_bounds:
  latitude:
    min: 51.0
    max: 72.0
  longitude:
    min: -170.0
    max: -130.0
```

- `latitude.min` / `latitude.max`
- `longitude.min` / `longitude.max`

Rows with coordinates outside these ranges are counted as invalid in the
geobounds check. This is also a **softfail**: the upload is still accepted,
but the GEO_CHECKS step will have `severity: "warning"` and nonzero
`invalid_row_count`.

If an upload has no usable latitude/longitude columns, the GEO_CHECKS step is
still recorded but treated as informational.

---

## Common edits

### 1. Adding a new optional column

Suppose you want to add a new optional `driver_phone` field as a string:

```yaml
columns:
  driver_phone:
    label: "Driver Phone"
    required: false
    type: string
```

- Uploads that include `driver_phone` will have the values validated as
  strings.
- Uploads that omit `driver_phone` are still accepted.

### 2. Making an existing column required

To require `posted_speed_limit` to be present in all uploads:

```yaml
columns:
  posted_speed_limit:
    label: "Posted Speed Limit"
    required: true
    type: int
    min: 0
    max: 120
```

Impact on the ingestion pipeline:

- If `posted_speed_limit` is missing from the uploaded file:
  - `SCHEMA_CHECK` step  `status: "failed"`, `severity: "error"`,
    `is_hard_fail: true`, `code: "SCHEMA_MISSING_COLUMNS"`.
  - `overall_status` becomes `"rejected"` and no dataset is stored.
- If the column is present but some rows have outofrange values:
  - The upload is **accepted**.
  - Value/range checks count invalid rows and report them via
    `row_checks.invalid_row_count` and a `TYPE_AND_RANGE_CHECKS` step with
    `severity: "warning"`.

### 3. Adjusting numeric ranges

To tighten the allowed age range:

```yaml
columns:
  driver_age:
    label: "Driver Age"
    required: false
    type: int
    min: 16
    max: 100
```

- Any row with `driver_age` < 16 or > 100 will:
  - Increase `invalid_row_count` in the value/range checks.
  - Leave the overall upload **accepted**, but with warnings.

### 4. Updating geo bounds

To widen the acceptable longitude range:

```yaml
geo_bounds:
  latitude:
    min: 51.0
    max: 72.0
  longitude:
    min: -180.0
    max: -130.0
```

- Future uploads will use the new bounds when computing
  `geo_checks.invalid_row_count`.

---

## When do changes take effect?

- The schema file is loaded at Django startup.
- After editing `mmucc_schema.yml`, you **must restart** the Django server
  (or the worker process if running under a process manager) for changes to
  take effect.

---

## Relationship to validation output

A few key fields in the validation JSON are directly driven by this file:

- `schema_version`
  - Comes from the YAML `schema_version`.
- `schema.missing_columns`
  - Any `required: true` columns that are absent from the upload.
- `schema.unknown_columns`
  - Columns present in the upload but not listed under `columns` in the YAML.
- `row_checks.total_rows` / `row_checks.invalid_row_count`
  - Computed using the `type`, `min`, and `max` configuration for each column.
- `geo_checks.invalid_row_count`
  - Computed using `geo_bounds`.

By keeping `mmucc_schema.yml` aligned with the MMUCC spec and local policy,
you control what the upload gateway considers "required" versus "nice to have"
and how strict basic data quality checks should be.

================================================================================
===== END FILE: docs\schema_config.md =====
================================================================================

================================================================================
===== BEGIN FILE: flatten_repo.py =====
================================================================================

#!/usr/bin/env python
"""
flatten_repo.py

Walks the repository and writes a single text file containing:

  - All developer-relevant file paths (relative to repo root)
  - The full contents of each included file

Each file is delimited in the output with BEGIN/END markers.

Usage:
    python flatten_repo.py
    python flatten_repo.py --root PATH/TO/REPO --output repo_flattened.txt
"""

import argparse
import os
from pathlib import Path
from typing import Iterable, List

# Directories we typically don't care about
EXCLUDED_DIR_NAMES = {
    ".git",
    ".hg",
    ".svn",
    ".mypy_cache",
    ".pytest_cache",
    ".ruff_cache",
    ".idea",
    ".vscode",
    ".vs",
    "__pycache__",
    ".ipynb_checkpoints",
    ".DS_Store",
    ".cache",
    "build",
    "dist",
    "site-packages",
    ".venv",
    "venv",
    "env",
    ".env",
    ".eggs",
    "node_modules",  # keep if you really want node_modules contents
}

# File extensions that are usually NOT edited by devs
EXCLUDED_FILE_EXTS = {
    ".pyc",
    ".pyo",
    ".pyd",
    ".so",
    ".dll",
    ".dylib",
    ".o",
    ".obj",
    ".a",
    ".lib",
    ".npy",
    ".npz",
    ".pkl",
    ".h5",
    ".hdf5",
    ".pt",
    ".ckpt",
    ".log",
    ".tmp",
}

# File extensions that developers usually *do* work on
INCLUDED_FILE_EXTS = {
    # Python / CUDA / C / C++
    ".py",
    ".pyi",
    ".pyx",
    ".pxd",
    ".c",
    ".cpp",
    ".cc",
    ".cxx",
    ".h",
    ".hpp",
    ".hh",
    ".hxx",
    ".cu",
    ".cuh",
    # Build / config
    ".cmake",
    ".cfg",
    ".ini",
    ".toml",
    ".json",
    ".yml",
    ".yaml",
    ".xml",
    ".conf",
    # Shell / scripting
    ".sh",
    ".bash",
    ".ps1",
    ".bat",
    ".cmd",
    # Docs
    ".md",
    ".rst",
    ".txt",
    ".tex",
    # Other "texty" project files
    ".csv",
    ".tsv",
}

# File *names* that are important even if they have no extension
IMPORTANT_FILENAMES = {
    "Makefile",
    "CMakeLists.txt",
    "Dockerfile",
    "LICENSE",
    "LICENSE.txt",
    "README",
    "README.md",
    "README.rst",
    ".gitignore",
    ".gitattributes",
    "pyproject.toml",
    "requirements.txt",
    "environment.yml",
    "Pipfile",
    "Pipfile.lock",
    "setup.py",
    "setup.cfg",
    "manage.py",
}


def should_skip_dir(dir_name: str) -> bool:
    """Return True if this directory name should be skipped entirely."""
    return dir_name in EXCLUDED_DIR_NAMES


def is_developer_file(path: Path) -> bool:
    """
    Decide whether a file is something developers are likely to work on.

    Heuristic: include if
      - extension in INCLUDED_FILE_EXTS, OR
      - basename in IMPORTANT_FILENAMES
    and NOT in EXCLUDED_FILE_EXTS.
    """
    if path.name in IMPORTANT_FILENAMES:
        return True

    ext = path.suffix.lower()

    if ext in EXCLUDED_FILE_EXTS:
        return False

    if ext in INCLUDED_FILE_EXTS:
        return True

    # By default, ignore unknown binary-ish extensions.
    return False


def collect_files(root: Path) -> List[Path]:
    """Walk the tree and collect all developer-relevant files."""
    files: List[Path] = []

    for dirpath, dirnames, filenames in os.walk(root):
        # Mutate dirnames in-place to skip excluded dirs
        dirnames[:] = [d for d in dirnames if not should_skip_dir(d)]

        dir_path = Path(dirpath)
        for fname in filenames:
            fpath = dir_path / fname
            if is_developer_file(fpath):
                files.append(fpath.relative_to(root))

    # Sort for deterministic output
    files.sort()
    return files


def write_files_with_contents(root: Path, paths: Iterable[Path], output_path: Path) -> None:
    """
    Write all files and their contents to a single text file.

    Each file is wrapped as:

        ===== BEGIN FILE: relative/path =====
        <contents>
        ===== END FILE: relative/path =====
    """
    with output_path.open("w", encoding="utf-8") as out:
        out.write("# Flattened repository file listing\n")
        out.write(f"# Root: {root}\n\n")

        for rel_path in paths:
            abs_path = root / rel_path
            out.write("=" * 80 + "\n")
            out.write(f"===== BEGIN FILE: {rel_path} =====\n")
            out.write("=" * 80 + "\n\n")

            try:
                with abs_path.open("r", encoding="utf-8", errors="replace") as f:
                    for line in f:
                        out.write(line)
            except Exception as e:
                # If we can't read the file, note the error and continue
                out.write(f"\n<<< ERROR READING FILE: {e} >>>\n")

            # Ensure there's a trailing newline, then write the end marker
            out.write("\n")
            out.write("=" * 80 + "\n")
            out.write(f"===== END FILE: {rel_path} =====\n")
            out.write("=" * 80 + "\n\n")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Generate a .txt file containing developer-relevant files and their contents."
        )
    )
    parser.add_argument(
        "--root",
        type=str,
        default=".",
        help="Root of the repository (default: current directory).",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="flattened_repo_with_contents.txt",
        help="Output text file (default: flattened_repo_with_contents.txt).",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    root = Path(args.root).resolve()
    output_path = Path(args.output).resolve()

    print(f"[flatten_repo] Scanning root: {root}")
    files = collect_files(root)
    print(f"[flatten_repo] Found {len(files)} developer-relevant files.")
    print(f"[flatten_repo] Writing contents to: {output_path}")

    write_files_with_contents(root, files, output_path)

    print(f"[flatten_repo] Done.")


if __name__ == "__main__":
    main()

================================================================================
===== END FILE: flatten_repo.py =====
================================================================================

================================================================================
===== BEGIN FILE: frontend\__init__.py =====
================================================================================


================================================================================
===== END FILE: frontend\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: frontend\apps.py =====
================================================================================

from django.apps import AppConfig


class FrontendConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "frontend"

================================================================================
===== END FILE: frontend\apps.py =====
================================================================================

================================================================================
===== BEGIN FILE: frontend\urls.py =====
================================================================================

from django.urls import path
from . import views

urlpatterns = [
    path("", views.index, name="frontend-index"),
]

================================================================================
===== END FILE: frontend\urls.py =====
================================================================================

================================================================================
===== BEGIN FILE: frontend\views.py =====
================================================================================

from django.shortcuts import render


def index(request):
    # Single-page application entrypoint
    return render(request, "frontend/index.html")

================================================================================
===== END FILE: frontend\views.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\__init__.py =====
================================================================================


================================================================================
===== END FILE: ingestion\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\admin.py =====
================================================================================

from django.contrib import admin

from .models import UploadedDataset


@admin.register(UploadedDataset)
class UploadedDatasetAdmin(admin.ModelAdmin):
    list_display = (
        "id",
        "owner",
        "original_filename",
        "schema_version",
        "size_bytes",
        "mime_type",
        "status",
        "created_at",
    )
    list_filter = ("status", "schema_version", "created_at")
    search_fields = ("original_filename", "owner__username", "schema_version")

================================================================================
===== END FILE: ingestion\admin.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\antivirus.py =====
================================================================================

import io
import logging
from typing import Dict

from django.conf import settings

try:
    import clamd  # type: ignore[import]
except Exception:  # noqa: BLE001
    clamd = None  # type: ignore[assignment]

logger = logging.getLogger(__name__)


def scan_bytes_with_clamav(data: bytes) -> Dict[str, str]:
    """
    Scan the provided bytes using a ClamAV daemon if one is available.

    Returns a dict with keys:
    - status: "passed", "failed", or "skipped"
    - details: human-readable string describing the outcome
    """
    if not data:
        return {"status": "passed", "details": "Empty payload; nothing to scan."}

    if clamd is None:
        return {
            "status": "skipped",
            "details": (
                "python-clamd is not installed; antivirus scanning was skipped. "
                "Install ClamAV and the clamd Python library to enable scanning."
            ),
        }

    try:
        unix_socket = getattr(settings, "CLAMAV_UNIX_SOCKET", None)
        tcp_host = getattr(settings, "CLAMAV_TCP_HOST", None)
        tcp_port = getattr(settings, "CLAMAV_TCP_PORT", 3310)

        if unix_socket:
            client = clamd.ClamdUnixSocket(path=unix_socket)
        elif tcp_host:
            client = clamd.ClamdNetworkSocket(host=tcp_host, port=tcp_port)
        else:
            # Fall back to default local Unix socket path
            client = clamd.ClamdUnixSocket()

        result = client.instream(io.BytesIO(data))
        # ClamAV returns {'stream': ('OK', None)} or ('FOUND', 'Malware-Name')
        stream_result = result.get("stream")
        if not stream_result:
            return {
                "status": "skipped",
                "details": f"Unexpected response from ClamAV: {result!r}",
            }

        outcome, signature = stream_result
        if outcome == "OK":
            return {
                "status": "passed",
                "details": "ClamAV did not find any malware in the uploaded file.",
            }
        if outcome == "FOUND":
            return {
                "status": "failed",
                "details": f"ClamAV detected malware: {signature}",
            }

        return {
            "status": "skipped",
            "details": f"Unexpected ClamAV outcome '{outcome}' with signature {signature!r}.",
        }
    except Exception as exc:  # noqa: BLE001
        logger.warning("ClamAV scan failed: %s", exc)
        return {
            "status": "skipped",
            "details": f"ClamAV scan could not be completed: {exc}",
        }

================================================================================
===== END FILE: ingestion\antivirus.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\apps.py =====
================================================================================

from django.apps import AppConfig


class IngestionConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "ingestion"

================================================================================
===== END FILE: ingestion\apps.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\config\mmucc_schema.yml =====
================================================================================

# MMUCC / KABCO-aligned schema configuration for the ingestion gateway.
#
# Non-developers can edit this file to add/remove columns and tweak ranges
# without touching Python code. Changes take effect after the Django server
# is restarted.

schema_version: "mmucc-alaska-v1"

columns:
  crash_id:
    label: "Crash ID"
    required: true
    type: string

  crash_date:
    label: "Crash Date"
    required: true
    type: date

  severity:
    label: "Injury Severity (KABCO)"
    required: true
    type: category

  kabco:
    label: "KABCO Code (optional)"
    required: false
    type: category

  latitude:
    required: true
    type: float
    min: 51.0
    max: 72.0

  longitude:
    required: true
    type: float
    min: -170.0
    max: -129.0

  posted_speed_limit:
    required: false
    type: int
    min: 0
    max: 85

  driver_age:
    required: false
    type: int
    min: 14
    max: 110

geo_bounds:
  latitude:
    min: 51.0
    max: 72.0
  longitude:
    min: -170.0
    max: -129.0

================================================================================
===== END FILE: ingestion\config\mmucc_schema.yml =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\models.py =====
================================================================================

import uuid

from django.conf import settings
from django.db import models


class UploadedDataset(models.Model):
    """Raw uploaded crash datasets plus their validation metadata.

    The ML / data-cleaning team can pick these up for deeper processing.
    """

    class Status(models.TextChoices):
        PENDING = "pending", "Pending"
        ACCEPTED = "accepted", "Accepted"
        REJECTED = "rejected", "Rejected"

    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    owner = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.CASCADE,
        related_name="uploaded_datasets",
    )
    original_filename = models.CharField(max_length=255)
    size_bytes = models.BigIntegerField()
    mime_type = models.CharField(max_length=255, blank=True)
    # Schema configuration used at validation time (e.g. "mmucc-alaska-v1").
    schema_version = models.CharField(max_length=64, blank=True)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    status = models.CharField(
        max_length=20,
        choices=Status.choices,
        default=Status.PENDING,
    )
    raw_file = models.FileField(upload_to="uploaded_datasets/")
    # Full structured status report returned from the ingestion endpoint.
    validation_report = models.JSONField(blank=True, null=True)

    class Meta:
        indexes = [
            models.Index(fields=["owner"]),
            models.Index(fields=["status"]),
            models.Index(fields=["created_at"]),
        ]

    def __str__(self) -> str:
        return f"{self.original_filename} ({self.id})"

================================================================================
===== END FILE: ingestion\models.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\tests\__init__.py =====
================================================================================

"""Test package for the ingestion app (FR1  upload gateway)."""

================================================================================
===== END FILE: ingestion\tests\__init__.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\tests\test_ingestion_gateway.py =====
================================================================================

from __future__ import annotations

from typing import Any, Dict

from django.contrib.auth import get_user_model
from django.core.files.uploadedfile import SimpleUploadedFile
from django.test import override_settings
from django.urls import reverse
from rest_framework import status
from rest_framework.test import APITestCase, APIClient
from unittest import mock

from ingestion.models import UploadedDataset
from ingestion import validation


User = get_user_model()


def make_csv_content(rows: list[list[Any]]) -> str:
    lines = []
    for row in rows:
        # join with commas, coercing to string
        line = ",".join("" if v is None else str(v) for v in row)
        lines.append(line)
    return "\n".join(lines)


class IngestionGatewayTests(APITestCase):
    def setUp(self) -> None:
        self.client = APIClient()
        self.user = User.objects.create_user(
            username="user",
            password="password",
        )
        self.other_user = User.objects.create_user(
            username="other",
            password="password",
        )
        self.admin = User.objects.create_user(
            username="admin",
            password="password",
            is_staff=True,
        )

        # Stub out MIME sniffing so tests are stable regardless of python-magic.
        sniff_patcher = mock.patch(
            "ingestion.views.validation.sniff_mime_type",
            side_effect=self._sniff_mime_type_passthrough,
        )
        self.mock_sniff = sniff_patcher.start()
        self.addCleanup(sniff_patcher.stop)

        # Stub out AV so tests do not depend on a running ClamAV daemon.
        av_patcher = mock.patch(
            "ingestion.views.scan_bytes_with_clamav",
            return_value={"status": "passed", "details": "AV scan stubbed for tests."},
        )
        self.mock_av = av_patcher.start()
        self.addCleanup(av_patcher.stop)

    def _sniff_mime_type_passthrough(
        self,
        file_bytes: bytes,
        original_name: str,
        declared_mime: str | None,
    ) -> Dict[str, Any]:
        # Default behaviour: always "passed" with the declared MIME (if any).
        return {
            "status": "passed",
            "detected_mime_type": declared_mime or "text/csv",
            "details": "sniff_mime_type stubbed for tests.",
        }

    def _upload_csv_as(self, user: User, filename: str, rows: list[list[Any]]) -> str:
        """Helper to upload a small CSV file as a given user and return upload_id."""
        self.client.force_authenticate(user=user)
        csv_content = make_csv_content(rows)
        upload = SimpleUploadedFile(
            filename,
            csv_content.encode("utf-8"),
            content_type="text/csv",
        )
        response = self.client.post(
            reverse("ingest-upload"),
            {"file": upload},
            format="multipart",
        )
        self.assertEqual(
            response.status_code,
            status.HTTP_201_CREATED,
            msg=f"Expected 201 for upload, got {response.status_code}: {response.content!r}",
        )
        data = response.json()
        self.assertEqual(data["overall_status"], "accepted")
        return data["upload_id"]

    def test_upload_valid_csv_happy_path(self) -> None:
        """Uploading a valid CSV produces an accepted upload with the full report."""
        self.client.force_authenticate(user=self.user)
        csv_rows = [
            ["crash_id", "crash_date", "severity", "latitude", "longitude"],
            [1, "2024-01-01", "K", 60.0, -150.0],
        ]
        csv_content = make_csv_content(csv_rows)
        upload = SimpleUploadedFile(
            "valid.csv",
            csv_content.encode("utf-8"),
            content_type="text/csv",
        )
        response = self.client.post(
            reverse("ingest-upload"),
            {"file": upload},
            format="multipart",
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        data = response.json()

        self.assertEqual(data["overall_status"], "accepted")
        self.assertIn("upload_id", data)
        self.assertEqual(data["schema_version"], validation.SCHEMA_VERSION)

        # Steps should include all expected pipeline stages.
        step_names = {s["step"] for s in data["steps"]}
        expected_steps = {
            "PAYLOAD",
            "EXTENSION_CHECK",
            "FILE_SIZE",
            "MIME_SNIFF",
            "AV_SCAN",
            "PARSE_TABLE",
            "SCHEMA_CHECK",
            "TYPE_AND_RANGE_CHECKS",
            "GEO_CHECKS",
        }
        self.assertEqual(step_names, expected_steps)

        # Each step should expose severity and is_hard_fail for the UI.
        for step in data["steps"]:
            self.assertIn("severity", step)
            self.assertIn(step["severity"], {"info", "warning", "error"})
            self.assertIn("is_hard_fail", step)

        # UploadedDataset row should be persisted with a matching report.
        upload_id = data["upload_id"]
        dataset = UploadedDataset.objects.get(id=upload_id)
        self.assertEqual(dataset.status, UploadedDataset.Status.ACCEPTED)
        self.assertIsNotNone(dataset.validation_report)
        self.assertEqual(dataset.validation_report["overall_status"], "accepted")

    def test_missing_file_hard_fail(self) -> None:
        """If no file is sent, the upload is rejected with PAYLOAD_MISSING_FILE."""
        self.client.force_authenticate(user=self.user)
        response = self.client.post(reverse("ingest-upload"), {}, format="multipart")
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
        data = response.json()
        self.assertEqual(data["overall_status"], "rejected")
        self.assertEqual(data["error_code"], "PAYLOAD_MISSING_FILE")

        step = next(s for s in data["steps"] if s["step"] == "PAYLOAD")
        self.assertEqual(step["status"], "failed")
        self.assertEqual(step["code"], "PAYLOAD_MISSING_FILE")
        self.assertEqual(step["severity"], "error")
        self.assertTrue(step["is_hard_fail"])

    def test_disallowed_extension_hard_fail(self) -> None:
        """A file with an extension not in INGESTION_ALLOWED_EXTENSIONS is rejected."""
        self.client.force_authenticate(user=self.user)
        bad_file = SimpleUploadedFile(
            "bad.txt",
            b"not,important\n",
            content_type="text/plain",
        )
        response = self.client.post(
            reverse("ingest-upload"),
            {"file": bad_file},
            format="multipart",
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
        data = response.json()
        self.assertEqual(data["overall_status"], "rejected")
        self.assertEqual(data["error_code"], "EXTENSION_NOT_ALLOWED")

        step = next(s for s in data["steps"] if s["step"] == "EXTENSION_CHECK")
        self.assertEqual(step["status"], "failed")
        self.assertEqual(step["code"], "EXTENSION_NOT_ALLOWED")
        self.assertEqual(step["severity"], "error")
        self.assertTrue(step["is_hard_fail"])

    @override_settings(INGESTION_MAX_FILE_SIZE_BYTES=10)
    def test_file_too_large_hard_fail(self) -> None:
        """Files larger than INGESTION_MAX_FILE_SIZE_BYTES are rejected."""
        self.client.force_authenticate(user=self.user)
        # Content longer than the 10-byte limit in the override_settings above.
        big_content = b"x" * 100
        upload = SimpleUploadedFile(
            "big.csv",
            big_content,
            content_type="text/csv",
        )
        response = self.client.post(
            reverse("ingest-upload"),
            {"file": upload},
            format="multipart",
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
        data = response.json()
        self.assertEqual(data["overall_status"], "rejected")
        self.assertEqual(data["error_code"], "FILE_TOO_LARGE")

        step = next(s for s in data["steps"] if s["step"] == "FILE_SIZE")
        self.assertEqual(step["status"], "failed")
        self.assertEqual(step["code"], "FILE_TOO_LARGE")
        self.assertEqual(step["severity"], "error")
        self.assertTrue(step["is_hard_fail"])

    def test_mime_mismatch_hard_fail(self) -> None:
        """A MIME sniff mismatch results in a MIME_MISMATCH error."""
        self.client.force_authenticate(user=self.user)

        def fake_sniff(file_bytes: bytes, original_name: str, declared_mime: str | None):
            return {
                "status": "failed",
                "detected_mime_type": "application/octet-stream",
                "details": "Declared type does not match detected MIME (test stub).",
            }

        self.mock_sniff.side_effect = fake_sniff

        csv_rows = [
            ["crash_id", "crash_date", "severity", "latitude", "longitude"],
            [1, "2024-01-01", "K", 60.0, -150.0],
        ]
        csv_content = make_csv_content(csv_rows)
        upload = SimpleUploadedFile(
            "mime.csv",
            csv_content.encode("utf-8"),
            content_type="text/csv",
        )
        response = self.client.post(
            reverse("ingest-upload"),
            {"file": upload},
            format="multipart",
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
        data = response.json()
        self.assertEqual(data["overall_status"], "rejected")
        self.assertEqual(data["error_code"], "MIME_MISMATCH")

        step = next(s for s in data["steps"] if s["step"] == "MIME_SNIFF")
        self.assertEqual(step["status"], "failed")
        self.assertEqual(step["code"], "MIME_MISMATCH")
        self.assertEqual(step["severity"], "error")
        self.assertTrue(step["is_hard_fail"])

    def test_av_detects_malware_hard_fail(self) -> None:
        """If AV marks the upload as infected, it is rejected with UPLOAD_INFECTED."""
        self.client.force_authenticate(user=self.user)
        self.mock_av.return_value = {
            "status": "failed",
            "details": "EICAR test signature detected (test stub).",
        }

        csv_rows = [
            ["crash_id", "crash_date", "severity", "latitude", "longitude"],
            [1, "2024-01-01", "K", 60.0, -150.0],
        ]
        csv_content = make_csv_content(csv_rows)
        upload = SimpleUploadedFile(
            "infected.csv",
            csv_content.encode("utf-8"),
            content_type="text/csv",
        )
        response = self.client.post(
            reverse("ingest-upload"),
            {"file": upload},
            format="multipart",
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
        data = response.json()
        self.assertEqual(data["overall_status"], "rejected")
        self.assertEqual(data["error_code"], "UPLOAD_INFECTED")

        step = next(s for s in data["steps"] if s["step"] == "AV_SCAN")
        self.assertEqual(step["status"], "failed")
        self.assertEqual(step["code"], "UPLOAD_INFECTED")
        self.assertEqual(step["severity"], "error")
        self.assertTrue(step["is_hard_fail"])

    @override_settings(INGESTION_REQUIRE_AV=True)
    def test_av_unavailable_when_required_hard_fail(self) -> None:
        """If AV is required but unavailable, uploads are rejected (AV_UNAVAILABLE_REQUIRED)."""
        self.client.force_authenticate(user=self.user)
        self.mock_av.return_value = {
            "status": "skipped",
            "details": "AV unavailable (test stub).",
        }

        csv_rows = [
            ["crash_id", "crash_date", "severity", "latitude", "longitude"],
            [1, "2024-01-01", "K", 60.0, -150.0],
        ]
        csv_content = make_csv_content(csv_rows)
        upload = SimpleUploadedFile(
            "noav.csv",
            csv_content.encode("utf-8"),
            content_type="text/csv",
        )
        response = self.client.post(
            reverse("ingest-upload"),
            {"file": upload},
            format="multipart",
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
        data = response.json()
        self.assertEqual(data["overall_status"], "rejected")
        self.assertEqual(data["error_code"], "AV_UNAVAILABLE_REQUIRED")

        step = next(s for s in data["steps"] if s["step"] == "AV_SCAN")
        self.assertEqual(step["status"], "skipped")
        self.assertEqual(step["code"], "AV_UNAVAILABLE_REQUIRED")
        self.assertEqual(step["severity"], "error")
        self.assertTrue(step["is_hard_fail"])

    def test_missing_required_schema_columns_hard_fail(self) -> None:
        """Missing required columns causes SCHEMA_MISSING_COLUMNS and a hard-fail."""
        self.client.force_authenticate(user=self.user)
        # A CSV with no MMUCC fields at all.
        csv_rows = [
            ["foo", "bar"],
            [1, 2],
        ]
        csv_content = make_csv_content(csv_rows)
        upload = SimpleUploadedFile(
            "schema.csv",
            csv_content.encode("utf-8"),
            content_type="text/csv",
        )
        response = self.client.post(
            reverse("ingest-upload"),
            {"file": upload},
            format="multipart",
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
        data = response.json()
        self.assertEqual(data["overall_status"], "rejected")
        self.assertEqual(data["error_code"], "SCHEMA_MISSING_COLUMNS")

        step = next(s for s in data["steps"] if s["step"] == "SCHEMA_CHECK")
        self.assertEqual(step["status"], "failed")
        self.assertEqual(step["code"], "SCHEMA_MISSING_COLUMNS")
        self.assertEqual(step["severity"], "error")
        self.assertTrue(step["is_hard_fail"])

    def test_soft_fail_type_and_geo_checks_still_accepted(self) -> None:
        """Type/range and geo issues increase invalid counts but do not reject upload."""
        self.client.force_authenticate(user=self.user)
        csv_rows = [
            ["crash_id", "crash_date", "severity", "latitude", "longitude", "driver_age"],
            [1, "2024-01-01", "K", 60.0, -150.0, 30],   # valid
            [2, "2024-01-02", "K", 80.0, -150.0, 200],  # bad lat + out-of-range age
        ]
        csv_content = make_csv_content(csv_rows)
        upload = SimpleUploadedFile(
            "softfail.csv",
            csv_content.encode("utf-8"),
            content_type="text/csv",
        )
        response = self.client.post(
            reverse("ingest-upload"),
            {"file": upload},
            format="multipart",
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        data = response.json()
        self.assertEqual(data["overall_status"], "accepted")

        row_checks = data["row_checks"]
        self.assertEqual(row_checks["total_rows"], 2)
        self.assertGreater(row_checks["invalid_row_count"], 0)
        self.assertGreater(row_checks["invalid_geo_row_count"], 0)

        type_step = next(s for s in data["steps"] if s["step"] == "TYPE_AND_RANGE_CHECKS")
        geo_step = next(s for s in data["steps"] if s["step"] == "GEO_CHECKS")

        self.assertEqual(type_step["status"], "passed")
        self.assertEqual(type_step["severity"], "warning")
        self.assertFalse(type_step["is_hard_fail"])

        self.assertEqual(geo_step["status"], "passed")
        self.assertEqual(geo_step["severity"], "warning")
        self.assertFalse(geo_step["is_hard_fail"])

    def test_list_uploads_respects_ownership_and_admin(self) -> None:
        """GET /api/ingest/uploads/ returns only own uploads for normal users and all for admins."""
        # Create one upload for the primary user and one for another user.
        user_upload_id = self._upload_csv_as(
            self.user,
            "mine.csv",
            [["crash_id", "crash_date", "severity", "latitude", "longitude"],
             [1, "2024-01-01", "K", 60.0, -150.0]],
        )
        other_upload_id = self._upload_csv_as(
            self.other_user,
            "theirs.csv",
            [["crash_id", "crash_date", "severity", "latitude", "longitude"],
             [2, "2024-01-02", "K", 60.0, -151.0]],
        )

        # As a normal user, we should only see our own upload.
        self.client.force_authenticate(user=self.user)
        response = self.client.get(reverse("ingest-upload-list"))
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        data = response.json()
        self.assertEqual(data["count"], 1)
        ids = {item["id"] for item in data["results"]}
        self.assertEqual(ids, {str(user_upload_id)})

        # As an admin, we should see both uploads.
        self.client.force_authenticate(user=self.admin)
        response = self.client.get(reverse("ingest-upload-list"))
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        data = response.json()
        self.assertEqual(data["count"], 2)
        ids = {item["id"] for item in data["results"]}
        self.assertEqual(ids, {str(user_upload_id), str(other_upload_id)})

    def test_get_upload_status_permissions(self) -> None:
        """Only owners (and admins) can read a specific upload's status."""
        upload_id = self._upload_csv_as(
            self.user,
            "mine.csv",
            [["crash_id", "crash_date", "severity", "latitude", "longitude"],
             [1, "2024-01-01", "K", 60.0, -150.0]],
        )

        # Owner can fetch status.
        self.client.force_authenticate(user=self.user)
        url = reverse("ingest-upload-status", args=[upload_id])
        response = self.client.get(url)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        data = response.json()
        self.assertEqual(str(data["upload_id"]), str(upload_id))

        # Non-owner cannot.
        self.client.force_authenticate(user=self.other_user)
        response = self.client.get(url)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

        # Admin can.
        self.client.force_authenticate(user=self.admin)
        response = self.client.get(url)
        self.assertEqual(response.status_code, status.HTTP_200_OK)

================================================================================
===== END FILE: ingestion\tests\test_ingestion_gateway.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\urls.py =====
================================================================================

from django.urls import path

from . import views

urlpatterns = [
    path("upload/", views.upload_dataset, name="ingest-upload"),
    path("uploads/", views.list_uploads, name="ingest-upload-list"),
    path(
        "uploads/<uuid:upload_id>/",
        views.get_upload_status,
        name="ingest-upload-status",
    ),
    path(
        "uploads/<uuid:upload_id>/export/validation.csv",
        views.export_validation_csv,
        name="ingest-validation-export",
    ),
]

================================================================================
===== END FILE: ingestion\urls.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\validation.py =====
================================================================================

import io
import mimetypes
from pathlib import Path
from typing import Any, Dict, List, Tuple

import pandas as pd
import yaml
from django.conf import settings

try:
    import magic  # type: ignore[import]
except Exception:  # noqa: BLE001
    magic = None  # type: ignore[assignment]

# -----------------------------------------------------------------------
# MMUCC-aligned schema configuration (externalized for non-devs)
# -----------------------------------------------------------------------


def _get_schema_config_path() -> Path:
    configured = getattr(settings, "INGESTION_SCHEMA_CONFIG_PATH", None)
    if configured:
        return Path(configured)
    base_dir = getattr(settings, "BASE_DIR", Path(__file__).resolve().parent.parent)
    return Path(base_dir) / "ingestion" / "config" / "mmucc_schema.yml"


def _load_schema_config() -> Dict[str, Any]:
    config_path = _get_schema_config_path()
    try:
        with config_path.open("r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
    except FileNotFoundError:
        data = {}
    return data


_SCHEMA_CONFIG: Dict[str, Any] = _load_schema_config()
SCHEMA_VERSION: str = _SCHEMA_CONFIG.get("schema_version", "unknown")
COLUMN_SPECS: Dict[str, Dict[str, Any]] = _SCHEMA_CONFIG.get("columns", {})

REQUIRED_COLUMNS = sorted(
    name for name, spec in COLUMN_SPECS.items() if spec.get("required")
)
KNOWN_COLUMNS = set(COLUMN_SPECS.keys())

_GEO = _SCHEMA_CONFIG.get("geo_bounds", {})
ALASKA_LAT_RANGE: Tuple[float, float] = (
    float(_GEO.get("latitude", {}).get("min", 51.0)),
    float(_GEO.get("latitude", {}).get("max", 72.0)),
)
ALASKA_LON_RANGE: Tuple[float, float] = (
    float(_GEO.get("longitude", {}).get("min", -170.0)),
    float(_GEO.get("longitude", {}).get("max", -129.0)),
)


def load_dataframe_from_bytes(file_bytes: bytes, extension: str) -> pd.DataFrame:
    """Load the uploaded file into a pandas DataFrame based on its extension."""
    ext = extension.lower()
    if ext == ".csv":
        text = file_bytes.decode("utf-8", errors="replace")
        return pd.read_csv(io.StringIO(text))
    if ext == ".parquet":
        # Requires pyarrow or fastparquet to be installed. pyarrow is
        # declared in requirements.txt to make this work out-of-the-box.
        return pd.read_parquet(io.BytesIO(file_bytes))

    raise ValueError(f"Unsupported file type for dataframe load: {extension!r}")


def sniff_mime_type(
    file_bytes: bytes, original_name: str, declared_mime: str | None
) -> Dict[str, Any]:
    """Use python-magic when available, otherwise fall back to filename-based detection.

    Returns a dict with:
    - status: "passed" or "failed"
    - detected_mime_type: str
    - details: human-friendly description
    """
    detected: str | None = None
    details: List[str] = []

    if magic is not None:
        try:
            detected = magic.from_buffer(file_bytes, mime=True)
            details.append(f"python-magic detected MIME type {detected!r}.")
        except Exception as exc:  # noqa: BLE001
            details.append(f"python-magic failed to sniff MIME type: {exc!r}.")

    if not detected:
        guessed, _ = mimetypes.guess_type(original_name)
        detected = guessed or declared_mime or "application/octet-stream"
        details.append(
            "Falling back to filename-based MIME detection and/or declared Content-Type."
        )

    status = "passed"
    if declared_mime and detected and detected != declared_mime:
        status = "failed"
        details.append(
            f"Declared MIME type {declared_mime!r} does not match detected {detected!r}."
        )
    else:
        details.append(
            f"Declared MIME type {declared_mime!r} is consistent with detected {detected!r}."
        )

    return {
        "status": status,
        "detected_mime_type": detected,
        "details": " ".join(details),
    }


def validate_schema(df: pd.DataFrame) -> Dict[str, Any]:
    """Compare the uploaded columns against the configured MMUCC-aligned schema."""
    columns = list(df.columns)
    actual = set(columns)
    missing = sorted(list(set(REQUIRED_COLUMNS) - actual))
    unknown = sorted(list(actual - KNOWN_COLUMNS))

    is_valid = len(missing) == 0
    if is_valid:
        details = "All required MMUCC-aligned columns are present."
    else:
        details = (
            "Dataset is missing required MMUCC-aligned columns: "
            + ", ".join(missing)
        )

    return {
        "schema_version": SCHEMA_VERSION,
        "is_valid": is_valid,
        "missing_columns": missing,
        "unknown_columns": unknown,
        "columns": columns,
        "details": details,
    }


def validate_value_types_and_ranges(df: pd.DataFrame) -> Dict[str, Any]:
    """Basic type & range checks for configured columns.

    Rows with violations are *flagged* (counted) but not dropped. The
    dataset can still be accepted for downstream cleaning.
    """
    total_rows = int(len(df))
    invalid_row_indexes: set[int] = set()
    column_summaries: Dict[str, Dict[str, Any]] = {}

    for name, spec in COLUMN_SPECS.items():
        if name not in df.columns:
            continue

        series = df[name]
        summary: Dict[str, Any] = {
            "non_null_values": int(series.notna().sum()),
            "missing_values": int(series.isna().sum()),
            "invalid_type_count": 0,
            "invalid_range_count": 0,
        }

        col_type = spec.get("type")
        if col_type in {"int", "float"}:
            numeric = pd.to_numeric(series, errors="coerce")
            invalid_type_mask = series.notna() & numeric.isna()
            summary["invalid_type_count"] = int(invalid_type_mask.sum())
            invalid_row_indexes.update(series[invalid_type_mask].index.tolist())

            lower = spec.get("min")
            upper = spec.get("max")
            if lower is not None or upper is not None:
                range_mask = pd.Series(False, index=series.index)
                if lower is not None:
                    range_mask |= numeric < lower
                if upper is not None:
                    range_mask |= numeric > upper

                range_mask &= numeric.notna()
                summary["invalid_range_count"] = int(range_mask.sum())
                invalid_row_indexes.update(series[range_mask].index.tolist())

        elif col_type == "date":
            parsed = pd.to_datetime(series, errors="coerce", utc=True)
            invalid_type_mask = series.notna() & parsed.isna()
            summary["invalid_type_count"] = int(invalid_type_mask.sum())
            invalid_row_indexes.update(series[invalid_type_mask].index.tolist())

        # "category" and "string" are not constrained beyond presence here.

        column_summaries[name] = summary

    invalid_row_count = len(invalid_row_indexes)
    if invalid_row_count == 0:
        details = "All configured columns passed type and range checks."
    else:
        details = (
            f"{invalid_row_count} rows have at least one type or range issue, "
            "but the dataset was accepted for downstream cleaning."
        )

    return {
        "total_rows": total_rows,
        "invalid_row_count": invalid_row_count,
        "columns": column_summaries,
        "details": details,
    }


def _resolve_column_name(df: pd.DataFrame, candidates: List[str]) -> str | None:
    for name in candidates:
        if name in df.columns:
            return name
    return None


def validate_geo_bounds(df: pd.DataFrame) -> Dict[str, Any]:
    """Ensure that coordinates (if present) fall within the Alaska bounding box.

    Rows that fall outside the bounding box are counted but do not cause rejection.
    """
    lat_col = _resolve_column_name(
        df, ["latitude", "Latitude", "LATITUDE", "lat", "Lat", "LAT"]
    )
    lon_col = _resolve_column_name(
        df, ["longitude", "Longitude", "LONGITUDE", "lon", "Lon", "LON"]
    )

    if lat_col is None or lon_col is None:
        return {
            "has_coordinates": False,
            "invalid_row_count": 0,
            "details": (
                "Dataset does not contain both latitude and longitude columns; "
                "geo bounds checks were skipped."
            ),
        }

    lat = pd.to_numeric(df[lat_col], errors="coerce")
    lon = pd.to_numeric(df[lon_col], errors="coerce")

    in_lat_range = lat.between(ALASKA_LAT_RANGE[0], ALASKA_LAT_RANGE[1])
    in_lon_range = lon.between(ALASKA_LON_RANGE[0], ALASKA_LON_RANGE[1])

    has_coords = lat.notna() & lon.notna()
    in_bounds = has_coords & in_lat_range & in_lon_range
    out_of_bounds_mask = has_coords & ~in_bounds

    invalid_row_count = int(out_of_bounds_mask.sum())

    if invalid_row_count == 0:
        details = "All rows with coordinates fall within the Alaska bounding box."
    else:
        details = (
            f"{invalid_row_count} rows fall outside the Alaska bounding box; "
            "they will be flagged as geo-invalid for downstream processing."
        )

    return {
        "has_coordinates": True,
        "invalid_row_count": invalid_row_count,
        "details": details,
        "lat_column": lat_col,
        "lon_column": lon_col,
        "lat_range": ALASKA_LAT_RANGE,
        "lon_range": ALASKA_LON_RANGE,
    }

================================================================================
===== END FILE: ingestion\validation.py =====
================================================================================

================================================================================
===== BEGIN FILE: ingestion\views.py =====
================================================================================

import csv
import io
import logging
import os
from typing import Any, Dict, List

from django.conf import settings
from django.core.files.base import ContentFile
from django.http import HttpResponse, JsonResponse
from django.shortcuts import get_object_or_404
from rest_framework import status
from rest_framework.decorators import api_view, parser_classes, permission_classes
from rest_framework.parsers import MultiPartParser
from rest_framework.permissions import IsAuthenticated

from .antivirus import scan_bytes_with_clamav
from .models import UploadedDataset
from . import validation

logger = logging.getLogger(__name__)

DANGEROUS_CSV_PREFIXES = ("=", "+", "-", "@")


def _safe_csv_value(value: Any) -> str:
    """Defend against CSV formula injection by prefixing dangerous cells."""
    if value is None:
        return ""
    if not isinstance(value, str):
        value = str(value)
    if value.startswith(DANGEROUS_CSV_PREFIXES):
        return "'" + value
    return value


def _step(
    step: str,
    step_status: str,
    details: str,
    *,
    severity: str | None = None,
    meta: Dict[str, Any] | None = None,
    code: str | None = None,
    is_hard_fail: bool | None = None,
) -> Dict[str, Any]:
    """Build a single validation step entry for the upload report.

    - status: "passed" | "failed" | "skipped"
    - severity: "error" | "warning" | "info"
      * "error" + is_hard_fail=True => upload will be rejected if this step fails
      * "warning" => non-fatal data quality issues (upload still accepted)
      * "info" => purely informational/housekeeping steps
    """
    item: Dict[str, Any] = {
        "step": step,
        "status": step_status,
        "details": details,
    }
    if severity is not None:
        item["severity"] = severity
    if is_hard_fail is not None:
        item["is_hard_fail"] = is_hard_fail
    if code is not None:
        item["code"] = code
    if meta is not None:
        item["meta"] = meta
    return item


def _user_is_admin(user) -> bool:
    return bool(
        getattr(user, "is_superuser", False)
        or getattr(user, "is_staff", False)
        or user.groups.filter(name="Admin").exists()
    )


@api_view(["POST"])
@parser_classes([MultiPartParser])
@permission_classes([IsAuthenticated])
def upload_dataset(request):
    """Secure upload gateway endpoint: POST /api/ingest/upload/

    This implements the ingestion pipeline described in the design doc:
    - file extension & size checks
    - MIME sniffing
    - antivirus scan (ClamAV)
    - schema validation
    - basic type/range checks
    - geo bounds checks

    If the upload is accepted, an UploadedDataset row is created with the
    raw file and the full structured validation report.
    """
    user = request.user
    steps: List[Dict[str, Any]] = []

    upload = request.FILES.get("file")
    if upload is None:
        error_code = "PAYLOAD_MISSING_FILE"
        steps.append(
            _step(
                "PAYLOAD",
                "failed",
                "Expected multipart/form-data with a single file field named 'file'.",
                severity="error",
                is_hard_fail=True,
                code=error_code,
            )
        )
        logger.info(
            "Upload rejected (no file provided)",
            extra={
                "event": "upload_rejected",
                "error_code": error_code,
                "user_id": getattr(user, "id", None),
                "username": getattr(user, "username", None),
            },
        )
        return JsonResponse(
            {
                "overall_status": "rejected",
                "error_code": error_code,
                "message": "No file was provided in the 'file' field.",
                "steps": steps,
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    # Payload successfully received
    steps.append(
        _step(
            "PAYLOAD",
            "passed",
            "Received multipart/form-data with a single file field named 'file'.",
            severity="info",
            is_hard_fail=False,
        )
    )

    original_name = upload.name or "uploaded_dataset"
    ext = os.path.splitext(original_name)[1].lower()
    allowed_exts = {
        e.lower().strip()
        for e in getattr(
            settings,
            "INGESTION_ALLOWED_EXTENSIONS",
            [".csv"],
        )
        if e
    }

    if ext not in allowed_exts:
        error_code = "EXTENSION_NOT_ALLOWED"
        steps.append(
            _step(
                "EXTENSION_CHECK",
                "failed",
                (
                    f"Unsupported file extension '{ext}'. "
                    f"Allowed extensions: {', '.join(sorted(allowed_exts))}."
                ),
                severity="error",
                is_hard_fail=True,
                code=error_code,
            )
        )
        logger.info(
            "Upload rejected (extension)",
            extra={
                "event": "upload_rejected",
                "error_code": error_code,
                "user_id": getattr(user, "id", None),
                "username": getattr(user, "username", None),
                "filename": original_name,
                "extension": ext,
            },
        )
        return JsonResponse(
            {
                "overall_status": "rejected",
                "error_code": error_code,
                "message": "File extension is not allowed.",
                "steps": steps,
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    steps.append(
        _step(
            "EXTENSION_CHECK",
            "passed",
            f"Extension '{ext}' is allowed.",
            severity="info",
            is_hard_fail=False,
        )
    )

    # Read file into memory once so we can reuse it for AV + validation.
    try:
        file_bytes = upload.read()
    except Exception as exc:  # noqa: BLE001
        error_code = "READ_FAILED"
        steps.append(
            _step(
                "READ_FILE",
                "failed",
                f"Failed to read uploaded file: {exc}",
                severity="error",
                is_hard_fail=True,
                code=error_code,
            )
        )
        logger.warning(
            "Upload rejected (read failure)",
            extra={
                "event": "upload_rejected",
                "error_code": error_code,
                "user_id": getattr(user, "id", None),
                "username": getattr(user, "username", None),
                "filename": original_name,
            },
        )
        return JsonResponse(
            {
                "overall_status": "rejected",
                "error_code": error_code,
                "message": "The uploaded file could not be read.",
                "steps": steps,
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    size_bytes = len(file_bytes)
    max_size = getattr(settings, "INGESTION_MAX_FILE_SIZE_BYTES", 10 * 1024 * 1024)
    if size_bytes > max_size:
        error_code = "FILE_TOO_LARGE"
        steps.append(
            _step(
                "FILE_SIZE",
                "failed",
                (
                    f"File is too large: {size_bytes} bytes. "
                    f"The configured maximum is {max_size} bytes."
                ),
                meta={"size_bytes": size_bytes, "max_size_bytes": max_size},
                severity="error",
                is_hard_fail=True,
                code=error_code,
            )
        )
        logger.info(
            "Upload rejected (size)",
            extra={
                "event": "upload_rejected",
                "error_code": error_code,
                "user_id": getattr(user, "id", None),
                "username": getattr(user, "username", None),
                "filename": original_name,
                "size_bytes": size_bytes,
                "max_size_bytes": max_size,
            },
        )
        return JsonResponse(
            {
                "overall_status": "rejected",
                "error_code": error_code,
                "message": "The uploaded file exceeds the configured size limit.",
                "steps": steps,
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    steps.append(
        _step(
            "FILE_SIZE",
            "passed",
            f"File size {size_bytes} bytes is within the configured limit.",
            meta={"size_bytes": size_bytes, "max_size_bytes": max_size},
            severity="info",
            is_hard_fail=False,
        )
    )

    # MIME sniffing
    sniff_result = validation.sniff_mime_type(
        file_bytes=file_bytes,
        original_name=original_name,
        declared_mime=getattr(upload, "content_type", None),
    )
    sniff_status = sniff_result["status"]
    mime_step_code = "MIME_MISMATCH" if sniff_status == "failed" else None
    mime_severity = "error" if sniff_status == "failed" else "info"
    mime_is_hard_fail = sniff_status == "failed"
    steps.append(
        _step(
            "MIME_SNIFF",
            sniff_status,
            sniff_result["details"],
            meta={
                "detected_mime_type": sniff_result["detected_mime_type"],
                "declared_mime_type": getattr(upload, "content_type", None),
            },
            severity=mime_severity,
            is_hard_fail=mime_is_hard_fail,
            code=mime_step_code,
        )
    )
    if sniff_status == "failed":
        error_code = "MIME_MISMATCH"
        logger.info(
            "Upload rejected (MIME mismatch)",
            extra={
                "event": "upload_rejected",
                "error_code": error_code,
                "user_id": getattr(user, "id", None),
                "username": getattr(user, "username", None),
                "filename": original_name,
                "detected_mime_type": sniff_result["detected_mime_type"],
                "declared_mime_type": getattr(upload, "content_type", None),
            },
        )
        return JsonResponse(
            {
                "overall_status": "rejected",
                "error_code": error_code,
                "message": "Declared Content-Type does not match the detected MIME type.",
                "steps": steps,
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    # Antivirus scanning
    require_av = getattr(settings, "INGESTION_REQUIRE_AV", False)
    av_result = scan_bytes_with_clamav(file_bytes)
    av_status = av_result["status"]

    if av_status == "failed":
        av_step_code = "UPLOAD_INFECTED"
        av_severity = "error"
        av_is_hard_fail = True
    elif av_status == "skipped" and require_av:
        av_step_code = "AV_UNAVAILABLE_REQUIRED"
        av_severity = "error"
        av_is_hard_fail = True
    elif av_status == "skipped":
        av_step_code = None
        av_severity = "warning"
        av_is_hard_fail = False
    else:
        av_step_code = None
        av_severity = "info"
        av_is_hard_fail = False

    steps.append(
        _step(
            "AV_SCAN",
            av_status,
            av_result["details"],
            severity=av_severity,
            is_hard_fail=av_is_hard_fail,
            code=av_step_code,
        )
    )

    if av_status == "failed" or (require_av and av_status == "skipped"):
        # Malware discovered or AV required but unavailable; hard reject.
        if av_status == "failed":
            error_code = "UPLOAD_INFECTED"
            message = (
                "The uploaded file contains malware and has been rejected."
            )
        else:
            error_code = "AV_UNAVAILABLE_REQUIRED"
            message = (
                "Antivirus scanning is required but could not be completed; "
                "the upload was rejected."
            )

        logger.warning(
            "Upload rejected (antivirus)",
            extra={
                "event": "upload_rejected",
                "error_code": error_code,
                "user_id": getattr(user, "id", None),
                "username": getattr(user, "username", None),
                "filename": original_name,
                "size_bytes": size_bytes,
                "av_status": av_status,
            },
        )
        return JsonResponse(
            {
                "overall_status": "rejected",
                "error_code": error_code,
                "message": message,
                "steps": steps,
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    # Load into pandas
    try:
        df = validation.load_dataframe_from_bytes(
            file_bytes=file_bytes,
            extension=ext,
        )
    except Exception as exc:  # noqa: BLE001
        error_code = "PARSE_FAILED"
        steps.append(
            _step(
                "PARSE_TABLE",
                "failed",
                f"Could not parse uploaded file into a tabular dataset: {exc}",
                severity="error",
                is_hard_fail=True,
                code=error_code,
            )
        )
        logger.warning(
            "Upload rejected (parse failure)",
            extra={
                "event": "upload_rejected",
                "error_code": error_code,
                "user_id": getattr(user, "id", None),
                "username": getattr(user, "username", None),
                "filename": original_name,
            },
        )
        return JsonResponse(
            {
                "overall_status": "rejected",
                "error_code": error_code,
                "message": "The uploaded file could not be parsed as a table.",
                "steps": steps,
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    steps.append(
        _step(
            "PARSE_TABLE",
            "passed",
            f"Parsed dataset with {len(df)} rows and {len(df.columns)} columns.",
            severity="info",
            is_hard_fail=False,
        )
    )

    # Schema validation
    schema_result = validation.validate_schema(df)
    schema_status = "passed" if schema_result["is_valid"] else "failed"
    schema_version = schema_result.get("schema_version", validation.SCHEMA_VERSION)
    schema_step_code = "SCHEMA_MISSING_COLUMNS" if schema_status == "failed" else None
    schema_severity = "error" if schema_status == "failed" else "info"
    schema_is_hard_fail = schema_status == "failed"

    steps.append(
        _step(
            "SCHEMA_CHECK",
            schema_status,
            schema_result["details"],
            meta={
                "missing_columns": schema_result["missing_columns"],
                "unknown_columns": schema_result["unknown_columns"],
                "columns": schema_result["columns"],
                "schema_version": schema_version,
            },
            severity=schema_severity,
            is_hard_fail=schema_is_hard_fail,
            code=schema_step_code,
        )
    )
    if schema_status == "failed":
        error_code = "SCHEMA_MISSING_COLUMNS"
        logger.info(
            "Upload rejected (schema)",
            extra={
                "event": "upload_rejected",
                "error_code": error_code,
                "user_id": getattr(user, "id", None),
                "username": getattr(user, "username", None),
                "filename": original_name,
                "missing_columns": schema_result["missing_columns"],
            },
        )
        return JsonResponse(
            {
                "overall_status": "rejected",
                "error_code": error_code,
                "message": "The uploaded file is missing required columns.",
                "steps": steps,
                "schema": {
                    "missing_columns": schema_result["missing_columns"],
                    "unknown_columns": schema_result["unknown_columns"],
                    "columns": schema_result["columns"],
                    "schema_version": schema_version,
                },
            },
            status=status.HTTP_400_BAD_REQUEST,
        )

    # Type & range checks
    value_checks = validation.validate_value_types_and_ranges(df)
    value_severity = "warning" if value_checks["invalid_row_count"] > 0 else "info"
    steps.append(
        _step(
            "TYPE_AND_RANGE_CHECKS",
            "passed",
            value_checks["details"],
            meta=value_checks,
            severity=value_severity,
            is_hard_fail=False,
        )
    )

    # Geo bounds checks
    geo_checks = validation.validate_geo_bounds(df)
    geo_invalid = int(geo_checks.get("invalid_row_count", 0) or 0)
    if not geo_checks.get("has_coordinates"):
        geo_severity = "info"
    else:
        geo_severity = "warning" if geo_invalid > 0 else "info"

    steps.append(
        _step(
            "GEO_CHECKS",
            "passed",
            geo_checks["details"],
            meta=geo_checks,
            severity=geo_severity,
            is_hard_fail=False,
        )
    )

    # At this point the upload is accepted, even if some rows are flagged.
    overall_status = "accepted"

    detected_mime = sniff_result["detected_mime_type"]
    content = ContentFile(file_bytes)

    dataset = UploadedDataset(
        owner=user,
        original_filename=original_name,
        size_bytes=size_bytes,
        mime_type=detected_mime,
        schema_version=schema_version,
        status=UploadedDataset.Status.ACCEPTED,
        validation_report={
            "overall_status": overall_status,
            "schema_version": schema_version,
            "steps": steps,
            "schema": schema_result,
            "value_checks": value_checks,
            "geo_checks": geo_checks,
        },
    )
    dataset.raw_file.save(original_name, content, save=True)
    upload_id = str(dataset.id)

    logger.info(
        "Upload accepted",
        extra={
            "event": "upload_accepted",
            "user_id": getattr(user, "id", None),
            "username": getattr(user, "username", None),
            "upload_id": upload_id,
            "filename": original_name,
            "size_bytes": size_bytes,
            "mime_type": detected_mime,
            "schema_version": schema_version,
        },
    )

    response_payload: Dict[str, Any] = {
        "upload_id": upload_id,
        "overall_status": overall_status,
        "schema_version": schema_version,
        "steps": steps,
        "schema": {
            "missing_columns": schema_result["missing_columns"],
            "unknown_columns": schema_result["unknown_columns"],
            "columns": schema_result["columns"],
            "schema_version": schema_version,
        },
        "row_checks": {
            "total_rows": value_checks["total_rows"],
            "invalid_row_count": value_checks["invalid_row_count"],
            "invalid_geo_row_count": geo_checks.get("invalid_row_count", 0),
        },
    }

    return JsonResponse(response_payload, status=status.HTTP_201_CREATED)


# Mark the view with a DRF throttle scope for configuration in settings.REST_FRAMEWORK.
upload_dataset.throttle_scope = "ingest_upload"  # type: ignore[attr-defined]


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def list_uploads(request):
    """Return a list of uploads visible to the current user.

    Non-admin users see only their own uploads; admins see everything.
    """
    user = request.user
    is_admin = _user_is_admin(user)

    qs = UploadedDataset.objects.all()
    if not is_admin:
        qs = qs.filter(owner=user)
    qs = qs.order_by("-created_at")

    results: List[Dict[str, Any]] = []
    for dataset in qs:
        report = dataset.validation_report or {}
        # Support both old and new report shapes
        row_checks = report.get("row_checks") or report.get("value_checks") or {}
        geo_checks = report.get("geo_checks") or {}

        total_rows = row_checks.get("total_rows")
        invalid_row_count = row_checks.get("invalid_row_count")
        invalid_geo_row_count = row_checks.get(
            "invalid_geo_row_count", geo_checks.get("invalid_row_count")
        )

        results.append(
            {
                "id": str(dataset.id),
                "original_filename": dataset.original_filename,
                "created_at": dataset.created_at.isoformat(),
                "status": dataset.status,
                "schema_version": dataset.schema_version,
                "mime_type": dataset.mime_type,
                "total_rows": total_rows,
                "invalid_row_count": invalid_row_count,
                "invalid_geo_row_count": invalid_geo_row_count,
            }
        )

    return JsonResponse(
        {
            "count": len(results),
            "results": results,
            "next": None,
            "previous": None,
        }
    )


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def get_upload_status(request, upload_id):
    """Retrieve the persisted validation report and metadata for a given upload.

    This lets the UI re-display validation results without requiring a re-upload.
    """
    dataset = get_object_or_404(UploadedDataset, id=upload_id)
    if dataset.owner_id != request.user.id and not _user_is_admin(request.user):
        return JsonResponse(
            {"detail": "You do not have permission to view this upload."},
            status=status.HTTP_403_FORBIDDEN,
        )

    report = dataset.validation_report or {}

    payload: Dict[str, Any] = {
        "upload_id": str(dataset.id),
        "original_filename": dataset.original_filename,
        "size_bytes": dataset.size_bytes,
        "mime_type": dataset.mime_type,
        "schema_version": dataset.schema_version,
        "status": dataset.status,
        "created_at": dataset.created_at.isoformat(),
        "validation_report": report,
    }
    return JsonResponse(payload)


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def export_validation_csv(request, upload_id):
    """Export a one-row-per-step CSV summary of the validation report."""
    dataset = get_object_or_404(UploadedDataset, id=upload_id)
    if dataset.owner_id != request.user.id and not _user_is_admin(request.user):
        return JsonResponse(
            {"detail": "You do not have permission to export this upload."},
            status=status.HTTP_403_FORBIDDEN,
        )

    report = dataset.validation_report or {}
    steps = report.get("steps", [])
    row_checks = report.get("value_checks", {})
    geo_checks = report.get("geo_checks", {})

    buffer = io.StringIO()
    writer = csv.writer(buffer)

    writer.writerow(
        [
            "step",
            "status",
            "code",
            "details",
            "schema_version",
            "meta_json",
        ]
    )

    schema_version = report.get("schema_version") or dataset.schema_version or ""

    for step in steps:
        writer.writerow(
            [
                _safe_csv_value(step.get("step")),
                _safe_csv_value(step.get("status")),
                _safe_csv_value(step.get("code", "")),
                _safe_csv_value(step.get("details", "")),
                _safe_csv_value(schema_version),
                _safe_csv_value(step.get("meta")),
            ]
        )

    # Append a couple of synthetic rows to capture row/geo summaries.
    writer.writerow([])
    writer.writerow(["summary", "row_checks", "", "", "", ""])
    writer.writerow(
        [
            "total_rows",
            row_checks.get("total_rows", ""),
            "",
            "",
            "",
            "",
        ]
    )
    writer.writerow(
        [
            "invalid_row_count",
            row_checks.get("invalid_row_count", ""),
            "",
            "",
            "",
            "",
        ]
    )
    writer.writerow(
        [
            "invalid_geo_row_count",
            geo_checks.get("invalid_row_count", ""),
            "",
            "",
            "",
            "",
        ]
    )

    csv_bytes = buffer.getvalue().encode("utf-8")
    response = HttpResponse(csv_bytes, content_type="text/csv")
    response["Content-Disposition"] = (
        f"attachment; filename=validation_{dataset.id}.csv"
    )
    return response


# Throttle scope for exports
export_validation_csv.throttle_scope = "exports"  # type: ignore[attr-defined]

================================================================================
===== END FILE: ingestion\views.py =====
================================================================================

================================================================================
===== BEGIN FILE: manage.py =====
================================================================================

#!/usr/bin/env python
import os
import sys


def main() -> None:
    os.environ.setdefault("DJANGO_SETTINGS_MODULE", "alaska_project.settings")
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable?"
        ) from exc
    execute_from_command_line(sys.argv)


if __name__ == "__main__":
    main()

================================================================================
===== END FILE: manage.py =====
================================================================================

================================================================================
===== BEGIN FILE: README.md =====
================================================================================

# Alaska Car Crash Analysis

This repository contains a Django + React application for exploring and summarizing
police-reported crash data. The backend follows an **App Server + Upload Gateway**
pattern with clear separation of responsibilities and a security-first upload flow.

## Backend (Django)

The Django project is split into three main apps:

- **`ingestion`**  upload gateway / validation engine
  - `POST /api/ingest/upload/`
    - Accepts authenticated uploads of crash datasets (CSV and Parquet by
      default; additional formats can be enabled via config).
    - Enforces file size limits (`INGESTION_MAX_FILE_SIZE_BYTES`).
    - Restricts extensions to a configurable whitelist
      (`INGESTION_ALLOWED_EXTENSIONS`).
    - Performs MIME sniffing (via `python-magic` when available).
    - Runs an antivirus scan via ClamAV (when configured).
    - Validates the header against a MMUCC-aligned schema loaded from
      `ingestion/config/mmucc_schema.yml` (including KABCO severity and core
      location fields). See `docs/schema_config.md` for non-developer editing
      guidance.
    - Runs basic type/range checks and Alaska-specific geo bounding-box checks.
    - Produces a structured per-step status report plus row-level summary stats.
    - Returns a `schema_version` so the UI can display which schema was applied.
  - `GET /api/ingest/uploads/`
    - Returns uploads visible to the current user (uploads owned by the current user; admins see all).
    - Each entry includes `id`, `original_filename`, `created_at`, `status`, `schema_version`, `mime_type`,
      and row summary counts (`total_rows`, `invalid_row_count`, `invalid_geo_row_count`).
  - `GET /api/ingest/uploads/<upload_id>/`
    - Returns the persisted validation report and metadata for a given upload.
    - Useful for re-displaying validation status in the UI without re-uploading.
  - `GET /api/ingest/uploads/<upload_id>/export/validation.csv`
    - CSV export of the validation steps + row summaries, with formula-injection
      defenses applied.
  - Persists each successful upload in `ingestion.UploadedDataset` with:
    - `owner` (uploading user)
    - `original_filename`, `size_bytes`, `mime_type`
    - `schema_version` (from the MMUCC config file)
    - `raw_file` (stored under `uploaded_datasets/`)
    - `status` (`pending | accepted | rejected`)
    - `validation_report` (JSON copy of the status report returned
      to the client)
  - All hard fail stages in the pipeline now return machine-readable
    `error_code`s (e.g. `EXTENSION_NOT_ALLOWED`, `FILE_TOO_LARGE`,
    `MIME_MISMATCH`, `UPLOAD_INFECTED`, `AV_UNAVAILABLE_REQUIRED`,
    `SCHEMA_MISSING_COLUMNS`). Error codes are documented in
    `docs/ingestion_errors.md`.

- **`crashdata`**  domain models & query helpers
  - Owns the `CrashRecord` model, which captures core MMUCC/KABCO fields and
    a PostGIS `PointField` for the crash location with a **GiST index** for
    fast spatial queries.
  - Includes helpers in `crashdata.queries` for:
    - severity histograms (`severity_histogram`)
    - spatial queries (`crashes_within_bbox`), with support for filters such as
      severity, municipality and date range.
  - Exposes read-only APIs for visualization and exports:
    - `GET /api/crashdata/severity-histogram/`
    - `GET /api/crashdata/within-bbox/`
    - `GET /api/crashdata/export/`
  - Most endpoints return a `count` and a list of results, suitable for
    paginated tables and charts.

- **`models`**  integration surface for ML / statistical models
  - Provides a thin wrapper around long-running model jobs driven by
    uploaded crash datasets.
  - Exposes:
    - `POST /api/models/run/`
    - `GET /api/models/results/<job_id>/`
  - Uses a `ModelJob` Django model to track jobs:
    - `id` (UUID), `upload`, `owner`, `model_name`, `status`
    - `parameters` (JSON), `result_metadata` (JSON)
    - timestamps (`created_at`, `updated_at`)
  - `POST /api/models/run/` validates the request body:
    - required: `upload_id`, `model`
    - optional: `parameters` object
    - `model` must be one of the enum values in `SUPPORTED_MODELS`
      (e.g. `crash_severity_risk_v1`, `ebm_v1`).
    - creates a `ModelJob` with status `queued` and returns 202 + `job_id`.
    - designed to be wired into a task queue by the ML team.
  - `GET /api/models/results/<job_id>/` returns the current job status plus any
    high-level `result_metadata`. For queued/running jobs it returns 202;
    for finished jobs it returns 200 and a `result_metadata` payload.

## Frontend (React)

The frontend (`alaska_ui`) is a Vite + React app that consumes the Django APIs
and provides:

- An upload page for CSV/Parquet datasets with a validation results panel.
- A map-based view for filtering and exploring crashes.
- A simple model runner UI for triggering backend model jobs and viewing
  high-level results.

## Running the project locally

See `docs/deployment.md` for detailed instructions on running the full stack
(with or without Docker).

In short:

- Start Postgres (and PostGIS).
- Run migrations (`python manage.py migrate`).
- Start the backend (`python manage.py runserver`).
- Run the frontend dev server (`npm run dev` inside `alaska_ui`).

## Ingestion pipeline details

The ingestion pipeline is designed to be:

- **Secure by default**  strict file type/size checks, MIME sniffing, and
  antivirus integration.
- **Configurable**  schema, allowed extensions, and size limits are all driven
  by configuration rather than hard-coded.
- **Observable**  every major step is recorded in a structured `steps` array
  along with row-level summary counts to help the UI explain what happened.

### File formats

By default, the ingestion pipeline accepts:

- **CSV**  parsed with `pandas.read_csv` using UTF-8 decoding.
- **Parquet**  parsed with `pandas.read_parquet` (requires `pyarrow` or `fastparquet`,
  which are already included in the backend dependencies).

The set of allowed extensions is controlled by the `INGESTION_ALLOWED_EXTENSIONS`
environment variable, which should be a comma-separated list of lower-cased
extensions such as:

```bash
INGESTION_ALLOWED_EXTENSIONS=".csv,.parquet"
```

If the environment variable is not set, the default is `.csv,.parquet`.

Additional formats can be enabled by extending `INGESTION_ALLOWED_EXTENSIONS`
**and** updating the loader in
`ingestion.validation.load_dataframe_from_bytes` to handle the new format.

The key knobs are controlled by environment variables:

- `INGESTION_ALLOWED_EXTENSIONS` (default: `.csv,.parquet`)
- `INGESTION_MAX_FILE_SIZE_BYTES` (default: `10MB`  intentionally small so
  production must explicitly opt into larger uploads)
- `INGESTION_SCHEMA_CONFIG_PATH` (optional override of the MMUCC schema path)

### Antivirus

Antivirus integration is handled via **ClamAV** and the `clamd` Python
library. Configuration is controlled through environment variables:

- `CLAMAV_UNIX_SOCKET`  path to the ClamAV Unix socket
- `CLAMAV_TCP_HOST` / `CLAMAV_TCP_PORT`  host/port for TCP connections
- `INGESTION_REQUIRE_AV`  when set to `"true"`, any upload where the AV
  step is skipped will be treated as a hard failure.

### Ingestion status report shape

`POST /api/ingest/upload/` returns a JSON payload of the form:

```json
{
  "upload_id": "uuid-of-UploadedDataset",
  "overall_status": "accepted",
  "schema_version": "mmucc-alaska-v1",
  "steps": [
    {
      "step": "PAYLOAD",
      "status": "passed",
      "severity": "info",
      "is_hard_fail": false,
      "details": "Received multipart/form-data with a single file field named 'file'."
    },
    {
      "step": "EXTENSION_CHECK",
      "status": "passed",
      "severity": "info",
      "is_hard_fail": false,
      "details": "Extension '.csv' is allowed."
    },
    {
      "step": "FILE_SIZE",
      "status": "passed",
      "severity": "info",
      "is_hard_fail": false,
      "details": "File size is within the configured limit.",
      "meta": { "size_bytes": 12345, "max_size_bytes": 10485760 }
    },
    {
      "step": "MIME_SNIFF",
      "status": "passed",
      "severity": "info",
      "is_hard_fail": false,
      "details": "Declared Content-Type matches detected MIME type.",
      "meta": { "detected_mime_type": "text/csv", "declared_mime_type": "text/csv" }
    },
    {
      "step": "AV_SCAN",
      "status": "passed",
      "severity": "info",
      "is_hard_fail": false,
      "details": "ClamAV did not detect malware in the upload."
    },
    {
      "step": "PARSE_TABLE",
      "status": "passed",
      "severity": "info",
      "is_hard_fail": false,
      "details": "Parsed dataset with 1234 rows and 25 columns."
    },
    {
      "step": "SCHEMA_CHECK",
      "status": "passed",
      "severity": "info",
      "is_hard_fail": false,
      "details": "All required MMUCC columns are present.",
      "meta": {
        "missing_columns": [],
        "unknown_columns": [],
        "columns": ["crash_id", "crash_date", "severity", "..."],
        "schema_version": "mmucc-alaska-v1"
      }
    },
    {
      "step": "TYPE_AND_RANGE_CHECKS",
      "status": "passed",
      "severity": "warning",
      "is_hard_fail": false,
      "details": "Some rows have out-of-range or invalid values.",
      "meta": {
        "total_rows": 1234,
        "invalid_row_count": 12,
        "details": "..."
      }
    },
    {
      "step": "GEO_CHECKS",
      "status": "passed",
      "severity": "warning",
      "is_hard_fail": false,
      "details": "Some rows have coordinates outside configured Alaska bounds.",
      "meta": {
        "has_coordinates": true,
        "invalid_row_count": 3,
        "details": "..."
      }
    }
  ],
  "schema": {
    "schema_version": "mmucc-alaska-v1",
    "missing_columns": [],
    "unknown_columns": [],
    "columns": ["crash_id", "crash_date", "severity", "..."]
  },
  "row_checks": {
    "total_rows": 1234,
    "invalid_row_count": 12,
    "invalid_geo_row_count": 3
  }
}
```

If the upload is rejected for a "hard" reason (extension, size, MIME,
antivirus (when required), or schema), `overall_status` is `"rejected"`,
`upload_id` is omitted, and `error_code` / `message` describe the reason.

- `status` describes the outcome of the step itself (`"passed"`, `"failed"`,
  `"skipped"`).
- `severity` describes how serious the outcome is for the upload as a whole:
  - `"error"` + `is_hard_fail: true`  the upload is rejected.
  - `"warning"`  upload is accepted, but there are data quality issues.
  - `"info"`  purely informational, no impact on acceptance.

## Security & authentication

- All ingestion and model endpoints require authentication (`IsAuthenticated`).
- Access control is enforced so that users can only see uploads and jobs they
  own, unless they are in the `Admin` group or have staff/superuser status.
- CSRF protection is enabled for browser-based sessions; API clients should use
  token or session authentication as appropriate.


================================================================================
===== END FILE: README.md =====
================================================================================

================================================================================
===== BEGIN FILE: requirements.txt =====
================================================================================

Django>=5.0,<6.0
djangorestframework>=3.15.0,<4.0.0
pandas>=2.2.0,<3.0.0
numpy>=1.26.0,<2.0.0
psycopg2-binary>=2.9,<3.0
python-magic>=0.4,<1.0
clamd>=1.0,<2.0
pyarrow>=10.0.0
PyYAML>=6.0.0,<7.0.0
gunicorn>=22.0.0,<23.0.0
django-filter>=24.1,<25.0
drf-spectacular>=0.27.0,<1.0.0
django-cors-headers>=4.3.1,<5.0.0
scikit-learn>=1.3.0,<2.0.0
xgboost>=2.0.0,<3.0.0
interpret>=0.4.0,<0.5.0

================================================================================
===== END FILE: requirements.txt =====
================================================================================

